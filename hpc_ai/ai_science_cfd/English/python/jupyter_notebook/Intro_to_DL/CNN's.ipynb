{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Notebook](Part_2.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Home Page](../Start_Here.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](Resnets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Primer and Keras 101 - Continued \n",
    "\n",
    "This notebook introduces convolutional neural networks and associated terminology and concepts.\n",
    "\n",
    "**Contents of this notebook:**\n",
    "\n",
    "- [Convolutional neural networks (CNNs)](#Convolutional-Neural-Networks-(CNNs))\n",
    "- [Why are CNNs good for image related tasks?](#Why-are-CNNs-good-for-image-related-tasks?)\n",
    "- [Implementing image classification using CNNs](#Implementing-image-classification-using-CNNs)\n",
    "- [Conclusion](#Conclusion)\n",
    "\n",
    "\n",
    "**By the end of this notebook you will:**\n",
    "\n",
    "- Understand how a convolutional neural network works\n",
    "- Write your own CNN classifier and train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs) \n",
    "\n",
    "Convolutional neural networks are widely used in the field of image classification, object detection, and facial recognition because they are very effective in reducing the number of parameters without sacrificing on the quality of models.\n",
    "\n",
    "Let's now understand what makes up a CNN architecture and how it works. Here is an example of a CNN architecture for a classification task:\n",
    "\n",
    "![alt_text](images/cnn.jpeg)\n",
    "\n",
    "*Source: https://fr.mathworks.com/solutions/deep-learning/convolutional-neural-network.html*\n",
    "\n",
    "Each input image will be passed through a series of convolution layers with filters (kernels), pooling layers, fully connected layers, and finally a softmax function will be applied to classify an object with probabilistic values between 0 and 1. \n",
    "\n",
    "We will discuss the following concepts:\n",
    "\n",
    "- Convolution Layer \n",
    "- Strides and Padding \n",
    "- Pooling Layer\n",
    "- Fully Connected Layer \n",
    "\n",
    "#### Convolution Layer: \n",
    "\n",
    "A convolution layer learns features from the input by preserving the relationships between neighbouring pixels, and is typically the first layer in the network. The kernel size is a hyperparameter and can be altered according to the complexity of the problem. Let's see how a kernel operates on input data:\n",
    "\n",
    "![alt_text](images/conv.gif)\n",
    "\n",
    "*Source: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53*\n",
    "\n",
    "Given this definition of a convolution operation, let us now see how a convolution layer is applied in the context of a neural network.\n",
    "\n",
    "![alt_text](images/conv_depth.png)\n",
    "\n",
    "*Source: https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215*\n",
    "\n",
    "Defining the terms:\n",
    "\n",
    "- Hin: Height dimension of the input layer\n",
    "- Win: Width dimension of the input layer\n",
    "- Din: Depth of the input layer\n",
    "- h: Height of the kernel\n",
    "- w: Width of the kernel\n",
    "- Dout: Number of kernels/filters to apply (and number of output channels) \n",
    "\n",
    "Note: The depth of the input layer (Din) needs to match the depth of each kernel.\n",
    "\n",
    "Din and Dout are also called the number of channels in the layer. We can notice from the introductory image that typically the number of channels increases as we go deeper into the network, while the height and width keep decrease. This is done so that the filters aggregate features from the previous layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Strides and Padding \n",
    "\n",
    "The stride is the number of pixels shifted over the input matrix during convolution. When the stride is 1, we move the filters 1 pixel at a time. When the stride is 2, we move the filters 2 pixels at a time, and so on.\n",
    "\n",
    "Sometimes filters do not fit perfectly on the input image. So, we have two options:\n",
    "- Pad the picture with zeros (zero-padding) so that it fits\n",
    "- Drop the part of the image where the filter did not fit. This is called valid padding which keeps only the valid part of the image.\n",
    "\n",
    "#### Pooling Layer\n",
    "\n",
    "Pooling layers reduce the number of parameters when the images are too large. Spatial pooling, also called subsampling or downsampling, reduces the dimensionality of each layer but retains important information. Spatial pooling can be of different types:\n",
    "- Max Pooling\n",
    "    - Take the largest element from the input data.\n",
    "- Average Pooling\n",
    "    - Take the average of the elements from the input data.\n",
    "- Sum Pooling\n",
    "    - Sum of all elements in the input data.\n",
    "\n",
    "![alt_text](images/max_pool.png)\n",
    "\n",
    "*Source: https://www.programmersought.com/article/47163598855/*\n",
    "\n",
    "#### Fully Connected Layer\n",
    "\n",
    "We will then flatten the output from the convolution layers and feed into it a _fully connected layer_ to generate a prediction. The fully connected layer is a model whose inputs are the features of the inputs obtained from the convolution layers.\n",
    "\n",
    "These fully connected layers are then trained along with the _kernels_ during the training process.\n",
    "\n",
    "### Transposed Convolution\n",
    "\n",
    "When we apply our convolution operation over an image, we find that the number of channels increases while the height and width of the image decrease. In some cases (for different applications) we will need to up-sample our images. _Transposed convolution_ helps to up-sample the images from neural network layers.\n",
    "\n",
    "<table><tr>\n",
    "<td> <img src=\"images/convtranspose.gif\" alt=\"Drawing\" style=\"width: 540px;\"/></td>\n",
    "<td> <img src=\"images/convtranspose_conv.gif\" alt=\"Drawing\" style=\"width: 500px;\"/> </td>\n",
    "</tr></table>\n",
    "\n",
    "*Source https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215*\n",
    "\n",
    "Tranposed convolution can also be visualised as convolution of a layer with 2x2 padding (as seen on the right).\n",
    "\n",
    "\n",
    "## Why are CNNs good for image related tasks? \n",
    "\n",
    "In 1970, **David Marr** wrote a paper called [Vision](http://lolita.unice.fr/~scheer/cogsci/Marr%2082%20-%20Vision.pdf). It was a breakthrough in understanding how the brain does vision: he stated that the vision task is performed in a hierarchical manner. You start simple and get complex. For example, you start with something as simple as identifying edges and colours and then build upon them to detect the object and then classify them and so on.\n",
    "\n",
    "The architecture of CNNs is designed as such to emulate the human brain's technique to deal with images. As convolutions are mainly used for extracting high-level features from the images such as edges/other patterns, these algorithms try to emulate our understanding of vision. Certain filters do operations such as blurring the image, sharpening the image and then performing pooling operations on each of these filters to extract information from an image. As stated earlier, our understanding of vision indicates that vision is a hierarchical process, and our brain deals with vision in a similar fashion. CNNs also deal with understanding and classifying images similarly, thereby making them the appropriate choice for these kinds of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Image Classification using CNNs\n",
    "\n",
    "We will follow the same steps for data preprocessing as mentioned in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Import the Dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Print array size of training dataset\n",
    "print(\"Size of Training Images: \" + str(train_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Training Labels: \" + str(train_labels.shape))\n",
    "\n",
    "# Print array size of test dataset\n",
    "print(\"Size of Test Images: \" + str(test_images.shape))\n",
    "# Print array size of labels\n",
    "print(\"Size of Test Labels: \" + str(test_labels.shape))\n",
    "\n",
    "# Let's see how our outputs look\n",
    "print(\"Training Set Labels: \" + str(train_labels))\n",
    "# Data in the test dataset\n",
    "print(\"Test Set Labels: \" + str(test_labels))\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further data preprocessing\n",
    "\n",
    "You may have noticed by now that the training set is of shape `(60000,28,28)`.\n",
    "\n",
    "In convolutional neural networks, we need to feed the data in the form of a 4D Array as follows:\n",
    "\n",
    "`(num_images, x-dims, y-dims, num_channels_per_image)`\n",
    "\n",
    "So, as our image is grayscale, we will reshape it to `(60000,28,28,1)` before passing it to our neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Convolution Layers\n",
    "\n",
    "Let us see how to define convolution, max pooling, and dropout layers.\n",
    "\n",
    "\n",
    "#### Convolution Layer \n",
    "\n",
    "We will be using the following API to define the Convolution Layer.\n",
    "\n",
    "```tf.keras.layers.Conv2D(filters, kernel_size, padding='valid', activation=None, input_shape)```\n",
    "\n",
    "\n",
    "Let us briefly define the parameters:\n",
    "\n",
    "- `filters`: The dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "- `kernel_size`: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "- `padding`: One of \"valid\" or \"same\" (case-insensitive).\n",
    "- `activation`: Activation function to use (see activations). If you don't specify anything, no activation is applied (i.e. \"linear\" activation: a(x) = x).\n",
    "\n",
    "Documentation: [Convolutional Layers](https://keras.io/layers/convolutional/)\n",
    "\n",
    "#### Pooling Layer \n",
    "\n",
    "`tf.keras.layers.MaxPooling2D(pool_size=2)`\n",
    "\n",
    "- `pool_size`: Size of the max pooling window.\n",
    "\n",
    "Documentation: [Pooling Layers](https://keras.io/layers/pooling/)\n",
    "\n",
    "#### Dropout \n",
    "\n",
    "Dropout is an approach to regularization in neural networks which helps reduce interdependent learning amongst the neurons.\n",
    "\n",
    "Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase, with a certain set of neurons chosen at random. By “ignoring,\" we mean these units are not considered during a particular forward or backward pass.\n",
    "\n",
    "It is defined by the following function:\n",
    "\n",
    "`tf.keras.layers.Dropout(0.3)`\n",
    "\n",
    "- Parameter: Fraction of the input units to drop (float between 0 and 1).\n",
    "\n",
    "Documentation: [Dropout](https://keras.io/layers/core/#dropout)\n",
    "\n",
    "## Defining our Model and Training  \n",
    "\n",
    "Now that we are aware of the code for building a CNN, let us now build a five layer model:\n",
    "\n",
    "- Input layer: (28, 28, 1) \n",
    "    - Size of the input image\n",
    "- Convolution layers:\n",
    "    - First layer: Kernel size (2 x 2), resulting in 64 channels.\n",
    "        - Pooling of size (2 x 2) makes the layer (14 x 14 x 64)\n",
    "    - Second layer: Kernel size (2 x 2), resulting in 32 channels.\n",
    "        - Pooling of size (2 x 2) makes the layer (7 x 7 x 32)\n",
    "- Fully connected layers:\n",
    "    - Flatten the convolution layers to 1567 nodes = (7 * 7 * 32)\n",
    "    - Dense layer of size 256\n",
    "- Output layer:\n",
    "    - Dense layer with 10 classes using `softmax` activation\n",
    "\n",
    "![alt_text](images/our_cnn.png)\n",
    "\n",
    "Now we can define our model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Must define the input shape in the first layer of the neural network\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "# model.add(tf.keras.layers.Dropout(0.3))\n",
    "# Second convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "# Fully connected layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n",
    "\n",
    "* *Loss function* —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "* *Optimizer* —This is how the model is updated based on the data it sees and its loss function.\n",
    "* *Metrics* —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training the neural network model requires the following steps:\n",
    "\n",
    "1. Feed the training data to the model. In this example, the training data is in the `train_images` and `train_labels` arrays.\n",
    "2. The model learns to associate images and labels.\n",
    "3. You ask the model to make predictions about a test set—in this example, the `test_images` array. Verify that the predictions match the labels from the `test_labels` array.\n",
    "\n",
    "To start training,  call the `model.fit` method—so called because it \"fits\" the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model using the test set\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the test_images\n",
    "\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data from (28, 28) to (28, 28, 1)\n",
    "w, h = 28, 28\n",
    "train_images = train_images.reshape(train_images.shape[0], w, h)\n",
    "test_images = test_images.reshape(test_images.shape[0], w, h)\n",
    "\n",
    "\n",
    "# Helper functions to plot images \n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array, true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Running both our models for five epochs, here is a table comparing them (your results may be slightly different):\n",
    "\n",
    "|  Model                                   | Train Accuracy | Train Loss | Test Accuracy | Test Loss |\n",
    "|------------------------------------------|----------------|------------|---------------|-----------|\n",
    "| Fully connected network - After 5 Epochs |         0.8923 |     0.2935 |        0.8731 |    0.2432 |\n",
    "| Convolutional network - After 5 Epochs   |         0.8860 |     0.3094 |        0.9048 |    0.1954 |        \n",
    "\n",
    "## Exercise\n",
    "\n",
    "Play with different hyper-parameters (number of epochs, depth of layers, kernel size) to bring down the loss further.\n",
    "\n",
    "## Important\n",
    "\n",
    "<mark>Shut down the kernel before clicking on “Next Notebook” to free up the GPU memory.</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement : \n",
    "\n",
    "\n",
    "[Transposed Convolutions explained](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8)\n",
    "\n",
    "[Why are CNNs used more for computer vision tasks than other tasks?](https://www.quora.com/Why-are-CNNs-used-more-for-computer-vision-tasks-than-other-tasks)\n",
    "\n",
    "[Comprehensive introduction to Convolution](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)\n",
    "\n",
    "## Licensing\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previous Notebook](Part_2.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Home Page](../Start_Here.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](Resnets.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
