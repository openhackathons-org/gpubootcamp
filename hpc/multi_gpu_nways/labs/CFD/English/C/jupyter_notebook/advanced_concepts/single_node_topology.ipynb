{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790904cd",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this lab, we will learn about:\n",
    "\n",
    "* Understanding intra-node GPU topology and interconnections like PCIe and NVLink\n",
    "* Architecture overview of NVIDIA DGX 1 Tesla V100 system\n",
    "* Comparison of communication links and their impact on application performance\n",
    "* p2pBandwidthLatencyTest micro-benchmark for P2P performance analysis.\n",
    "\n",
    "# Intra-Node Communication Topology\n",
    "\n",
    "Let's dive deeper into how the underlying communication architecture of our system affects program performance. Run the command below to display your node's GPU and NIC communication topology:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf585d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57e0dd",
   "metadata": {},
   "source": [
    "If the output is unclear, you can launch a Terminal session by clicking on `File` $\\rightarrow$ Open and following the steps as shown:\n",
    "\n",
    "![open_terminal_session](../../images/open_terminal_session.png)\n",
    "\n",
    "On a DGX-1 system, the output is as follows:\n",
    "\n",
    "![nvidia_smi_topo_output](../../images/nvidia_smi_topo_output.png)\n",
    "\n",
    "Focus on a particular row, say GPU 0. The output states that GPUs 1 through 4 are connected to it via NVLink (in addition to PCIe) and GPUs 5 through 7 are connected to it via PCIe as well as an \"SMP\" interconnect. We have a dual-socket system and the CPUs in these sockets are connected by an interconnect known as SMP interconnect.\n",
    "\n",
    "Thus, GPU 0 to GPU 5 communication happens via not just PCIe, but also over the inter-socket interconnect within the same node. Clearly, this is a longer path than say the one between GPU 0 and GPU 1, which are connected via NVLink directly. We will discuss the NIC to GPU connection in the inter-node section of this bootcamp.\n",
    "\n",
    "Even within the GPUs connected via NVLink, we see different annotations such as `NV1` and `NV2` that affect the communication bandwidth and hence the performance. In this section, we will explore the nuances associated with a diverse intra-node GPU communication topology like in the output above. Specifically, in our system, the communication topology is as follows:\n",
    "\n",
    "![dgx1_8x_tesla_v100_topo](../../images/dgx1_8x_tesla_v100_topo.png)\n",
    "\n",
    "Qualitatively, the bandwidth and latency vary with the topology as follows:\n",
    "\n",
    "![intra_node_topology_map](../../images/intra_node_topology_map.png)\n",
    "\n",
    "Host staging implies traversing through the CPU and the travel path taken is one of PHB, NODE, and SYS. In contrast, if the path taken is either NV1, NV2, or PIX, then P2P is available. PXB implies that the GPUs belong to different PCIe hubs and P2P is usually not supported in this case.\n",
    "\n",
    "A double NVLink connection provides twice the bandwidth compared to a single NVLink. \n",
    "\n",
    "For a pair of 2 GPUs, the peak bidirectional bandwidth are as follows:\n",
    "* PCIe: Using PIX topology, 15.75GB/s for PCIe Gen 3.0 and 31.5GB/s for PCIe Gen 4.0.\n",
    "* NVLink: Using NV# topology, 50GB/s per connection. So a double NVLink connection has 100GB/s peak bidirectional bandwidth.\n",
    "\n",
    "Let us understand what difference the underlying communication topology can make to the application performance in the following sub-section.\n",
    "\n",
    "**Note:** If your command output doesn't show any NVLink connection or if there's no difference in connection type (PIX, PXB, PHB, NODE, SYS, NV#) between any 2 pair of GPUs, then the communication bandwidth and latency will likely be the same between any pair and the following sub-sections will not display any performance difference.\n",
    "\n",
    "## Performance variation due to system topology\n",
    "\n",
    "So far, the code runs the multi-GPU version on all available GPUs in a node (8 in our case). We can supply the `-gpus` runtime flag to the binary to run our code on specific GPUs. If we want to run on only 2 GPUs, namely GPU 0 and GPU 3, we use the `-gpus 0,3` argument. \n",
    "\n",
    "Try to find the GPU pair with highest bandwidth available as per the table above and replace `0,3` with those GPUs, and then run the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93961dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../../source_code/cuda && ./jacobi_memcpy -p2p -gpus 0,3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5d159",
   "metadata": {},
   "source": [
    "The efficiency would likely be higher than before due to less inter-GPU communication (each GPU does more wok instead). Our output is as follows:\n",
    "\n",
    "```bash\n",
    "Num GPUs: 2. Using GPU ID: 0, 3, \n",
    "16384x16384: 1 GPU:   4.4513 s, 2 GPUs:   2.2664 s, speedup:     1.96, efficiency:    98.20  \n",
    "```\n",
    "\n",
    "Now, run the binary a pair of GPUs that have the lowest available bandwidth. In our case, we use GPU 0 and GPU 7. Our output is:\n",
    "\n",
    "```bash\n",
    "Num GPUs: 2. Using GPU ID: 0, 7, \n",
    "16384x16384: 1 GPU:   4.4529 s, 2 GPUs:   2.3454 s, speedup:     1.90, efficiency:    94.93  \n",
    "```\n",
    "\n",
    "Now remove the `-p2p` flag and run the command again for GPUs 0 and 7. We didn't get any difference in performance. As you may recall, P2P is not possible between GPUs 0 and 7, so the underlying communication path doesn't change, resulting in same performance with and without the `-p2p` flag. \n",
    "\n",
    "The same can be confirmed by profiling the application and looking at the operations performed in the Nsight Systems timeline. \n",
    "\n",
    "![p2p_2_gpu_memcpy_nsys](../../images/p2p_2_gpu_memcpy_nsys.png)\n",
    "\n",
    "Try a few other GPU combinations and toggle P2P so see if the performance variation correlates with the table above. Also try reducing the grid size using `-nx` and `-ny` flags (to say 8192$\\times$8192) and see the effect on efficiency. \n",
    "\n",
    "## Benchmarking the system topology\n",
    "\n",
    "Our application is not very memory intensive. As is visible from the profiler output, $\\gt95\\%$ of the time in GPU is spent on computation. Therefore, to get a quantitative measure of latency and bandwidth impact due to topology, we run a micro-benchmark.\n",
    "\n",
    "### p2pBandwidthLatencyTest micro-benchmark\n",
    "\n",
    "p2pBandwidthLatencyTest is a part of [CUDA Samples GitHub repository](https://github.com/NVIDIA/cuda-samples) available to help CUDA developers. \n",
    "\n",
    "As the name suggests, this test measures the bandwidth and latency impact of P2P and underlying communication topology. Let's compile the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a8dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../../source_code/p2pBandwidthLatencyTest/ && make clean && make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83369c1b",
   "metadata": {},
   "source": [
    "Now, let's run the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeb793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd ../../source_code/p2pBandwidthLatencyTest/ && ./p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584f5ef",
   "metadata": {},
   "source": [
    "The first part of the benchmark gives device information and P2P access available from each GPU (similar to `nvidia-smi topo -m` command). Next, the benchmark measures the unidirectional and bidirectional bandwidth and latency with P2P disabled and enabled.\n",
    "\n",
    "We share partial results obtained in our DGX-1 system:\n",
    "\n",
    "```bash\n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3      4      5      6      7 \n",
    "     0 783.95   9.56  14.43  14.46  14.47  14.24  14.51  14.43 \n",
    "\n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3      4      5      6      7 \n",
    "     0 784.87  48.49  48.49  96.85  96.90  14.25  14.54  14.49 \n",
    "     \n",
    "P2P=Disabled Latency Matrix (us)\n",
    "   GPU     0      1      2      3      4      5      6      7 \n",
    "     0   1.78  17.52  16.41  16.43  17.35  16.88  17.34  16.85 \n",
    "     \n",
    "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
    "   GPU     0      1      2      3      4      5      6      7 \n",
    "     0   1.76   1.62   1.61   2.01   2.02  18.44  19.15  19.34\n",
    "```\n",
    "\n",
    "Our system is based on PCIe gen 3.0 with a peak maximum GPU-GPU PCIe banwidth of 15.75 GB/s. Let us analyze and understand these results:\n",
    "\n",
    "* GPU 0 and GPU 1/2: Connected by a single NVLink connection. By enabling P2P-\n",
    "  - Bandwidth reaches close to the maximum peak of 50 GB/s.\n",
    "  - Latency decreases by an order of magnitude.\n",
    "* GPU 0 and GPU 3/4: Connected by a double NVLink connection. By enabling P2P-\n",
    "  - Bandwidth reaches close to the maximum peak of 100 GB/s.\n",
    "  - Latency decreases by an order of magnitude.\n",
    "* GPU 0 and GPU 5/6/7: Connected by PCIe and SMP interconnect. By enabling P2P- \n",
    "  - Bandwidth is unchanged.\n",
    "  - Latency increases a marginally.\n",
    "  \n",
    "Correlate these results with the communication topology that can be displayed by usng `nvidia-smi topo -m` command and the qualtitative table in the previous section. They should be consistent with one another.\n",
    "\n",
    "In general, we should try to set the GPUs in an application such that a GPU can share data with its neighbours using a high-bandwidth, low-latency communication topology. Enabling P2P, when possible, usually improves the performance by eliminating host staging.\n",
    "\n",
    "We now have an in-depth understanding of intra-node topology and its effects on performance. Let us now analyze our P2P-enabled application again to uncover opportunities to extract more performance.\n",
    "\n",
    "Click on the link below to access the next lab where we discuss the need for CUDA streams and then implement them in our application.\n",
    "\n",
    "# [Next: CUDA Streams](../cuda/streams.ipynb)\n",
    "\n",
    "Here's a link to the home notebook through which all other notebooks are accessible:\n",
    "\n",
    "# [HOME](../../../start_here.ipynb)\n",
    "\n",
    "---\n",
    "## Links and Resources\n",
    "\n",
    "* [Documentation: NVIDIA DGX 1 Tesla V100 Whitepaper](https://images.nvidia.com/content/pdf/dgx1-v100-system-architecture-whitepaper.pdf)\n",
    "* [Concepts: NVLink](https://www.nvidia.com/en-in/data-center/nvlink/)\n",
    "* [Research: Effect of topology-awareness on communication](https://ieeexplore.ieee.org/abstract/document/7529932)\n",
    "* [Code: p2pBandwidthLatencyTest](https://github.com/NVIDIA/cuda-samples/tree/master/Samples/p2pBandwidthLatencyTest)\n",
    "* [Code: Multi-GPU Programming Models](https://github.com/NVIDIA/multi-gpu-programming-models)\n",
    "* [Code: GPU Bootcamp](https://github.com/gpuhackathons-org/gpubootcamp/)\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
