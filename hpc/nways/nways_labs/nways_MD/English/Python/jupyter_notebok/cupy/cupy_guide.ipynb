{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \n",
    "\n",
    "# A Quick Guide To CuPy\n",
    "---\n",
    "\n",
    "### Target Audience\n",
    "The CuPy quick guide targets python developers who are interested in developing HPC applications using CUDA accelerated Python library on the GPU. A background in C programming maybe recommended for intermediate level to foster easy understanding.\n",
    "\n",
    "### Objectives\n",
    "- **The objectives of this guide are to:**\n",
    "     - quickly get you started with CuPy from beginner to intermediate level\n",
    "     - teach you application of GPU programming concept to HPC field(s)\n",
    "     - show you how to maximize the throughput of your HPC implementation through computational speedup on the GPU.  \n",
    "\n",
    "### Outline\n",
    "   1. What is CuPy?\n",
    "   2. Features of CuPy\n",
    "   3. Installation Guide\n",
    "   4. CuPy Fundamentals\n",
    "   5. CUDA Kernels\n",
    "   6. Summary\n",
    "   7. HPC Approach\n",
    "    \n",
    "\n",
    "\n",
    "## 1.  What is CuPy?\n",
    "- CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA\n",
    "- CuPy consists of :\n",
    "    - cupy.ndarray \n",
    "    - the core multi-dimensional array class \n",
    "    - many functions \n",
    "- It supports a subset of numpy.ndarray interface which include:\n",
    "    - Basic and advance indexing \n",
    "    - Data types (int32, float32, uint64, complex64,... )\n",
    "    - Array manipulation routine (reshape)\n",
    "    - Linear Algebra functions (dot, matmul, etc)\n",
    "    - Reduction along axis (max, sum, argmax, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 6 7 8 9]\n",
      "[1 3 5]\n",
      "max: 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "#Basic indexing and slicing\n",
    "print(X[5:])\n",
    "\n",
    "#output: [5 6 7 8 9]\n",
    "\n",
    "print(X[1:7:2])\n",
    "\n",
    "#output: [1 3 5]\n",
    "\n",
    "#reduction and Linear Algebra function\n",
    "print(\"max:\", max(X))\n",
    "\n",
    "#output: max: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 5]\n",
      "matmul: 70.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Advance indexing\n",
    "X = np.array([[1, 2],[3, 4],[5, 6]])\n",
    "print(X[[0, 1, 2], [0, 1, 0]])\n",
    "\n",
    "#output: [1 4 5]\n",
    "\n",
    "B = np.array([1,2,3,4], dtype=np.float32)\n",
    "C = np.array([5,6,7,8], dtype=np.float32)\n",
    "print(\"matmul:\",np.matmul(B, C))\n",
    "\n",
    "#output: matmul: 70.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.+0.j 0.+1.j 0.+2.j]\n",
      " [0.+3.j 0.+4.j 0.+5.j]\n",
      " [0.+6.j 0.+7.j 0.+8.j]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#data type and array manipulation routine \n",
    "A =1j*np.arange(9, dtype=np.complex64).reshape(3,3)\n",
    "print(A)\n",
    "\n",
    "\n",
    "#output:\n",
    "#[[0.+0.j 0.+1.j 0.+2.j]\n",
    "# [0.+3.j 0.+4.j 0.+5.j]\n",
    "# [0.+6.j 0.+7.j 0.+8.j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features of CuPy\n",
    "\n",
    "- **Features of CuPy includes:**\n",
    "    - User-define elementwise CUDA kernels\n",
    "    - User-define reduction CUDA kernels\n",
    "    - Fusing CUDA kernels to optimize user-define calculation\n",
    "    - Customizable memory allocator and memory pool\n",
    "    - cuDNN utilities\n",
    "- These features  are developed to support performance\n",
    "- CuPy uses on-the-fly kernel synthesis: when a kernel call is required, it compiles a kernel code optimized for the shapes and dtypes of given arguments, sends it to the GPU device, and executes the kernel.\n",
    "- CuPy also caches the kernel code sent to GPU device within the process, which reduces the kernel transfer time on further calls. \n",
    "\n",
    "## 3. CuPy Installation Guide\n",
    "\n",
    "- **Requirements:**\n",
    "    - Recommended Linux distributions are Centos and Ubuntu\n",
    "    - NVIDIA CUDA GPU with the Compute Capability 3.0 or larger \n",
    "    - CUDA Toolkit: v9.0 - v11.2\n",
    "    - Python: v3.5.1+ - v3.9.0+ \n",
    "    - CuPy can also be install Windows OS but only supports Python 3.6.0 or later\n",
    "- **Python Dependencies**\n",
    "    - NumPy/SciPy-compatible API in CuPy v8 is based on NumPy 1.19 and SciPy 1.5.\n",
    "\n",
    "- **Wheels (precompiled binary package)**\n",
    "    - Available for Linux (x86_64, Python 3.5+) and Windows (amd64, Python 3.6+).\n",
    "\n",
    "- **Conda-Forge**\n",
    "    - conda install -c conda-forge cupy\n",
    "    - If you need to enforce the installation of a particular CUDA version (say 10.0) for driver compatibility, you can do:\n",
    "        - conda install -c conda-forge cupy cudatoolkit=10.0 \n",
    "    - To install CuPy with the cuTENSOR support enabled, you can do: \n",
    "        - conda install -c conda-forge cupy cutensor cudatoolkit=10.2\n",
    "\n",
    "- **CuPy inside Docker**\n",
    "    - You can pull CuPy Docker images from [here](https://hub.docker.com/r/cupy/cupy/). \n",
    "    - Using docker pull cupy/cupy\n",
    "    - Use NVIDIA Container Toolkit to run CuPy image with GPU. You can login to the environment with bash, and run the Python interpreter: \n",
    "        - docker run --gpus all -it cupy/cupy /bin/bash \n",
    "        - docker run --gpus all -it cupy/cupy /usr/bin/python \n",
    "\n",
    "- **Conda (full RAPIDS package)**\n",
    "<img src=\"../images/rapids_package.png\">\n",
    "\n",
    "###           CuPy Architecture\n",
    "\n",
    "<img src=\"../images/cupy_arch.png\">\n",
    "\n",
    "\n",
    "## 4. CuPy Fundamentals\n",
    "- **Cupy.ndarray**: CuPy is a GPU array backend that implements a subset of NumPy interface.\n",
    "```python\n",
    "#CuPy version\n",
    "import cupy as cp\n",
    "X_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "#NumPy version\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "```\n",
    "\n",
    "- CuPy considers the current device as the default device with device ID 0. It also allows temporary switch between GPU devices.\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "####### Current device (GPU ID: 0)##############\n",
    "gpu_0 = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Switch device\n",
    "cp.cuda.Device(1).use()\n",
    "gpu_1 = cp.array([1, 2, 3, 4])\n",
    "\n",
    "###### Switch GPU temporarily################\n",
    "import numpy as np\n",
    "with cp.cuda.Device(1):\n",
    "      gpu_1 = cp.array([1, 2, 3, 4])\n",
    "# back to device id 0\n",
    "gpu0 = cp.array([1, 2, 3, 4, 5]) \n",
    "```\n",
    "\n",
    "### Data transfer\n",
    "- Arrays can be moved from Host to Device (CPU -> GPU) using **cupy.asarray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "x_gpu = cp.asarray(x)\n",
    "print(x_gpu)\n",
    "\n",
    "#output: [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Device array can be move to Host(GPU -> CPU) using: **cupy.asnumpy or cupy.ndarray.get()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cpu:  [1 2 3 4 5]\n",
      "x_cpu_alt:  [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "x_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "#copy to Host\n",
    "x_cpu = cp.asnumpy(x_gpu)\n",
    "print(\"x_cpu: \",x_cpu)\n",
    "\n",
    "#alternative option\n",
    "x_cpu_alt = x_gpu.get()\n",
    "print(\"x_cpu_alt: \",x_cpu_alt) \n",
    "\n",
    "#output: \n",
    "#x_cpu:  [1 2 3 4 5]\n",
    "#x_cpu_alt:  [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to transfer an array between devices(GPU to GPU), **cupy.ndarray** is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_gpu_0:\n",
      " [[[0.00000000e+00 2.00000047e+00 5.12000122e+02]\n",
      "  [8.19200196e+03 1.31072031e+05 5.24288126e+05]\n",
      "  [2.09715251e+06 8.38861003e+06 3.35544401e+07]]\n",
      "\n",
      " [[6.71088803e+07 1.34217761e+08 2.68435521e+08]\n",
      "  [5.36871042e+08 1.07374209e+09 2.14748417e+09]\n",
      "  [4.29496834e+09 8.58993669e+09 1.28849040e+10]]]\n",
      "x_gpu_1:\n",
      " [[[0.00000000e+00 2.00000047e+00 5.12000122e+02]\n",
      "  [8.19200196e+03 1.31072031e+05 5.24288126e+05]\n",
      "  [2.09715251e+06 8.38861003e+06 3.35544401e+07]]\n",
      "\n",
      " [[6.71088803e+07 1.34217761e+08 2.68435521e+08]\n",
      "  [5.36871042e+08 1.07374209e+09 2.14748417e+09]\n",
      "  [4.29496834e+09 8.58993669e+09 1.28849040e+10]]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "with cp.cuda.Device(0):\n",
    "    x_gpu_0 = cp.ndarray([ 2, 3, 3]) \n",
    "print(\"x_gpu_0:\\n\", x_gpu_0)\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "      x_gpu_1 = cp.asarray(x_gpu_0)\n",
    "print(\"x_gpu_1:\\n\", x_gpu_1)\n",
    "\n",
    "#output\n",
    "#x_gpu_0:\n",
    "# [[[0. 0. 0.]\n",
    "#  [0. 0. 0.]\n",
    "#  [0. 0. 0.]]\n",
    "\n",
    "# [[0. 0. 0.]\n",
    "#  [0. 0. 0.]\n",
    "#  [0. 0. 0.]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU  & CPU agnostic code\n",
    "- The compatibility of CuPy with NumPy enables the implementation of CPU/GPU generic code using **cupy.get_array_module()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.31326169 2.12692801 3.04858735 4.01814993 5.00671535]\n",
      "[ 7  9 11 13 15]\n",
      "[ 7  9 11 13 15]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "#example: log(1 + exp(x))\n",
    "x_cpu  = np.array([1, 2, 3, 4, 5])\n",
    "x_gpu  = cp.get_array_module(x_cpu)\n",
    "result = x_gpu.maximum(0, x_cpu) + x_gpu.log1p(x_gpu.exp(-abs(x_cpu)))\n",
    "print(result)\n",
    "\n",
    "#output: [1.31326169 2.12692801 3.04858735 4.01814993 5.00671535]\n",
    "\n",
    "#An explicit conversion to a host \n",
    "x_gpu  = cp.array([6, 7, 8, 9, 10])\n",
    "result = cp.asnumpy(x_gpu) + x_cpu\n",
    "print(result)\n",
    "\n",
    "#output: [ 7  9 11 13 15]\n",
    "\n",
    "#An explicit conversion to a device\n",
    "result = x_gpu + cp.asarray(x_cpu)\n",
    "print(result)\n",
    "\n",
    "#output: [ 7  9 11 13 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CUDA Kernels\n",
    "\n",
    "- **CUDA Kernels can be define in Cupy as follows:**\n",
    "\n",
    "    - Elementwise Kernels\n",
    "    - Reduction Kernels\n",
    "    - Raw Kernels\n",
    "    - Kernel Fusion\n",
    "- These kernels are user-defined based.\n",
    "\n",
    "### Elementwise Kernels\n",
    "- The ElementwiseKernel class is used to define this type of kernel.\n",
    "- This kernel consists of four parts which includes:\n",
    "    1. a list of input argument \n",
    "    2. a list of output argument\n",
    "    3. a loop body code\n",
    "    4. kernel name\n",
    "- Variable name starting with underscore ‚Äú_‚Äù , ‚Äún‚Äù, and ‚Äúi‚Äù are regarded as reserved keywords.\n",
    "\n",
    "#### Example: z = x*w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5  0.5  3.5]\n",
      " [ 8.5 15.5 24.5]\n",
      " [35.5 48.5 63.5]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'float32 x , float32 w, float32 b'\n",
    "output_list = 'float32 z'\n",
    "code_body  = 'z =  (x * w) + b'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "dnnLayerNode = cp.ElementwiseKernel(input_list, output_list, code_body,'dnnLayerNode')\n",
    "\n",
    "# data\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "b = cp.array([-0.5], dtype=cp.float32)\n",
    "z = cp.empty((3,3), dtype=cp.float32)\n",
    "\n",
    "# kernel call with argument passing\n",
    "dnnLayerNode(x,w,b,z)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "# [ 8.5 15.5 24.5]\n",
    "# [35.5 48.5 63.5]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Kernel: Generic-type kernels\n",
    "- It can be used to define a generic-type kernels. It treats a type specifier of one character as a type placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5  0.5  3.5]\n",
      " [ 8.5 15.5 24.5]\n",
      " [35.5 48.5 63.5]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'T x , T w, T b'\n",
    "output_list = 'T z'\n",
    "code_body  = 'z =  (x * w) + b'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "dnnLayerNode = cp.ElementwiseKernel(input_list, output_list, code_body,'dnnLayerNode')\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "b = cp.array([-0.5], dtype=cp.float32)\n",
    "z = cp.empty((3,3), dtype=cp.float32)\n",
    "\n",
    "# kernel call with argument passing\n",
    "dnnLayerNode(x,w,b,z)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "# [ 8.5 15.5 24.5]\n",
    "# [35.5 48.5 63.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Kernels\n",
    "\n",
    "- Reduction kernel is implemented through the ReductionKernel class. \n",
    "- In order to implement this kernel class, the following parts must be defined:\n",
    "    - **Identity value**: to initialize reduction value.\n",
    "    - **Mapping expression**: Used for the pre-processing of each element to be reduced.\n",
    "    - **Reduction expression**: It is an operator to reduce the multiple mapped values. The special variables **a** and **b** are used for its operands.\n",
    "    - **Post mapping expression**: It is used to transform the resulting reduced values. The special variable a is used as its input. Output should be written to the output parameter.\n",
    "\n",
    "\n",
    "**Example:  ùëß=‚àë_(ùëñ=1)ùë•_ùëñ *ùë§_ùëñ+ùëè**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.9\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "dnnLayer = cp.ReductionKernel(\n",
    "\t'T x, T w, T bias',              \n",
    "\t'T z',                         \n",
    "\t'x * w',                      \n",
    "\t'a + b', \n",
    "\t'z = a + bias',              \n",
    "\t'0',                            \n",
    "\t'dnnLayer'  )\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2,5)\n",
    "w = cp.arange(10, dtype=np.float32).reshape(2,5)\n",
    "bias = -0.1\n",
    "z = dnnLayer(x,w,bias)\n",
    "print(z)\n",
    "\n",
    "#output: 284.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Kernels\n",
    "\n",
    "- Raw kernels enables  the  direct use of kernels from CUDA source, and it is defined through the RawKernel class.\n",
    "- The RawKernel object allows you to call the kernel with CUDA‚Äôs cuLaunchKernel interface. In other words, you have control over:\n",
    "    - grid size\n",
    "    - block size\n",
    "    - shared memory size \n",
    "    - and stream. \n",
    "<img src=\"../images/cupy_kernel_memory.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.]\n",
      " [ 20.  22.  24.  26.  28.  30.  32.  34.  36.  38.]\n",
      " [ 40.  42.  44.  46.  48.  50.  52.  54.  56.  58.]\n",
      " [ 60.  62.  64.  66.  68.  70.  72.  74.  76.  78.]\n",
      " [ 80.  82.  84.  86.  88.  90.  92.  94.  96.  98.]\n",
      " [100. 102. 104. 106. 108. 110. 112. 114. 116. 118.]\n",
      " [120. 122. 124. 126. 128. 130. 132. 134. 136. 138.]\n",
      " [140. 142. 144. 146. 148. 150. 152. 154. 156. 158.]\n",
      " [160. 162. 164. 166. 168. 170. 172. 174. 176. 178.]\n",
      " [180. 182. 184. 186. 188. 190. 192. 194. 196. 198.]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "add_kernel = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void add_func(const float* x1, const float* x2, float* y) {\n",
    "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "y[tid] = x1[tid] + x2[tid];\n",
    "}\n",
    "''', 'add_func')\n",
    "\n",
    "N = 100\n",
    "shape = (10, 10)\n",
    "\n",
    "x1 = cp.arange(N, dtype=cp.float32).reshape(shape)\n",
    "x2 = cp.arange(N, dtype=cp.float32).reshape(shape)\n",
    "y = cp.zeros((shape), dtype=cp.float32)\n",
    "\n",
    "add_kernel((10,), (10,), (x1, x2, y)) \n",
    "\n",
    "print(y)\n",
    "\n",
    "#output:\n",
    "#[[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.]\n",
    "#[ 20.  22.  24.  26.  28.  30.  32.  34.  36.  38.]\n",
    "#[ 40.  42.  44.  46.  48.  50.  52.  54.  56.  58.]\n",
    "#[ 60.  62.  64.  66.  68.  70.  72.  74.  76.  78.]\n",
    "#[ 80.  82.  84.  86.  88.  90.  92.  94.  96.  98.]\n",
    "#[100. 102. 104. 106. 108. 110. 112. 114. 116. 118.]\n",
    "#[120. 122. 124. 126. 128. 130. 132. 134. 136. 138.]\n",
    "#[140. 142. 144. 146. 148. 150. 152. 154. 156. 158.]\n",
    "#[160. 162. 164. 166. 168. 170. 172. 174. 176. 178.]\n",
    "#[180. 182. 184. 186. 188. 190. 192. 194. 196. 198.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/raw_kernel.png\">\n",
    "\n",
    "#### Raw Kernels : Complex-value arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. +0.j  1. +2.j  2. +4.j  3. +6.j  4. +8.j]\n",
      " [ 5.+10.j  6.+12.j  7.+14.j  8.+16.j  9.+18.j]\n",
      " [10.+20.j 11.+22.j 12.+24.j 13.+26.j 14.+28.j]\n",
      " [15.+30.j 16.+32.j 17.+34.j 18.+36.j 19.+38.j]\n",
      " [20.+40.j 21.+42.j 22.+44.j 23.+46.j 24.+48.j]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "complex_kernel = cp.RawKernel(r'''\n",
    "#include <cupy/complex.cuh>\n",
    "extern \"C\" __global__\n",
    "void my_func(const complex<float>* x1, const complex<float>* x2, complex<float>* y, float a){\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    y[tid] = x1[tid] + a * x2[tid];\n",
    "}\n",
    "''', 'my_func')\n",
    "\n",
    "x1 = cp.arange(25, dtype=cp.complex64).reshape(5,5)\n",
    "x2 = 1j*cp.arange(25, dtype=cp.complex64).reshape(5,5)\n",
    "y = cp.zeros((5,5), dtype=cp.complex64)\n",
    "\n",
    "complex_kernel((1,),(25,),(x1, x2,y,cp.float32(2.0)))\n",
    "print(y)\n",
    "\n",
    "#output:\n",
    "#[[ 0. +0.j  1. +2.j  2. +4.j  3. +6.j  4. +8.j]\n",
    "#[ 5.+10.j  6.+12.j  7.+14.j  8.+16.j  9.+18.j]\n",
    "#[10.+20.j 11.+22.j 12.+24.j 13.+26.j 14.+28.j]\n",
    "#[15.+30.j 16.+32.j 17.+34.j 18.+36.j 19.+38.j]\n",
    "#[20.+40.j 21.+42.j 22.+44.j 23.+46.j 24.+48.j]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. +0.j  1. +2.j  2. +4.j  3. +6.j  4. +8.j]\n",
      " [ 5.+10.j  6.+12.j  7.+14.j  8.+16.j  9.+18.j]\n",
      " [10.+20.j 11.+22.j 12.+24.j 13.+26.j 14.+28.j]\n",
      " [15.+30.j 16.+32.j 17.+34.j 18.+36.j 19.+38.j]\n",
      " [20.+40.j 21.+42.j 22.+44.j 23.+46.j 24.+48.j]]\n"
     ]
    }
   ],
   "source": [
    "#This also produced the same output:\n",
    "complex_kernel((5,), (5,), (x1, x2, y, cp.float32(2.0)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_dynamic_shared_size_bytes:  49152\n",
      "max_threads_per_block:  1024\n",
      "attributes:  {'max_threads_per_block': 1024, 'shared_size_bytes': 0, 'const_size_bytes': 0, 'local_size_bytes': 0, 'num_regs': 12, 'ptx_version': 75, 'binary_version': 75, 'cache_mode_ca': 0, 'max_dynamic_shared_size_bytes': 49152, 'preferred_shared_memory_carveout': -1}\n"
     ]
    }
   ],
   "source": [
    "####### Kernel Attributes ##################\n",
    "print(\"max_dynamic_shared_size_bytes: \", complex_kernel.max_dynamic_shared_size_bytes )\n",
    "\n",
    "print(\"max_threads_per_block: \", complex_kernel.max_threads_per_block )\n",
    "\n",
    "print(\"attributes: \",complex_kernel.attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Modules \n",
    "\n",
    "- The **RawModule** class is used to defining a large raw CUDA C source or loading an existing CUDA binary.\n",
    "- It is initialized by a CUDA C source code having a several kernels (functions) such that needed kernels are retrieved by calling the **get_function()** method.\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "loaded_from_source = r'''\n",
    "extern \"C\" {\n",
    "__global__ void test_sum(const float* A, const float* B, float* C, int N)\n",
    " { \n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(tid < N)\n",
    "    {\n",
    "      C[tid] = A[tid] + B[tid]; \n",
    "    }\n",
    " }\n",
    " __global__ void test_multiply(const float* A, const float* B, float* C, int N )\n",
    " {\n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(tid < N)\n",
    "    {\n",
    "        C[tid] = A[tid] * B[tid];\n",
    "    }\n",
    " }\n",
    "}'''\n",
    "```\n",
    "##### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "load_raw_module = r'''\n",
    "extern \"C\" {\n",
    "__global__ void sum_ker(const float* a, const float* b, float* c)\n",
    " { \n",
    "    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    c[tid] = a[tid] + b[tid]; \n",
    "    \n",
    " }\n",
    " __global__ void multiply_ker(const float* a, const float* b, float* c )\n",
    " {\n",
    "    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    c[tid] = a[tid] * b[tid];\n",
    " }\n",
    "}'''\n",
    "\n",
    "module = cp.RawModule(code = load_raw_module)\n",
    "\n",
    "ker_sum = module.get_function('sum_ker')\n",
    "ker_times = module.get_function('multiply_ker')\n",
    "\n",
    "a = cp.arange(25, dtype=cp.float32).reshape(5,5)\n",
    "b = cp.ones((5,5), dtype=cp.float32)\n",
    "c = cp.zeros((5,5), dtype=cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15.]\n",
      " [16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25.]]\n"
     ]
    }
   ],
   "source": [
    "# run the above cell before runing this cell\n",
    "ker_sum((1,),(25,), (a,b,c))\n",
    "print(c)\n",
    "\n",
    "#output:\n",
    "#[[ 1.  2.  3.  4.  5.]\n",
    "#[ 6.  7.  8.  9. 10.]\n",
    "#[11. 12. 13. 14. 15.]\n",
    "#[16. 17. 18. 19. 20.]\n",
    "#[21. 22. 23. 24. 25.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]]\n"
     ]
    }
   ],
   "source": [
    "ker_times((5,),(5,),(a,b,c))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Fusion\n",
    "\n",
    "- Kernel fusion is a decorator that fuses functions. It can be used to define an elementwise or reduction kernels easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5  0.5  3.5]\n",
      " [ 8.5 15.5 24.5]\n",
      " [35.5 48.5 63.5]]\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "@cp.fuse(kernel_name='dnnlayerNode')\n",
    "def dnnlayerNode(x, w, bias):\n",
    "    return  (x * w) + bias\n",
    "    \n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "bias = cp.array([-0.5], dtype=cp.float32)\n",
    "         \n",
    "z = dnnlayerNode(x,w,bias)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "#[ 8.5 15.5 24.5]\n",
    "#[35.5 48.5 63.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "<img src=\"../images/cupy_summary.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align:center; color:#FF0000\"><b>Click on HPC Approach Link to view task on HPC serial code</b> </div>\n",
    "\n",
    "## 7. [HPC Approach](serial_RDF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "##  References\n",
    "- https://docs.cupy.dev/en/stable/\n",
    "- https://cupy.dev/\n",
    "- CuPy Documentation Release 8.5.0, Preferred Networks, inc. and Preferred Infrastructure inc., Feb 26, 2021.\n",
    "- Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018.\n",
    "- Crissman Loomis and Emilio Castillo, CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features, GTC Digital March, March 2020.\n",
    "- https://www.gpuhackathons.org/technical-resources\n",
    "- https://rapids.ai/start.html\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
