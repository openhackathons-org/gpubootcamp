{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \n",
    "\n",
    "# Numba For CUDA GPU: A Quick Guide\n",
    "---\n",
    "\n",
    "### Target Audience\n",
    "This quick guide targets python developers who are interested in developing HPC applications using CUDA accelerated Python library on the GPU.\n",
    "\n",
    "### Objectives\n",
    "- **The objectives of this guide are to:**\n",
    "    -   quickly get you started with Numba from beginner to advanced level\n",
    "    -   teach you application of CUDA GPU programming concept in HPC field(s)\n",
    "    -   show you how to maximize the throughput of your HPC implementation through computational speedup on the GPU.  \n",
    "\n",
    "### Outline\n",
    "   1. Introduction\n",
    "   2. Installation Guide\n",
    "   3. Definition of Terms\n",
    "   4. CUDA Kernels\n",
    "   5. Memory Management\n",
    "   6. Atomic Operation\n",
    "   7. CUDA UFuncs\n",
    "   8. Summary\n",
    "   9. HPC Approach\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "### 1. Introduction\n",
    "- Numba is a just-in-time (jit) compiler for Python that works best on code that uses NumPy arrays, functions, and loops. Numba has set of decorators that can be specified before user-defined functions to determine how they are compiled.  \n",
    "- A decorated function written in python is compiled into CUDA kernel to speed up execution rate, thus, Numba supports CUDA GPU programming model. \n",
    "- A kernel is written in Numba automatically have direct access to NumPy arrays. This implies a great support for data visiblilty between the host (CPU) and the device (GPU). \n",
    "\n",
    "### 2. Installation Guide\n",
    "- OS: \n",
    "    support Windows (32 and 64 bit) and Linux (32 and 64 bit)\n",
    "- Numba is available as a conda package for the Anaconda Python distribution:\n",
    "    - conda install cudatoolkit \n",
    "    - conda install numba\n",
    "- Numba also has wheels available:\n",
    "    - pip install numba\n",
    "\n",
    "### 3. Definition of Terms\n",
    "- The CPU is called a **Host**.  \n",
    "- The GPU is called a **Device**.\n",
    "- A GPU function launched by the host and executed on the device is called a **Kernels**.\n",
    "- A GPU function executed on the device which can only be called from the device is called a **Device function**.\n",
    "\n",
    "### Note\n",
    "- It is recommended to visit the NVIDIA official documentary web page and read through [CUDA C programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide), because most CUDA programming features exposed by Numba map directly to the CUDA C language offered by NVidia. \n",
    "- Numba does not implement of these CUDA features of CUDA:\n",
    "     - dynamic parallelism\n",
    "     - texture memory\n",
    "\n",
    "## 4. CUDA Kernel\n",
    "- In CUDA, written code can be executed by hundreds or thousands of threads at a single run, hence, a solution is modeled after the following thread hierarchy: \n",
    "    - **Grid**: A kernel executed as a collection of blocks \n",
    "    - **Thread Block**: Collection of threads that can communicate via a shared memory. Each thread is executed by a core.\n",
    "    - **Thread**: Single execution units that run kernels on GPU.\n",
    "- Numba exposes three kinds of GPU memory: \n",
    "    - global device memory  \n",
    "    - shared memory \n",
    "    - local memory. \n",
    "- Memory access should be carefully considered in order to keep bandwidth contention at minimal.\n",
    "\n",
    " <img src=\"../images/thread_blocks.JPG\"/> <img src=\"../images/memory_architecture.png\"/> \n",
    "\n",
    "### Kernel Declaration\n",
    "- A kernel function is a GPU function that is called from a CPU code by specifying the number of block threads and threads per block, and can not explicitly return a value except through a passed array. \n",
    "- A kernel can be called multiple times with varying number of blocks per grid and threads per block after its has been compiled once.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def arrayAdd(array_A, array_B, array_out):\n",
    "    #...code body ...\n",
    "```\n",
    "###### Kernel Invocation\n",
    "- A kernel is typically launched in the following way:\n",
    "```python\n",
    "threadsperblock = 128\n",
    "N = array_out.size\n",
    "blockspergrid = ( N + (threadsperblock - 1))// threadsperblock\n",
    "arrayAdd[blockspergrid, threadsperblock](array_A, array_B, array_out)\n",
    "```\n",
    "\n",
    "###### Choosing Block Size\n",
    "- The block size determines how many threads share a given area of shared memory.\n",
    "- The block size must be large enough to accommodate all computation units. See more details [here](https://docs.nvidia.com/cuda/cuda-c-programming-guide/).\n",
    "\n",
    "### Thread Positioning \n",
    "- When running a kernel, the kernel function’s code is executed by every thread once. Hence is it important to uniquely identify distinct threads.\n",
    "- The default way to determine a thread position in a grid and block is to manually compute the corresponding array position:\n",
    "\n",
    "\n",
    "<img src=\"../images/thread_position.png\"/>\n",
    "\n",
    "\n",
    "```python\n",
    "threadsperblock = 128\n",
    "N = array_out.size\n",
    "\n",
    "@cuda.jit\n",
    "def arrayAdd(array_A, array_B, array_out):\n",
    "    tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if tid < N: #Check array boundaries\n",
    "        array_out[tid] =  array_A[tid] + array_B[tid]\n",
    "\n",
    "#Unless you are sure the block size and grid size are a divisor of your array size, you must check boundaries as shown in the code block above. \n",
    "```\n",
    "### Example 1: Addition on 1D-Arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [     0      2      4 ... 999994 999996 999998] \n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "N = 500000\n",
    "threadsperblock = 1000\n",
    "\n",
    "@cuda.jit()\n",
    "def arrayAdd(array_A, array_B, array_out):\n",
    "    tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if tid < N:\n",
    "        array_out[tid] = array_A[tid] + array_B[tid]\n",
    "\n",
    "\n",
    "        \n",
    "array_A = np.arange(N, dtype=np.int)\n",
    "array_B = np.arange(N, dtype=np.int)\n",
    "array_out = np.zeros(N, dtype=np.int)\n",
    "\n",
    "blockpergrid  = N + (threadsperblock - 1) // threadsperblock\n",
    "\n",
    "arrayAdd[blockpergrid, threadsperblock](array_A, array_B, array_out)\n",
    "\n",
    "print(\"result: {} \".format(array_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Example 1:** \n",
    "> - N is the size of the array and the number of threads in a single block is 128.\n",
    "> - The **cuda.jit()** decorator indicates that the function (arrayAdd) below is a device kernel and should run parallel. The **tid** is the estimate of a unique index for each thread in the device memory grid: \n",
    ">> **tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x**.\n",
    "> - **array_A** and **array_B** are input data, while **array_out** is output array and is already preload with zeros.\n",
    "> - The statement **blockpergrid  = N + (threadsperblock - 1) // threadsperblock** Computes the size of block per grid. This line of code is commonly use as the default formular to estimate number of blocks per grid in several GPU programming documentations.\n",
    "> - **arrayAdd[blockpergrid, threadsperblock](array_A, array_B, array_out)** indicate a call to a kernel function **addAdd** having the number of blocks per grid and number of threads per block in square bracket, while kernel arguments are in round brackets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###  Matrix multiplication on 2D Array \n",
    "\n",
    "<img src=\"../images/2d_array.png\"/>\n",
    "\n",
    "<img src=\"../images/2d_col_mult.png\"/>\n",
    "\n",
    "> **Note**\n",
    "> - **Approach 2** would not be possible if the matrix size exceed the maximum number of threads per block on the device, while **Approach 1** would continue to execute. Most latest GPUs have maximum of 1024 threads per thread block. \n",
    "\n",
    "### Example 2:  Matrix multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_A:\n",
      " [[0 0 0 0]\n",
      " [1 1 1 1]\n",
      " [2 2 2 2]\n",
      " [3 3 3 3]]\n",
      "\n",
      "array_B:\n",
      " [[0 1 2 3]\n",
      " [0 1 2 3]\n",
      " [0 1 2 3]\n",
      " [0 1 2 3]]\n",
      "\n",
      "array_A * array_B:\n",
      " [[ 0  0  0  0]\n",
      " [ 0  4  8 12]\n",
      " [ 0  8 16 24]\n",
      " [ 0 12 24 36]]\n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N = 4\n",
    "@cuda.jit()\n",
    "def MatrixMul2D(array_A, array_B, array_out):\n",
    "   row, col = cuda.grid(2)\n",
    "   if row < array_out.shape[0] and col < array_out.shape[1]:\n",
    "      for k in range(N):\n",
    "         array_out[row][col]+= array_A[row][k] * array_B[k][col]\n",
    "\n",
    "\n",
    "array_A   = np.array([[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]], dtype=np.int)\n",
    "array_B   = np.array([[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]], dtype=np.int)\n",
    "array_out = np.zeros(N*N, dtype=np.int).reshape(N, N)\n",
    "\n",
    "threadsperblock = (2,2)\n",
    "blockpergrid_x  = (math.ceil( N / threadsperblock[0]))\n",
    "blockpergrid_y  = (math.ceil( N / threadsperblock[1]))\n",
    "blockpergrid    = (blockpergrid_x, blockpergrid_y)\n",
    "\n",
    "MatrixMul2D[blockpergrid,threadsperblock](array_A, array_B, array_out)\n",
    "\n",
    "print(\"array_A:\\n {}\\n\".format(array_A))\n",
    "print(\"array_B:\\n {}\\n\".format(array_B))\n",
    "print(\"array_A * array_B:\\n {}\".format(array_out))\n",
    "\n",
    "#Note\n",
    "#The cuda.grid() returns the thread ID in X and Y (row & col) direction of the memory grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exaample 3: A 225 × 225 Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  848610000   848635200   848660400 ...   854204400   854229600\n",
      "    854254800]\n",
      " [ 2124360000  2124435825  2124511650 ...  2141193150  2141268975\n",
      "   2141344800]\n",
      " [ -894857296  -894730846  -894604396 ...  -866785396  -866658946\n",
      "   -866532496]\n",
      " ...\n",
      " [  597268464   608532414   619796364 ... -1197101932 -1185837982\n",
      "  -1174574032]\n",
      " [ 1873018464  1884333039  1895647614 ...    89886818   101201393\n",
      "    112515968]\n",
      " [-1146198832 -1134833632 -1123468432 ...  1376875568  1388240768\n",
      "   1399605968]]\n"
     ]
    }
   ],
   "source": [
    "N = 225\n",
    "\n",
    "@cuda.jit()\n",
    "def MatrixMul2D(array_A, array_B, array_out):\n",
    "   x, y = cuda.grid(2)\n",
    "   if x < array_out.shape[0] and y < array_out.shape[1]:\n",
    "      for k in range(N):\n",
    "         array_out[x][y] += array_A[x][k] * array_B[k][y]\n",
    "\n",
    "threadsperblock = (25,25)\n",
    "array_A = np.arange((N*N), dtype=np.int).reshape(N,N)\n",
    "array_B = np.arange((N*N), dtype=np.int).reshape(N,N)\n",
    "array_out = np.zeros((N*N), dtype=np.int).reshape(N,N)\n",
    "\n",
    "blockpergrid_x  = (math.ceil( N / threadsperblock[0]))\n",
    "blockpergrid_y  = (math.ceil( N / threadsperblock[1]))\n",
    "blockpergrid    = (blockpergrid_x, blockpergrid_y)\n",
    "\n",
    "MatrixMul2D[blockpergrid,threadsperblock](array_A, array_B, array_out)\n",
    "\n",
    "print(array_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread reuse \n",
    "\n",
    "- It is possible to specify a few number of threads for a data size such that threads are reused to complete the computation of the entire data. This is one of the approach used when a data to be computed is larger than the maximum number of threads available in a device memory. \n",
    "- This statement is used in a while loop: ***tid += cuda.blockDim.x * cuda.gridDim.x***\n",
    "- An example is given below to illustrates thread reuse. In the example, small number of thread is specified on purpose in order to show the possibility of this approach. \n",
    "\n",
    "\n",
    "#### Example 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: [     0      2      4 ... 999994 999996 999998] \n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "N = 500000\n",
    "threadsperblock = 1000\n",
    "\n",
    "@cuda.jit\n",
    "def arrayAdd(array_A, array_B, array_out):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   while tid < N:\n",
    "      array_out[tid] = array_A[tid] + array_B[tid]\n",
    "      tid += cuda.blockDim.x * cuda.gridDim.x\n",
    "\n",
    "array_A = np.arange(N, dtype=np.int)\n",
    "array_B = np.arange(N, dtype=np.int)\n",
    "array_out = np.zeros(N, dtype=np.int)\n",
    "\n",
    "arrayAdd[1, threadsperblock](array_A, array_B, array_out)\n",
    "\n",
    "print(\"result: {} \".format(array_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> - The task in **example 4** is the same as in **example 1** but with limited number of threads specified, howbeit, the same result was achieved. \n",
    "> - Note that this approach may delegate more threads than required. In the code above, an excess of 1 block of threads may be delegated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory Management\n",
    "\n",
    "### Data Transfer \n",
    "- When a kernel is excuted, Numba automatically transfer NumPy arrays to the device and vice versa.\n",
    "- In order to avoid the unnecessary transfer for read-only arrays, the following APIs can be used to manually control the transfer.\n",
    "\n",
    "##### 1.  Copy host to device\n",
    "```python\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "N = 500000\n",
    "h_A = np.arange(N, dtype=np.int)\n",
    "h_B = np.arange(N, dtype=np.int)\n",
    "h_C = np.zeros(N, dtype=np.int)\n",
    "\n",
    "d_A = cuda.to_device(h_A)\n",
    "d_B = cuda.to_device(h_B)\n",
    "d_C = cuda.to_device(h_C)\n",
    "```\n",
    "##### 2.  Enqueue the transfer to a stream\n",
    "```python\n",
    "h_A    = np.arange(N, dtype=np.int)\n",
    "stream = cuda.stream()\n",
    "d_A    = cuda.to_device(h_A, stream=stream)\n",
    "```\n",
    "##### 3.  Copy device to host / enqueue the transfer to a stream \n",
    "```python\n",
    "h_C = d_C.copy_to_host()\n",
    "h_C = d_C.copy_to_host(stream=stream)\n",
    "```\n",
    "### Example 5:  data movement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34\n",
      "  36  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70\n",
      "  72  74  76  78  80  82  84  86  88  90  92  94  96  98 100 102 104 106\n",
      " 108 110 112 114 116 118 120 122 124 126 128 130 132 134 136 138 140 142\n",
      " 144 146 148 150 152 154 156 158 160 162 164 166 168 170 172 174 176 178\n",
      " 180 182 184 186 188 190 192 194 196 198 200 202 204 206 208 210 212 214\n",
      " 216 218 220 222 224 226 228 230 232 234 236 238 240 242 244 246 248 250\n",
      " 252 254 256 258 260 262 264 266 268 270 272 274 276 278 280 282 284 286\n",
      " 288 290 292 294 296 298 300 302 304 306 308 310 312 314 316 318 320 322\n",
      " 324 326 328 330 332 334 336 338 340 342 344 346 348 350 352 354 356 358\n",
      " 360 362 364 366 368 370 372 374 376 378 380 382 384 386 388 390 392 394\n",
      " 396 398]\n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "N = 200\n",
    "threadsperblock = 25\n",
    "\n",
    "@cuda.jit\n",
    "def arrayAdd(d_A, d_B, d_C):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   if tid < N:\n",
    "      d_C[tid] = d_A[tid] + d_B[tid]\n",
    "      \n",
    "h_A = np.arange(N, dtype=np.int)\n",
    "h_B = np.arange(N, dtype=np.int)\n",
    "h_C = np.zeros(N, dtype=np.int)\n",
    "\n",
    "d_A = cuda.to_device(h_A)\n",
    "d_B = cuda.to_device(h_B)\n",
    "d_C = cuda.to_device(h_C)\n",
    "\n",
    "blockpergrid  = N + (threadsperblock - 1) // threadsperblock\n",
    "arrayAdd[blockpergrid, threadsperblock](d_A, d_B, d_C)\n",
    "\n",
    "h_C = d_C.copy_to_host()\n",
    "print(h_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Atomic Operation\n",
    "\n",
    "- Atomic operation is required in a situation where multiple threads attempt to modify a common portion of the memory. \n",
    "- Typical example includes: simultaneous withdrawal from a bank account through ATM machine or large number of threads modfying a particular index of an array based on certain condition(s)\n",
    "- List of presently implemented atomic operations supported by Numba are:\n",
    "> **import numba.cuda as cuda**\n",
    "> - cuda.atomic.add(array, index, value)\n",
    "> - cuda.atomic.min(array, index, value)\n",
    "> - cuda.atomic.max(array, index, value)\n",
    "> - cuda.atomic.nanmax(array, index, value)\n",
    "> - cuda.atomic.nanmin(array, index, value)\n",
    "> - cuda.atomic.compare_and_swap(array, old_value, current_value)\n",
    "> - cuda.atomic.sub(array, index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic: [55]\n",
      "Non atomic:  [1]\n"
     ]
    }
   ],
   "source": [
    "# Task ==> sum of an array: [1,2,3,4,5,6,7,8,9,10] in parallel\n",
    "# Note that threads are executed randomly\n",
    "\n",
    "# atomic operation example \n",
    "size = 10\n",
    "nthread = 10\n",
    "@cuda.jit()\n",
    "def add_atomic(my_array, total):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   cuda.atomic.add(total,0, my_array[tid])\n",
    "\n",
    "my_array = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.int32)\n",
    "total = np.zeros(1, dtype=np.int32)\n",
    "nblock = int(size / nthread)\n",
    "add_atomic[nblock, nthread](my_array, total)\n",
    "print(\"Atomic:\", total)\n",
    "\n",
    "######################################################################################\n",
    "# Non-atomic operation example  \n",
    "size = 10\n",
    "nthread = 10\n",
    "@cuda.jit()\n",
    "def add_atomic(my_array, total):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   total[0] += my_array[tid]\n",
    "   \n",
    "\n",
    "my_array = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.int32)\n",
    "total = np.zeros(1, dtype=np.int32)\n",
    "nblock = int(size / nthread)\n",
    "add_atomic[nblock, nthread](my_array, total)\n",
    "print(\"Non atomic: \", total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Management\n",
    "\n",
    "#### Multi-GPU\n",
    "\n",
    "- During computation, the CUDA driver selects the fastest GPU as the device 0 by default. One can also select other GPUs manually or temporarily.\n",
    "\n",
    "```python\n",
    "import numba.cuda as cuda\n",
    "cuda.select_device(0)\n",
    "..............\n",
    "Cuda.close()#close device 0\n",
    "\n",
    "\n",
    "Cuda.select_device(1)\n",
    "..............\n",
    "\n",
    "Cuda.close()#close device 1\n",
    "```\n",
    "\n",
    "#### Device List\n",
    "- The Device List (**cuda.gpus**) list all the existing GPUs in a machine (workstation) and can be indexed using (**with cuda.gpus[ ]**) to ensure execution on a particluar GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n",
      "<Managed Device 0>\n",
      "gpu 0 selected\n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "\n",
    "print(cuda.gpus)\n",
    "print(cuda.gpus[0])\n",
    "\n",
    "with cuda.gpus[0]:\n",
    "    print(\"gpu 0 selected\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. CUDA Ufuncs\n",
    "\n",
    "- The CUDA ufunc supports passing intra-device arrays to reduce traffic over the PCI-express bus. \n",
    "- It also support asynchronous mode by using stream keyword.\n",
    "\n",
    "<img src=\"../images/ufunc.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from numba import vectorize\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32, float32)'],target='cuda')\n",
    "def compute(a, b):\n",
    "    return (a - b) * (a + b)\n",
    "\n",
    "N = 10000\n",
    "A = np.arange(N , dtype=np.float32)\n",
    "B = np.arange(N, dtype=np.float32)\n",
    "C = compute(A, B)\n",
    "\n",
    "print(C.reshape(100,100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device function\n",
    "\n",
    "- The CUDA device functions can only be invoked from within the device and can return a value like normal functions. The device function is usually placed before the CUDA ufunc kernel otherwise a call to the device function may not be visible inside the ufunc kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "@cuda.jit('float32(float32)', device=True, inline=True)\n",
    "def device_ufunc(c):\n",
    "   return math.sqrt(c)\n",
    "\n",
    "@vectorize(['float32(float32, float32)'],target='cuda')\n",
    "def compute(a, b):\n",
    "    c = (a - b) * (a + b)\n",
    "    return device_ufunc(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "<img src=\"../images/numba_summary1.png\"/>\n",
    "\n",
    "\n",
    "---\n",
    "<div style=\"text-align:center; color:#FF0000\"><b>Click on HPC Approach Link to view task on HPC serial code</b> </div>\n",
    "\n",
    "## 9. [HPC Approach](serial_RDF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "- Numba Documentation, Release 0.52.0-py3.7-linux-x86_64.egg, Anaconda, Nov 30, 2020.\n",
    "- Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018.\n",
    "- https://docs.nvidia.com/cuda/cuda-c-programming-guide/\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by NVIDIA Corporation under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
