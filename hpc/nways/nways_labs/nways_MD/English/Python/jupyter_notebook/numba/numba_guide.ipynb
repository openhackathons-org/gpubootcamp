{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# \n",
    "\n",
    "#  Numba Lab1: Numba For CUDA GPU\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to:**\n",
    "    -   enable you to quickly start using Numba (beginner to advanced level)\n",
    "    -   teach you to apply the concepts of CUDA GPU programming to HPC field(s); and\n",
    "    -   show you how to achieve computational speedup on GPUs to maximize the throughput of your HPC implementation.\n",
    "\n",
    "\n",
    "Before we begin, let's execute the cell below to display information about the CUDA driver and GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by clicking on it with your mouse, and pressing Ctrl-Enter, or pressing the play button in the toolbar above. You should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "##  Introduction\n",
    "\n",
    "**Numba** is a just-in-time compiler for python that converts python functions into optimized machine code at runtime. In other words, user-defined functions written in python would be run at native machine code speed. For example, a programmer can delegate functions that are computationally intensive (especially those with consecutive nested loops and arrays) within his/her code to Numba execution and gain speed up. This is achievable by placing Numba decorator at the top of a user-define function. A Numba decorator determines how a function would be compiled, and more on it would be explained later in the notebook. Numba has huge support for NumPy library and also enables parallel programming on `CPU (multicore) and GPU (via CUDA API binding)`, thus making execution on NumPy arrays faster. Our focus would be using `Numba for CUDA GPU`, therefore, parallel programming concept would be described from CUDA C kernel perspective. The rest of this notebook would include frequently use terms like **Host** (this refers to a CPU), **Device** (means a GPU), and **Kernel** (a user-defined function that runs on the GPU with Numba decorator specified at the top).\n",
    "\n",
    "## Memory Architecture\n",
    "\n",
    "<p>When written codes run on the Device (GPU), execution is shared amongst threads and blocks memory space. The execution could be mapped to thousands of threads modelled in blocks and grids form. This idea is illustrated in figure 1.0 with a view that a thread can be seen as a single unit on the Device.  A thread block (also known as a block) is as collection of threads that can communicate, while a collection of these blocks is referred to as a Grid. In several devices the maximum number of threads within a thread block could be 1024 and 65535 blocks within a grid.</p>\n",
    "\n",
    " <img src=\"../images/thread_blocks.JPG\" height=\"720px\" width=\"640px\"/> \n",
    " <div style=\"margin-left:300px\">Figure 1.0. Thread, block, and grid concept </div> \n",
    " \n",
    " <p>As shown in figure 2.0, the GPU memory space is hierarchically arranged into share memory, local memory, global memory, constant memory, and texture memory. Currently, Numba does not implement texture memory CUDA features. Within a block, each thread has its own local memory and register and does communicate with other threads using the shared memory. </p>\n",
    " \n",
    " <img src=\"../images/memory_architecture.png\" height=\"512px\" width=\"600px\"/> \n",
    " <div style=\"margin-left:370px\">Figure 2.0. Memory Architecture</div> \n",
    " \n",
    " ***Image source***: <i>Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018</i>.\n",
    " \n",
    "\n",
    "## Numba CUDA Kernel\n",
    "\n",
    "This section highlights steps in writing your first CUDA Kernel in Numba. The steps would be illustrated using a simple task as follows:\n",
    "\n",
    "**Task 1**: Add two arrays A and B together and stores the result in array C. The size of A and B is 10,000.\n",
    "\n",
    "#### Step 1: \n",
    " - First, `import numba.cuda as cuda` library at the top of your notebook in order to access `cuda.jit`. \n",
    " - Next, write an empty python function and specify `@cuda.jit` at the top of the function. An example is given below:\n",
    "\n",
    "\n",
    "```python\n",
    "import numba.cuda as cuda\n",
    "@cuda.jit\n",
    "def <function_name>(<arguments>):\n",
    "    #...code body ...\n",
    "```\n",
    "\n",
    " - **Write code body**: To successfully write the kernel code body, we must understand that computation within CUDA kernels execute in thread blocks and grids such that values in input array are accessed using thread id as index. Therefore, it is important to uniquely identify distinct threads. A simple illustration on how to estimate thread id(s) is given in figure 3.0 using four blocks of threads stacked over each other to form a matrix in row column arrangement. Thread id(s) are calculated in x-dimension (ideally thread block are in x,y,z diamensions) by viewing the stack as a single row and then apply the statement below:\n",
    "\n",
    "```python\n",
    "tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "```\n",
    "<img src=\"../images/thread_position.png\" height=\"400px\" width=\"400px\"/>\n",
    "<div style=\"margin-left:300px\">Figure 3.0 Estimating thread id for threads in green and orange</div>\n",
    "\n",
    "Now that we know how to compute thread ids, we can proceed to write the kernel body code as follows:  \n",
    "\n",
    "```python\n",
    "import numba.cuda as cuda\n",
    "N = 10000 #initialize array size\n",
    "@cuda.jit\n",
    "def Add_arrays(d_A, d_B, d_C):\n",
    "\ttid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if tid < N: #ensuring that index is not out of bound\n",
    "        d_C[tid] = d_A[tid] + d_B[tid]\n",
    "\n",
    "```\n",
    "\n",
    "Note that kernel function does not return value through variable, it is stream/copy back from the Device to Host.\n",
    "\n",
    "#### Step 2:\n",
    "\n",
    "- **Write the Host code**- The first thing to do is to initialize your input arrays as follows:\n",
    " \n",
    "```python\n",
    "import numpy as np\n",
    "h_A = np.arange(N, dtype=np.int32)\n",
    "h_B = np.arange(N, dtype=np.int32)\n",
    "h_C = np.zeros(N, dtype=np.int32) #initialize zero filled array\n",
    "````\n",
    "Next is data transfer by copying data (input array) from the Host to the Device using `cuda.to_device()` function. \n",
    " \n",
    "```python\n",
    "d_A = cuda.to_device(h_A)\n",
    "d_B = cuda.to_device(h_B)\n",
    "d_C = cuda.to_device(h_C)\n",
    "\n",
    "```\n",
    "Note that kernel written in Numba automatically has direct access to NumPy arrays residing on the Host and its implies not explicitly stating ```cuda.to_device()``` for a NumPy array to be visible on the Device. Consequently, unneeded data from the Device may be visible(streamed) to Host consuming memory space unnecessarily.\n",
    "\n",
    "#### Step 3: \n",
    "\n",
    "With the existence of on the Device and visibility to the Kernel, the next step would be to call the kernel function from the Host. But before that, it is important to initialize the number of threads that would make up a single block (thread block) so that number of blocks in a grid required to execute `Add_array` Kernel can be estimated. In Numba, Kernel calls has a definition pattern stated as follows:\n",
    "\n",
    "`<kernel_name> [ <num_of_blocks_per_grid>, <num_of_threads_per_block>] (<arguments>)`\n",
    "\n",
    "The total number of threads required is equivalent to the size of our array which is `10,000`, therefore:\n",
    "\n",
    "```python \n",
    "num_of_threads_per_block = 256 # this has not exceed the limit i.e < 1024 \n",
    "```\n",
    "\n",
    "Then, `num_of_blocks_per_grid` can be estimated as:\n",
    "\n",
    "```python \n",
    "num_of_blocks_per_grid = math.ceil(N / num_of_threads_per_block)\n",
    "```\n",
    "\n",
    "Subsequently, Add_arrays Kernel function can be called this way:\n",
    "\n",
    "`Add_arrays[num_of_blocks_per_grid , num_of_threads_per_block](d_A, d_B, d_C)`\n",
    "\n",
    "#### Step 4:\n",
    "\n",
    "Copy result from Device to Host using `copy_to_host()` function, thus:\n",
    "\n",
    "```python\n",
    "h_C = d_C.copy_to_host()\n",
    "````\n",
    "Thereafter, the value of `h_c` is printed to the console. You can run the entire code in the below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "N = 10000 #initialize array size\n",
    "#kernel function\n",
    "@cuda.jit\n",
    "def Add_arrays(d_A, d_B, d_C):\n",
    "    tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if tid < N: #ensuring that index is not out of bound\n",
    "        d_C[tid] = d_A[tid] + d_B[tid]\n",
    "        \n",
    "#input data initialzed on the Host\n",
    "h_A = np.arange(N, dtype=np.int32)\n",
    "h_B = np.arange(N, dtype=np.int32)\n",
    "h_C = np.zeros(N, dtype=np.int32) # initialize zero filled array\n",
    "\n",
    "#input data copied to Device\n",
    "d_A = cuda.to_device(h_A)\n",
    "d_B = cuda.to_device(h_B)\n",
    "d_C = cuda.to_device(h_C)\n",
    "\n",
    "#set block and grid size and call kernel\n",
    "num_of_threads_per_block = 256 # this has no exceed the limit i.e < 1024\n",
    "num_of_blocks_per_grid = math.ceil(N / num_of_threads_per_block)\n",
    "Add_arrays[num_of_blocks_per_grid , num_of_threads_per_block](d_A, d_B, d_C)\n",
    "\n",
    "#copy result back to Host\n",
    "h_C = d_C.copy_to_host()\n",
    "print(\"h_C :\", h_C)\n",
    "\n",
    "####################################################################\n",
    "#expected output: h_C : [    0     2     4 ... 19994 19996 19998]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 1**: Follow the steps highlighted above and write a Numba program (Host & Device code) that adds two arrays and stores the result in a third array. The size of each array is 500,000. Execute this task in the cell below:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#import library\n",
    "#import library\n",
    "\n",
    "N = 500000 #initialize array size\n",
    "\n",
    "#kernel function\n",
    "def Addition_kernel(A, B, C):\n",
    "    #write kernel code body\n",
    "\n",
    "\n",
    "########## Host code body #########\n",
    "\n",
    "#input data initialzed on the Host\n",
    "\n",
    "\n",
    "\n",
    "#input data copied to Device\n",
    "\n",
    "\n",
    "\n",
    "#set block and grid size and call kernel\n",
    "\n",
    "\n",
    "\n",
    "#copy result back to Host\n",
    "\n",
    "\n",
    "\n",
    "################### expected output #################################\n",
    "# [     0      2      4 ... 999994 999996 999998]\n",
    "######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread Reuse\n",
    "\n",
    "It is possible to specify a few numbers of thread for a large data in a way that threads are reused to complete the computation of the entire data. This is one of the approaches used when data to be computed is larger than the maximum number of threads available in the device memory. The statement below is used in a `while loop` to achieve such purpose: \n",
    "\n",
    "```python \n",
    "tid += cuda.blockDim.x * cuda.gridDim.x\n",
    "```\n",
    "The sample code given below illustrates thread reuse using task 1 as a case study. In the example, number of blocks per grid is set to 1 on purpose to show the possibility of this approach. Therefore, a single block of thread having 256 threads would be reused to compute addition operation on two arrays, each of size 10,000. \n",
    "\n",
    "```python\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "N = 10000 #initialize array size\n",
    "#kernel function\n",
    "@cuda.jit\n",
    "def arrayAdd(d_A, d_B, d_C):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   while tid < N:\n",
    "      array_out[tid] = array_A[tid] + array_B[tid]\n",
    "      tid += cuda.blockDim.x * cuda.gridDim.x\n",
    "        \n",
    "################################################\n",
    "#set block and grid size and call kernel\n",
    "num_of_threads_per_block = 256\n",
    "num_of_blocks_per_grid = 1\n",
    "arrayAdd[num_of_blocks_per_grid, threadsperblock](d_A, d_B, d_C)\n",
    "\n",
    "print(array_out)\n",
    "\n",
    "#output: [    0     2     4 ... 19994 19996 19998]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Dimensional Array\n",
    "\n",
    "In this section, focus would be on performing simple calculation with 2D arrays. In two different approaches, let’s consider multiplication of `4x4` arrays A and B. In the first approach, each array is split into `2x2` segments in a way that threads per block and blocks per grid would be `2x2` respectively.  In the second approach, each array would fit into a thread block as `4x4` and block per grid would be `1x1(1 grid)`. This process is exemplified in figure 4.0 using small array size for ease of understanding. It is assumed that the mathematical process of multiply two matrixes is already known, however, a glimpse on process is shown figure 5.0.\n",
    "\n",
    "<img src=\"../images/2d_array.png\" height=\"512px\" width=\"600px\"/>\n",
    "<div style=\"margin-left:300px\">Figure 4.0 2D array device fitting logic</div>\n",
    "<br/>\n",
    "<img src=\"../images/2d_col_mult.png\" height=\"400px\" width=\"400px\"/>\n",
    "<div style=\"margin-left:330px\">Figure 5.0 Matrix multiplication logic</div>\n",
    "\n",
    "**Implementation:** The 4 basic steps that were previously explained would be followed.\n",
    "\n",
    "#### Step 1\n",
    "```python\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N = 4 #initialize array size\n",
    "\n",
    "@cuda.jit()\n",
    "def MatrixMul2D(d_A, d_B, d_C):\n",
    "   row, col = cuda.grid(2)\n",
    "   if row < d_C.shape[0] and col < d_C.shape[1]:\n",
    "      for k in range(N):\n",
    "         d_C[row][col]+= d_A[row][k] * d_B[k][col]\n",
    "```\n",
    "#### step 2\n",
    "```python\n",
    "#input data initialzed on the Host \n",
    "h_A = np.array([[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]], dtype=np.int32)\n",
    "h_B = np.array([[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]], dtype=np.int32)\n",
    "h_C = np.zeros(N*N, dtype=np.int32).reshape(N, N)\n",
    "\n",
    "#input data copied to Device\n",
    "d_A = cuda.to_device(h_A)\n",
    "d_B = cuda.to_device(h_B)\n",
    "d_C = cuda.to_device(h_C)\n",
    "\n",
    "```\n",
    "#### step 3\n",
    "```python\n",
    "#set block and grid size and call kernel\n",
    "num_of_threads_per_block  = (2,2)\n",
    "num_of_blocks_per_grid_x  = (math.ceil( N / num_of_threads_per_block[0]))\n",
    "num_of_blocks_per_grid_y  = (math.ceil( N / num_of_threads_per_block[1]))\n",
    "num_of_blocks_per_grid    = (num_of_blocks_per_grid_x , num_of_blocks_per_grid_y)\n",
    "\n",
    "MatrixMul2D[num_of_blocks_per_grid, num_of_threads_per_block](d_A, d_B, d_C)\n",
    "```\n",
    "#### Step 4\n",
    "```python\n",
    "h_C = d_C.copy_to_host()\n",
    "\n",
    "print(\"h_A:\\n {}\\n\".format(h_A))\n",
    "print(\"h_B:\\n {}\\n\".format(h_B))\n",
    "print(\"h_C:\\n {}\".format(h_C))\n",
    "\n",
    "```\n",
    "---\n",
    "**Exercise 2**: Write a Numba program (Host & Device) that multiply two matrixes of `225 x 225` dimensions. Part of the code has been written for you in the cell below and you are to complete the rest. In the solution, the intention is to split each matrix into `25x25` segments such that threads per block and blocks per grid would be `25x25` respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy \n",
    "#import library\n",
    "\n",
    "N = 225 #initialize array size\n",
    "\n",
    "#kernel function\n",
    "def MatrixMul2D(d_A, d_B, d_C):\n",
    "  x, y = cuda.grid(2)\n",
    "  #complete kernel code\n",
    "\n",
    "\n",
    "#input data initialized on the Host\n",
    "h_A = np.arange((N*N), dtype=np.int32).reshape(N,N)\n",
    "h_B = np.arange((N*N), dtype=np.int32).reshape(N,N)\n",
    "h_C = np.zeros((N*N), dtype=np.int32).reshape(N,N)\n",
    "\n",
    "\n",
    "#input data copied to Device\n",
    "\n",
    "\n",
    "\n",
    "#set block and grid size and call kernel\n",
    "num_of_threads_per_block = (25,25)\n",
    "\n",
    "\n",
    "\n",
    "#copy result back to Host\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################### expected output #########################################\n",
    "\n",
    "#[[  848610000   848635200   848660400 ...   854204400   854229600 854254800]\n",
    "# [ 2124360000  2124435825  2124511650 ...  2141193150  2141268975 2141344800]\n",
    "# [ -894857296  -894730846  -894604396 ...  -866785396  -866658946 -866532496]\n",
    "# ...\n",
    "# [  597268464   608532414   619796364 ... -1197101932 -1185837982 -1174574032]\n",
    "# [ 1873018464  1884333039  1895647614 ...    89886818   101201393 112515968]\n",
    "# [-1146198832 -1134833632 -1123468432 ...  1376875568  1388240768 1399605968]]\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Universal Functions (Ufuncs) and Device Function\n",
    "\n",
    "Ufuncs are NumPy functions used to implement vectorization on ndarray. Vectorization supports broadcasting mechanism that eliminates the use of loop(s) when operating on ndarrays and as a result, execution is faster. Numba implements CUDA Ufuncs using `@vectorize()` decorator as shown in figure 6.0. Please note that the arguments are not limited to just two but depends on the task to solve.\n",
    "\n",
    "<img src=\"../images/ufunc.png\" />\n",
    "<div style=\"margin-left:350px\">Figure 6.0   Ufuncs definition</div>\n",
    "\n",
    "**Task 2**- Addition of two squares:  $a^2$+ $b^2$= $(a-b)^2$ + 2ab.\n",
    "\n",
    "**Solution**:\n",
    "```python\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def additionOfSquares(a, b):\n",
    "    return (a - b)**2 + (2*a*b)\n",
    "\n",
    "N = 10000\n",
    "# prepare the input\n",
    "A = np.arange(N , dtype=np.float32)\n",
    "B = np.arange(N, dtype=np.float32)\n",
    "\n",
    "# calling ufuncs\n",
    "C = additionOfSquares(A, B)\n",
    "print(C.reshape(100,100)) # print result\n",
    "\n",
    "#expected result:\n",
    "...\n",
    "[1.8818000e+08 1.8821880e+08 1.8825760e+08 ... 1.9196242e+08 1.9200160e+08 1.9204080e+08]\n",
    "[1.9208000e+08 1.9211920e+08 1.9215840e+08 ... 1.9590122e+08 1.9594080e+08 1.9598040e+08]\n",
    "[1.9602000e+08 1.9605960e+08 1.9609920e+08 ... 1.9988002e+08 1.9992000e+08 1.9996000e+08]]\n",
    "```\n",
    "---\n",
    "**Exercise 3**: Write a CUDA ufunc to solve difference of two squares $a^2$- $b^2$ = (a-b)(a+b)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write Ufunc here\n",
    "\n",
    "\n",
    "N = 10000\n",
    "# prepare the input\n",
    "A = np.arange(N , dtype=np.float32)\n",
    "B = np.arange(N, dtype=np.float32)\n",
    "\n",
    "# call Ufunc here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA device functions can only be invoked from a Kernel function (not from Host) and would return value like Python functions. The device function is usually placed before the CUDA ufunc kernel otherwise a call to it may not be visible in the ufunc kernel. The attributes `device=True` and `inline=true` indicates `device_ufunc` as a device function. Solution for addition of squares is be rewritten as follows:\n",
    "\n",
    "```python\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@cuda.jit('float32(float32,float32)', device=True, inline=True)\n",
    "def device_ufunc(a,b):\n",
    "   return (a - b)**2\n",
    "\n",
    "@vectorize(['float32(float32, float32)'], target='cuda')\n",
    "def additionOfSquares(a, b):\n",
    "    return device_ufunc(a,b) + (2*a*b)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic Operation\n",
    "\n",
    "Atomic operation is required when multiple threads attempt to modify a common portion of the memory. Typical example includes simultaneous withdrawal from a bank account through ATM machines or many threads modifying a particular index of an array based on certain condition(s). In parallel execution, atomic operation helps eliminate race conditions that often occur in share resources. List of presently implemented atomic operations supported in Numba are:\n",
    "\n",
    "    - cuda.atomic.add(array, index, value)\n",
    "    - cuda.atomic.min(array, index, value)\n",
    "\t- cuda.atomic.max(array, index, value)\n",
    "\t- cuda.atomic.nanmax(array, index, value)\n",
    "\t- cuda.atomic.nanmin(array, index, value)\n",
    "    - cuda.atomic.compare_and_swap(array, old_value, current_value)\n",
    "\t- cuda.atomic.sub(array, index, value)\n",
    "\n",
    "Complete list can be found here: https://numba.pydata.org/numba-doc/dev/cuda/intrinsics.html#\n",
    "\n",
    "In the cell below are two examples that sum elements of an array together in parallel. The first uses atomic operation approach and gives correct answer while the other does not, therefore gives incorrect result.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task ==> sum of an array: [1,2,3,4,5,6,7,8,9,10] in parallel\n",
    "# Note that threads are executed randomly\n",
    "\n",
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "# atomic operation example \n",
    "size = 10\n",
    "nthread = 10\n",
    "@cuda.jit()\n",
    "def add_atomic(my_array, total):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   cuda.atomic.add(total,0, my_array[tid])\n",
    "\n",
    "my_array = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.int32)\n",
    "total = np.zeros(1, dtype=np.int32)\n",
    "nblock = int(size / nthread)\n",
    "add_atomic[nblock, nthread](my_array, total)\n",
    "print(\"Atomic:\", total)\n",
    "\n",
    "######################################################################################\n",
    "# Non-atomic operation example  \n",
    "size = 10\n",
    "nthread = 10\n",
    "@cuda.jit()\n",
    "def add_non_atomic(my_array, total):\n",
    "   tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "   total[0] += my_array[tid]\n",
    "   \n",
    "\n",
    "my_array = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.int32)\n",
    "total = np.zeros(1, dtype=np.int32)\n",
    "nblock = int(size / nthread)\n",
    "add_non_atomic[nblock, nthread](my_array, total)\n",
    "print(\"Non atomic: \", total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the atomic operation sample code, `cuda.to_device()` and `copy_to_host()` were purposely not used in order to show that NumPy arrays are visible from Host to Device and vice versa.  \n",
    "\n",
    "---\n",
    "**Exercise 4**: Write a Numba program (Host & Device) that counts prime numbers between `1 – 1,000,000`. The serial function is written for you to serve as a guide to write the corresponding Numba Kernel `count_prime_nos` and complete the rest of the Host code.\n",
    "\n",
    "---\n",
    "\n",
    "```python \n",
    "\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "\n",
    "N = 1000000\n",
    "\n",
    "@njit()\n",
    "def prime_nos_count_serial(num, counter):\n",
    "    for i in num:\n",
    "        if i > 1:\n",
    "            track = 0\n",
    "            for j in range(2, i):\n",
    "                if (i % j) == 0:\n",
    "                    track += 1\n",
    "                    break\n",
    "            if track == 0:\n",
    "                counter[0] += 1\n",
    "                #print(i)\n",
    "    return counter\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#kernel function\n",
    "@cuda.jit()\n",
    "def count_prime_nos(num_list, counter):\n",
    "    ##complete the code \n",
    "    \n",
    "   cuda.atomic.add(counter, 0, 1)\n",
    "\n",
    "   #counter[0] += 1\n",
    "\n",
    "\n",
    "    \n",
    "#input data initialized on the Host\n",
    "counter = np.zeros(1, dtype=np.int32)\n",
    "num_list = np.arange(N, dtype=np.int32)\n",
    "##prime_nos_count_serial(num_list, counter)\n",
    "\n",
    "\n",
    "#set block and grid size and call kernel\n",
    "threads_in_block = 512\n",
    "##complete the rest\n",
    "\n",
    "\n",
    "print(counter)\n",
    "\n",
    "################# expected output #########\n",
    "# [78498]\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "<img src=\"../images/numba_summary1.png\"/>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Task\n",
    "\n",
    "In this section, you are expected to click on the **Serial Code Lab Assignment** link and proceed to Lab 2. In this lab you will find three python serial code functions. You are required to revise the **pair_gpu** function to run on the GPU, and likewise do a few modifications within the **main** function.\n",
    "\n",
    "## <div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Serial Code Lab Assignment](serial_RDF.ipynb) </b> </div>\n",
    "\n",
    "---\n",
    "\n",
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, we recommend you go to your browser's File menu (not the Jupyter notebook file menu) and save the complete web page. This will ensure the images are copied as well. You can also execute the following cell block to create a zip-file of the files you've been working on and download it with the link below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -f nways_files.zip\n",
    "zip -r nways_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**After** executing the above zip command, you should be able to download and save the zip file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](../nways_files.zip).\n",
    "\n",
    "**IMPORTANT**: Please click on **HOME** to go back to the main notebook for *N ways of GPU programming for MD* code.\n",
    "\n",
    "---\n",
    "\n",
    "# <p style=\"text-align:center;border:3px; border-style:solid; border-color:#FF0000  ; padding: 1em\"> <a href=../../../nways_MD_start_python.ipynb>HOME</a></p>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Links and Resources\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download the latest version Nsight System from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "- Numba Documentation, Release 0.52.0-py3.7-linux-x86_64.egg, Anaconda, Nov 30, 2020.\n",
    "- Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018.\n",
    "- https://docs.nvidia.com/cuda/cuda-c-programming-guide/\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0  International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
