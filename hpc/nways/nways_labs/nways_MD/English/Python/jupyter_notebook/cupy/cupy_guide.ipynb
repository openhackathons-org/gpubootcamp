{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \n",
    "\n",
    "#  CuPy Lab 1\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to:**\n",
    "     - enable you to quickly start using CuPy (beginner to intermediate level)\n",
    "     - teach you to apply the concepts of GPU programming to HPC field(s); and\n",
    "     - show you how to achieve a computational speedup on the GPU to maximize the throughput of your HPC implementation.\n",
    "\n",
    "Before we begin, let's execute the cell below to display information about the CUDA driver and GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by clicking on it with your mouse, and pressing Ctrl-Enter, or pressing the play button in the toolbar above. You should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Introduction\n",
    "CuPy is an open-source library that implements GPU-accelerated NumPy arrays on CUDA. CuPy represents a GPU version of NumPy. NumPy runs only on CPU cores while CuPy leverages on multiple CUDA cores for parallel execution, therefore, CuPy is considered to run fastest and delivers maximum speed up. Due to the NumPy-compatibility nature of CuPy, almost all NumPy functionalities including multi-dimensional arrays and data types are implemented by CuPy. The rest of this notebook includes simple illustration on `CuPy architecture`, `CuPy fundamentals`, `CuPy CUDA kernels` and, frequently use terms like `Host` (this refers to a CPU), `Device` (means a GPU), and `Kernel` (a CuPy user-defined function that runs on the GPU).\n",
    "   \n",
    "   \n",
    "## CuPy Architecture\n",
    "The CuPy architecture exposes functionalities within the CuPy API that allows developers (or users) to create a user-defined CUDA kernel and make use of deep neural network utility through the `cuDNN` functionality. Linear algebras are solved through `cuBLAS` while systems of equations are solved with `cuSOLVER`. The `cuSPARSE` and `cuTENSOR` API functions specifically target sparse matrix and tensor operations respectively. Random numbers are generated using `cuRAND`. Sort, Scan and Reduction operations are conveniently executed using `CUB` and `Thrust`. Furthermore, Multi-GPU data transfer tasks are initiated with `NCCL` functionality. It is important to know that all these API functionalities rely on `CUDA`, while the CUDA itself depends on the `NVIDIA GPU` as shown in figure 1.0.\n",
    "<center><img src=\"../images/cupy_arch.png\" height=\"416px\" width=\"506px\"></center>\n",
    "<center><div>Figure 1.0 CuPy Architecture</div></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CuPy Fundamentals\n",
    "\n",
    "In this section, three frequently used CuPy paradigm namely variable initialization, data transfer, and device selection would be considered. \n",
    "\n",
    "- **Variable or data initialization**: This is the process of assigning data or value to CuPy ndarray. The first step is to import the CuPy library and then initialize variables with data type as follows:\n",
    "```python \n",
    "import cupy as cp\n",
    "X1 = cp.array([1,2,3,4,5,6,7,8,9,10], dtype=cp.int32)#array of 10 values\n",
    "X2 = cp.arange(100, dtype=cp.float32)#generating array of 100 values \n",
    "X3 = cp.empty((3,3), dtype=cp.float32)#initializing empty 2D array of 3X3 matrix\n",
    "Sizebin = 10000\n",
    "X4 = cp.zeros(sizebin, dtype=cp.int64)#initializing array filled with 10,000 zeros\n",
    "```\n",
    "\n",
    "\n",
    "- **Data transfer**: The idea is to move or copy data from the Host (NumPy) to the Device (CuPy) and vice versa such that data is visible to the Kernel and the resulting output would be copied back to the Host.\n",
    "```python\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "#copy data from Host to Device using cp.asarray()\n",
    "h_X = np.arange(100, dtype=np.float32)#generating array of 100 values on the Host with NumPy\n",
    "d_X = cp.asarray(x)# copy data to Device \n",
    "#copy data from Device to Host using cp.asnumpy()\n",
    "h_X = cp.asnumpy(d_X)\n",
    "```\n",
    "\n",
    "\n",
    "- **Device selection**: This is a mechanism used by CuPy to select a particular GPU or switch from one Device to another (when there are more than one Device, default device is given 0 index id).\n",
    "```python\n",
    "Using default Device\n",
    "X1 = cp.array([1,2,3,4,5,6,7,8,9,10], dtype=cp.int32)\n",
    "```\n",
    "Switching Devices\n",
    "```python\n",
    "cp.cuda.Device(1)\n",
    "X1 = cp.array([1,2,3,4,5,6,7,8,9,10], dtype=cp.int32)\n",
    "```\n",
    "\n",
    "\n",
    "Switch GPU temporarily to GPU index 2 (minimum of 3 GPUs must exist to use index 2)\n",
    "```python\n",
    "with cp.cuda.Device(2): \n",
    "\tX2 = cp.arange(100, dtype=cp.float32)\n",
    "```\n",
    "```python\n",
    "Sizebin = 10000\n",
    "X4 = cp.zeros(sizebin, dtype=cp.int64)# back to default GPU with index 0 \n",
    "```\n",
    "\n",
    "Having establish some basic steps, let‚Äôs consider example 1.\n",
    "\n",
    "**Example 1**: *Write a CuPy program that adds two arrays A and B and store the result in array C. Assume that A and B have 10,000 elements each*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "N = 10000\n",
    "#select Device with index 1. \n",
    "with cp.cuda.Device(1):\n",
    "    #input data initialzed\n",
    "    d_A = cp.arange(N, dtype=cp.int32)\n",
    "    d_B = cp.arange(N, dtype=cp.int32)\n",
    "    d_C = cp.zeros(N, dtype=cp.int32) # initialize zero filled array\n",
    "    d_C = d_A + d_B\n",
    "\n",
    "#optional: copy result from Device to Host \n",
    "h_C = cp.asnumpy(d_C)\n",
    "print(h_C)\n",
    "#expected output: [    0     2     4 ... 19994 19996 19998]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 1**: *Follow the steps highlighted above and write a CuPy program to add two arrays. The size of each array is 500,000*. Execute this task in the cell below: \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#expected output: [     0      2      4 ... 999994 999996 999998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Dimensional Array\n",
    "\n",
    "In this section, the focus would be on performing simple calculation with 2D arrays. 2D arrays are usually in matrix form and matrix-matrix multiplication operation can be perform on them using CuPy `SGEMM` (Single precision GEneral Matrix Multiplication) and `DGEMM` (Double precision GEneral Matrix Multiplication). Let‚Äôs consider two examples of matrix multiplication. First example would be a simple mathematically verifiable `4x4` matrixes `A and B` as shown in figure 2.0, while the second example is a large matrixes `d_A and d_B` of shape `10,000x10,000`. The latter example would use `cuRAND` to randomly generate values for `d_A & d _B` on the Device and python matrix operator `@` based on `cuBLAS` to perform matrix multiplication. \n",
    "\n",
    "**Example 2**:  Multiplication of matrix A & B using `cp.dot()` and `@`. \n",
    "\n",
    "<center><img src=\"../images/matrix.png\" height=\"600px\" width=\"506px\"></center>\n",
    "<center><div>Figure 2.0 Matrix A & B multiplication</div></center> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "A = cp.array([[0,0,0,0],[1,1,1,1],[2,2,2,2],[3,3,3,3]],dtype=cp.int32)\n",
    "B = cp.array([[0,1,2,3],[0,1,2,3],[0,1,2,3],[0,1,2,3]],dtype=cp.int32)\n",
    "\n",
    "C = cp.dot(A,B)\n",
    "C2 = A@B\n",
    "print(\"dot ops:\", C)\n",
    "print(\"@ ops:\", C2)\n",
    "\n",
    "#expected output\n",
    "#dot ops: \n",
    "#[[ 0  0  0  0]\n",
    "# [ 0  4  8 12]\n",
    "# [ 0  8 16 24]\n",
    "# [ 0 12 24 36]]\n",
    "#@ ops: \n",
    "#[[ 0  0  0  0]\n",
    "# [ 0  4  8 12]\n",
    "# [ 0  8 16 24]\n",
    "# [ 0 12 24 36]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**:  Multiply matrixes d_A and d_B using Python matrix operator `@`. \n",
    "\n",
    "- **Step 1**: initialize matrix size (assume the two matrixes have equal rows and columns)\n",
    "```python\n",
    "import cupy as cp\n",
    "N = 10000\n",
    "```\n",
    "\n",
    "\n",
    "- **Step 2**: Fetch or generate matrix values. Matrixes d_A and d_B would be generated using `cuRAND`\n",
    "```python\n",
    "d_A = cp.random.random((N,N), dtype=cp.float32)\n",
    "d_B = cp.random.random(N*N, dtype=cp.float32).reshape(N, N)\n",
    "```\n",
    "\n",
    "\n",
    "- **Step 3**: Apply Python matrix operator `@` that uses `cuBLAS`  \n",
    "```python\n",
    "d_C = d_A@d_B\n",
    "print(d_C)\n",
    "#expected output\n",
    "...\n",
    "[2496.929  2493.3096 2512.024  ... 2523.2388 2486.2688 2502.8193]\n",
    "[2512.366  2522.0713 2518.3489 ... 2529.164  2493.486  2488.1067]\n",
    "[2493.215  2483.601  2493.606  ... 2523.578  2474.8271 2469.6057]]\n",
    "```\n",
    "\n",
    "---\n",
    "**Exercise 2**: *Write a CuPy program that multiply two matrixes of dimensions 225 x 225. Part of the code has been written for you in the cell below and you are to complete the rest*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "N = 225\n",
    "\n",
    "#generate matrix \n",
    "\n",
    "\n",
    "\n",
    "#apply matrix operator @ or cp.dot()\n",
    "\n",
    "\n",
    "\n",
    "#expected output:\n",
    "#[[  848610000   848635200   848660400 ...   854204400   854229600 854254800]\n",
    "# [ 2124360000  2124435825  2124511650 ...  2141193150  2141268975 2141344800]\n",
    "# [ -894857296  -894730846  -894604396 ...  -866785396  -866658946 -866532496]\n",
    "# ...\n",
    "# [  597268464   608532414   619796364 ... -1197101932 -1185837982 -1174574032]\n",
    "# [ 1873018464  1884333039  1895647614 ...    89886818   101201393 112515968]\n",
    "# [-1146198832 -1134833632 -1123468432 ...  1376875568  1388240768 1399605968]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Fusion\n",
    "\n",
    "Kernel fusion is all about fusing functions and it is defined by specifying a decorator `@cp.fuse()` at the top of a user-defined function. Kernel fusion creates and caches the CUDA kernel on it first call in a way that subsequent calls with the same input type are executed on the cached kernel, hence more speed up is gained.\n",
    "\n",
    "```python\n",
    "@cp.fuse(kernel_name='<function_name>')\n",
    "def function_name(<arguments>):\n",
    "  #<body code> \n",
    "```\n",
    "or as\n",
    "\n",
    "```python\n",
    "@cp.fuse()\n",
    "def function_name(<arguments>):\n",
    "  #<body code> \n",
    "```\n",
    "\n",
    "**Example 4**: compute  z = $‚àë_{ùëñ=1}$ $ùë•_{ùëñ}$ * $ùë§_{ùëñ}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "@cp.fuse()\n",
    "def compute(x,w):\n",
    "    return cp.sum(x * w)\n",
    "\n",
    "N = 225\n",
    "#input data\n",
    "x = cp.random.random((N), dtype=cp.float32)\n",
    "w = cp.random.random((N), dtype=cp.float32)\n",
    "\n",
    "#calling fuse function\n",
    "z = compute(x,w)\n",
    "print(z)\n",
    "#expected output: 57.776024. output may varies because of random values of x & w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy CUDA Kernels\n",
    "\n",
    "CuPy CUDA kernels are user defined kernels namely:\n",
    "- Elementwise Kernels\n",
    "- Reduction Kernels\n",
    "- Raw Kernels\n",
    "\n",
    "### Elementwise Kernels\n",
    "\n",
    "The elementwise kernel class definition comprises list of input and output arguments with data types specified explicitly or in generic form`(<T>)` that follows C language style. It also includes the kernel body code that denotes computation statement and the kernel name. Note that character `i` and `n`, and variable names that begin with `‚Äú_‚Äù` are not allowed for use within the elementwise kernel definition. A stepwise example is illustrated below:\n",
    "\n",
    "**Example 5**: compute r= ‚àö($x^2$+$y^2$+$z^2$ ) \n",
    "\n",
    "**Step 1**: Set the list of input and output arguments and their data type\n",
    "```python\n",
    "input_list = 'float32 d_x, float32 d_y, float32 d_z '\n",
    "output_list = 'float32 r'\n",
    "```\n",
    "you may as well use a generic form of data type as follows:\n",
    "\n",
    "```python\n",
    "input_list = 'T d_x, T d_y, T d_z'\n",
    "output_list = 'T r'\n",
    "```\n",
    "**Step 2**: Write the kernel body code to compute the equation\n",
    "```python\n",
    "code_body = 'r = sqrt(d_x*d_x + d_y*d_y + d_z*d_z)'\n",
    "```\n",
    "**Step 3**: Define elementwise class and set the kernel name\n",
    "```python\n",
    "compute_call = cp.ElementwiseKernel(input_list, output_list, code_body,'compute')\n",
    "```\n",
    "**Step 4**: Initialize input values\n",
    "```python\n",
    "N =2000\n",
    "d_x = cp.arange(N, dtype=cp.float32)\n",
    "d_y = cp.arange(N, dtype=cp.float32)\n",
    "d_z = cp.arange(N, dtype=cp.float32)\n",
    "r = cp.empty(N, dtype=cp.float32)\n",
    "```\n",
    "\n",
    "**step 5**: Make the kernel call\n",
    "```python\n",
    "compute_call(d_x,d_y, d_z, r)\n",
    "print(r)\n",
    "#expected output: [0.0000000e+00 1.7320508e+00 3.4641016e+00 ... 3.4589055e+03 3.4606375e+03 3.4623696e+03]\n",
    "```\n",
    "You can run the above code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'float32 d_x, float32 d_y, float32 d_z '\n",
    "output_list = 'float32 r'\n",
    "code_body = 'r = sqrt(d_x*d_x + d_y*d_y + d_z*d_z)'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "compute_call = cp.ElementwiseKernel(input_list, output_list, code_body,'compute')\n",
    "# data\n",
    "N =2000\n",
    "\n",
    "d_x = cp.arange(N, dtype=cp.float32)\n",
    "d_y = cp.arange(N, dtype=cp.float32)\n",
    "d_z = cp.arange(N, dtype=cp.float32)\n",
    "r = cp.empty(N, dtype=cp.float32)\n",
    "# kernel call with argument passing\n",
    "compute_call(d_x,d_y, d_z, r)\n",
    "print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Kernels\n",
    "\n",
    "Reduction kernels is defined as follows:\n",
    "\n",
    "    - Input and output arguments with data types specified explicitly or in generic form(<T>) that follows C language style.\n",
    "    - Identity value that initialized argument to be reduced to zero. \n",
    "\t- mapping expression that maps each argument values to operands a & b and applies arithmetic operator. \n",
    "\t- reduction expression that sums operand a & b and stores the output in a, \n",
    "\t- post mapping expression that executes further operation on operand a. \n",
    "\t- kernel name\n",
    "\n",
    "For ease of understanding, `example 6` is used to exemplify reduction kernel.\n",
    "\n",
    "**Example 6**: Evaluate  z = $‚àë_{ùëñ=1}$ $ùë•_{ùëñ}$ * $ùë§_{ùëñ}$ + bais\n",
    "\n",
    "\n",
    "**Step 1**: Set the list of input and output arguments and their data type\n",
    "```python\n",
    "input_list = 'float32 x, float32 w, float32 bias'\n",
    "output_list = 'float32 y'\n",
    "```\n",
    "you may as well use a generic form of data type as follows:\n",
    "```python\n",
    "input_list = 'T x, T w, T bias'\n",
    "output_list = 'T y'\n",
    "```\n",
    "**Step 2**: set mapping expression\n",
    "```python\n",
    "mapping_expr = 'x * w'\n",
    "```\n",
    "**Step 3**: set reduction expression `a & b`\n",
    "```python\n",
    "reduction_expr= 'a + b'\n",
    "```\n",
    "**Step 4**: set post expression for `a`\n",
    "```python\n",
    "post_expr = 'y = a + bias'\n",
    "```\n",
    "**Step 5**: initialize identity value 0\n",
    "```python\n",
    "identity_value = '0'\n",
    "```\n",
    "**Step 6**: define reduction kernel class and set the kernel name\n",
    "```python\n",
    "dnnLayer = cp.ReductionKernel(\n",
    "   input_list,\n",
    "   output_list,\n",
    "   mapping_expr,\n",
    "   reduction_expr,\n",
    "   post_expr,\n",
    "   identity_value,\n",
    "   'dnnLayer')\n",
    "```\n",
    "**Step 7**: Initialize input values\n",
    "```python\n",
    "N = 2000\n",
    "x = cp.random.random(N, dtype=cp.float32)\n",
    "w = cp.random.random(N, dtype=cp.float32)\n",
    "bias = -0.01\n",
    "```\n",
    "**Step 8**: make the kernel call\n",
    "```python\n",
    "y = dnnLayer(x,w,bias)\n",
    "print(y)\n",
    "```\n",
    "You can run the above code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'float32 x, float32 w, float32 bias'\n",
    "output_list = 'float32 y'\n",
    "mapping_expr = 'x * w'\n",
    "reduction_expr= 'a + b'\n",
    "post_expr = 'y = a + bias'\n",
    "identity_value = '0'\n",
    "\n",
    "dnnLayer = cp.ReductionKernel(\n",
    "   input_list,\n",
    "   output_list,\n",
    "   mapping_expr,\n",
    "   reduction_expr,\n",
    "   post_expr,\n",
    "   identity_value,\n",
    "   'dnnLayer'  )\n",
    "\n",
    "N = 2000\n",
    "x = cp.random.random(N, dtype=cp.float32)\n",
    "w = cp.random.random(N, dtype=cp.float32)\n",
    "bias = -0.01\n",
    "\n",
    "y = dnnLayer(x,w,bias)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Kernels\n",
    "\n",
    "The CuPy Raw kernels are defined through the RawKernel object that enables the direct use of kernels from CUDA source using CUDA‚Äôs cuLaunchKernel interface. Raw Kernels are written using CUDA C paradigm therefore there is need to understand the memory architecture to know how best to manipulate threads, thread blocks and grid size. This is important to effectively write Raw Kernels that solve complex task.\n",
    "\n",
    "### Memory Architecture\n",
    "\n",
    "When written codes run on the device (GPU), execution is shared amongst threads and blocks of memory space. The execution could be mapped to thousands of threads modelled in blocks and grids form. This idea is illustrated in figure 3.0 with a view that a thread can be seen as a single executing unit on the device.  A `thread block` (also known as a block) is as collection of threads that can communicate, while a collection of these blocks is referred to as a `Grid`. In several devices the maximum number of threads within a thread block is `1,024` and `65,535` blocks within a grid.\n",
    "\n",
    "<center><img src=\"../images/thread_blocks.JPG\" height=\"620px\" width=\"540px\"/> </center>\n",
    " <center><div>Figure 3.0. Thread, block, and grid concept </div></center>\n",
    "\n",
    "\n",
    "As shown in figure 4.0, the GPU memory space is hierarchically arranged into `shared memory`, `local memory`, `global memory`, `constant memory`, and `texture memory`. Within a block, each thread has its own local memory and register and does communicate with other threads using the shared memory.\n",
    "\n",
    "<center><img src=\"../images/memory_architecture.png\" height=\"412px\" width=\"500px\"/> </center>\n",
    "<center><div>Figure 4.0. Memory Architecture</div></center>\n",
    "\n",
    "**Image source** : <i>Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018</i>.\n",
    "\n",
    "\n",
    "\n",
    "A raw kernel runs on the Device and it is defined by creating a `RawKernel` object the embeds CUDA C kernel codes. Let‚Äôs illustrate this using example 7 as follows:\n",
    "\n",
    "**Example 7**: Write a CuPy raw kernel program that adds two arrays assume that both arrays contain 10,000 elements each.\n",
    "\n",
    "**Step 1**:\n",
    "- First, import `cupy as cp` at the top of your notebook to access `RawKernel` class.\n",
    "- Next, write an empty raw kernel function enclosed in parenthesis. An example is given below:\n",
    "```python\n",
    "import cupy as cp\n",
    "add_array = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void <function_name>(<arguments>) {\n",
    "\n",
    "  <body code>\n",
    "}\n",
    "''', '<function_name>')\n",
    "```\n",
    "\n",
    "\n",
    "- **Write code body**: To successfully write the kernel code body, it is important to know that computations within CUDA kernels execute in thread blocks and grids in a way that input array elements are accessed using global thread id as index. Therefore, it is necessary to uniquely identify distinct threads. A simple illustration on how to estimate global thread `id(s)` is given in figure 5.0 using four blocks of threads stacked over each other to form a matrix in rows and columns arrangement. Global thread ids are calculated in `x-dimension` (ideally thread block are in x,y,z dimensions) by rearranging the thread blocks as single row and then estimate using statement below:\n",
    "\n",
    "```python\n",
    "tid = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "```\n",
    "<center><img src=\"../images/thread_position.png\" height=\"350px\" width=\"350px\"/></center>\n",
    "<center><div>Figure 5.0 Estimating thread id for threads in green and orange</div></center>\n",
    "\n",
    "Now that we know how to compute global thread ids, we can proceed to write the CUDA C body code within the Raw Kernel as follows:\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "N = 10000 #initialize array size\n",
    "add_array = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void addFunc(const int* d_A, const int* d_B, int* d_C ) {\n",
    " int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    " d_C[tid]= d_A[tid] + d_B[**tid];\n",
    "}\n",
    "''', 'addFunc')\n",
    "```\n",
    "\n",
    "**Step 2**:\n",
    "\n",
    "- **Write the Host code**: The first thing to do is to initialize your input arrays as follows\n",
    "```python\n",
    "import numpy as np\n",
    "h_A = np.arange(N, dtype=np.int32)\n",
    "h_B = np.arange(N, dtype=np.int32)\n",
    "```\n",
    "Do data transfer by copying data (input array) from the `Host` to the `Device` using `cp.asarray()` function.\n",
    "\n",
    "```python\n",
    "d_A = cp.asarray(h_A)\n",
    "d_B = cp.asarray(h_B)\n",
    "d_C = cp.zeros(N, dtype=cp.int32) # initialize zero filled array\n",
    "```\n",
    "\n",
    "**Step 3**:\n",
    "\n",
    "The next step is to call the raw kernel function from the Host. But before that, a vital move would be to initialize the number of threads that would make up a single block (thread block) so that number of blocks required in a grid to execute the raw kernel can be estimated. In CuPy, raw kernel calls have a definition pattern as follows:\n",
    "```python\n",
    "<raw_kernel_name>((<num_of_blocks_per_grid>),(<num_of_threads_per_block>),(<arguments>))\n",
    "```\n",
    "The total number of threads required is equivalent to the size of initialized array, which is 10,000, therefore:\n",
    "\n",
    "```python\n",
    "num_of_threads_per_block = 256 # this has not exceeded the limit i.e < 1024\n",
    "```\n",
    "Then, `num_of_blocks_per_grid` can be estimated as:\n",
    "```python\n",
    "num_of_blocks_per_grid = math.ceil (N / num_of_threads_per_block)\n",
    "```\n",
    "Subsequently, the raw kernel function is called this way:\n",
    "\n",
    "```python\n",
    "add_array((num_of_blocks_per_grid,),(num_of_threads_per_block,),(d_A, d_B, d_C))\n",
    "```\n",
    "\n",
    "**Step 4**:\n",
    "\n",
    "Copy result from Device to Host using `cp.asnumpy()` function, thus:\n",
    "\n",
    "```python\n",
    "h_C = cp.asnumpy(d_C)\n",
    "```\n",
    "\n",
    "You can run the entire code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N = 10000 #initialize array size\n",
    "add_array = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void addFunc(const int* d_A, const int* d_B, int* d_C ) {\n",
    " int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    " d_C[tid]= d_A[tid] + d_B[tid];\n",
    "\n",
    "}\n",
    "''', 'addFunc')\n",
    "\n",
    "h_A = np.arange(N, dtype=np.int32)\n",
    "h_B = np.arange(N, dtype=np.int32)\n",
    "\n",
    "d_A = cp.asarray(h_A)\n",
    "d_B = cp.asarray(h_B)\n",
    "d_C = cp.zeros(N, dtype=cp.int32) # initialize zero filled array\n",
    "\n",
    "num_of_threads_per_block = 256\n",
    "num_of_blocks_per_grid = math.ceil(N / num_of_threads_per_block)\n",
    "\n",
    "add_array((num_of_blocks_per_grid,),(num_of_threads_per_block,),(d_A, d_B, d_C))\n",
    "h_C = cp.asnumpy(d_C)\n",
    "print(h_C)\n",
    "\n",
    "#expected output: [    0     2     4 ... 19994 19996 19998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 3**: *Follow the steps highlighted above and write a CuPy Raw Kernel program that multiply two arrays and store the result in a third array. The size of each array is `500,000`. Execute this task in the cell below:*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N = 500000 #initialize array size\n",
    "\n",
    "\n",
    "\n",
    "#expected output[ 0 1 4 ... 888896841 889896836 890896833]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Modules\n",
    "\n",
    "The Raw Modules has the same procedure as the Raw Kernel. In addition to this, several CUDA C kernel functions can be included within in form of a module as the name connote. Each kernel function within the Raw Module can be accessed by instantiating the  object of `RawModule` class and a call to `get_function()` method. \n",
    "\n",
    "**Example 8:** \n",
    "(i) z = $‚àë_{ùëñ=1}$ $ùë•_{ùëñ}$ * $ùë§_{ùëñ}$      \n",
    "\n",
    "(ii) r= ‚àö($x^2$+$y^2$+$z^2$)\n",
    "\n",
    "The two tasks in `example 8` are solved using raw module approach. Kernel `sum_mul` and `compute_xyz` proffer solutions to example 8(i) and 8(ii) respectively. In the `sum_mul` kernel, `__syncthread()` was used to synchronize threads in blocks in a way that all threads within a block complete the multiplication operation before moving ahead to the sum operation. The `atomicAdd()` method helps avoid incorrect sum by preventing multiple threads from performing addition operation at same time, thus, only a single thread is allowed at a time. Note that this is not the best approach, it is however written this way to reduce complexity at this level.  \n",
    "\n",
    "```python\n",
    "raw_module_code = r'''\n",
    "extern \"C\" {\n",
    "            __global__ void sum_mul(float* d_x, float* d_w, float* d_z) \n",
    "            {\n",
    "                 float sum[2000];\n",
    "                 int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "                 sum[tid] = d_x[tid] * d_w[tid];\n",
    "                 __syncthreads();\n",
    "                 atomicAdd(d_z, sum[tid]);\n",
    "            }\n",
    "            __global__ void compute_xyz(float* x, float* y, float* z, float* r ) \n",
    "            {\n",
    "                int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "                r[tid] = sqrt(x[tid] * x[tid] + y[tid] * y[tid] + z[tid] * z[tid]) ;\n",
    "\n",
    "            }\n",
    "       }\n",
    "'''\n",
    "```\n",
    "The next step is to load the raw module by creating an object.\n",
    "```python\n",
    "#loading module through RawModule object\n",
    "raw_module_object = cp.RawModule(code = raw_module_code)\n",
    "```\n",
    "Get the kernels within the raw module through the `get_function()` method\n",
    "```python\n",
    "#acessing kernels within the Raw module\n",
    "sum_mul = raw_module_object.get_function('sum_mul')\n",
    "compute_xyz = raw_module_object.get_function('compute_xyz')\n",
    "```\n",
    "Initialize data size, thread block size and, grid size\n",
    "```python\n",
    "#data\n",
    "N = 2000 #initialize array size\n",
    "num_of_threads_per_block = 128\n",
    "num_of_blocks_per_grid = math.ceil(N / num_of_threads_per_block)\n",
    "```\n",
    "Initialize data for example 8(i) and copy data to the Device using `cp.asarray()`\n",
    "```python\n",
    "h_x = np.arange(N, dtype=np.float32)\n",
    "h_w = np.arange(N, dtype=np.float32)\n",
    "\n",
    "d_x = cp.asarray(h_x)\n",
    "d_w = cp.asarray(h_w)\n",
    "d_z = cp.zeros(1, dtype=cp.float32)# initialize zero\n",
    "```\n",
    "\n",
    "Call kernel `sum_mul` and pass the required arguments\n",
    "```python\n",
    "sum_mul((num_of_blocks_per_grid,),(num_of_threads_per_block,),(d_x, d_w, d_z))\n",
    "h_z = cp.asnumpy(d_z)\n",
    "print(\"h_z:\", h_z)\n",
    "\n",
    "verifying result\n",
    "print(\"non kernel:\", cp.sum(h_x * h_w))\n",
    "```\n",
    "Initialize data for example 8(ii) directly on the Device using `cp.arange()`\n",
    "\n",
    "```python\n",
    "x = cp.arange(N, dtype=cp.float32)\n",
    "y = cp.arange(N, dtype=cp.float32)\n",
    "z = cp.arange(N, dtype=cp.float32)\n",
    "r = cp.empty(N, dtype=cp.float32)\n",
    "```\n",
    "Call kernel `compute_xyz` and pass the required arguments \n",
    "```python\n",
    "compute_xyz((num_of_blocks_per_grid,),(num_of_threads_per_block,),(x, y, z, r))\n",
    "h_r = cp.asnumpy(r)\n",
    "print(\"h_r:\", h_r)\n",
    "\n",
    "Verifying result\n",
    "print(\"non kernel:\", cp.sqrt(x * x + y * y+ z * z ))\n",
    "#expected result: \n",
    "h_z: [2.6646702e+09]\n",
    "h_r: [0.0000000e+00 1.7320508e+00 3.4641016e+00 ... 3.4589055e+03 3.4606375e+03 3.4623696e+03]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT Kernel\n",
    "\n",
    "The JIT kernel is defined through the `cupyx.jit.rawkernel` decorator. It uses the same concept as the raw kernel but differs by using python functions rather than CUDA C kernels. The decorator the specify at the top of a python function, hence the function becomes a JIT kernel. Let‚Äôs illustrate this using `example 7` from the raw kernel section.\n",
    "\n",
    "Firstly, `import jit from cupyx` library\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "from cupyx import jit\n",
    "```\n",
    "Next, write the Jit kernel\n",
    "\n",
    "```python\n",
    "@jit.rawkernel()\n",
    "def addFunc(d_A, d_B, d_C):\n",
    "    tid = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x\n",
    "    d_C[tid] = d_A[tid] + d_B[tid]\n",
    "```\n",
    "Initialize data size, thread block size and, grid size\n",
    "\n",
    "```python\n",
    "N = 10000 #initialize array size\n",
    "num_of_threads_per_block = 128\n",
    "num_of_blocks_per_grid = math.ceil(N / num_of_threads_per_block)\n",
    "```\n",
    "Initialize data directly on the Device using `cp.arange()`\n",
    "\n",
    "```python\n",
    "d_A = cp.arange(N, dtype=cp.float32)\n",
    "d_B = cp.arange(N, dtype=cp.float32)\n",
    "d_C = cp.zeros(N, dtype=cp.int32) # initialize zero filled array\n",
    "```\n",
    "Call jit kernel `addFunc` and pass the required arguments\n",
    "```python\n",
    "addFunc((num_of_blocks_per_grid,),(num_of_threads_per_block,),(d_A, d_B, d_C))\n",
    "\n",
    "print(\"d_C:\", d_C)\n",
    "\n",
    "#expected output: d_C: [    0     2     4 ... 19994 19996 19998]\n",
    "```\n",
    "\n",
    "There are two notable actions in the code above, first, data was not initialized on the Host but directly on the Device; second, the output of the `d_C` was not copy to the Host using `asnumpy()` but was used directly on the Host. This shows data visibility, however, it is not the best data management approach in some context. Please run the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "from cupyx import jit\n",
    "import math\n",
    "\n",
    "@jit.rawkernel()\n",
    "def addFunc(d_A, d_B, d_C):\n",
    "    tid = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x\n",
    "    d_C[tid] = d_A[tid] + d_B[tid]\n",
    "\n",
    "N = 100000 #initialize array size\n",
    "num_of_threads_per_block = 128\n",
    "num_of_blocks_per_grid = math.ceil(N / num_of_threads_per_block)\n",
    "\n",
    "d_A = cp.arange(N, dtype=cp.int32)\n",
    "d_B = cp.arange(N, dtype=cp.int32)\n",
    "d_C = cp.zeros(N, dtype=cp.int32) # initialize zero filled array\n",
    "addFunc((num_of_blocks_per_grid,),(num_of_threads_per_block,),(d_A, d_B, d_C))\n",
    "print(\"d_C:\", d_C)\n",
    "\n",
    "#expected output: [    0     2     4 ... 19994 19996 19998]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise 4**: *Follow the steps highlighted above and write a CuPy Raw Kernel program that multiply two arrays and store the result in a third array. The size of each array is 500,000. Execute this task in the cell below:*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N = 500000 #initialize array size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#expected output: [  0   1  4 ... 2147483647 2147483647 2147483647]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<img src=\"../images/cupy_summary.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Task\n",
    "\n",
    "In this section, you are expected to click on the **Serial Code Lab Assignment** link and proceed to Lab 2. In this lab, you will find three python serial code functions. You are required to revise the **pair_gpu** function to run on the GPU, and likewise do a few modifications within the **main** function.\n",
    "\n",
    "## <center><div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Serial Code Lab Assignment](serial_RDF.ipynb) </b> </div></center>\n",
    "\n",
    " \n",
    "---\n",
    "\n",
    "\n",
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, we recommend you go to your browser's File menu (not the Jupyter notebook file menu) and save the complete web page. This will ensure the images are copied as well. You can also execute the following cell block to create a zip-file of the files you've been working on and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -f nways_files.zip\n",
    "zip -r nways_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**After** executing the above zip command, you should be able to download and save the zip file by holding down <mark>Shift</mark> and <mark>Right-Clicking</mark> [Here](../nways_files.zip).\n",
    "\n",
    "**IMPORTANT**: Please click on **HOME** to go back to the main notebook for *N ways of GPU programming for MD* code.\n",
    "\n",
    "---\n",
    "# <center><div style=\"text-align: center;border:3px; border-style:solid; border-color:#FF0000; padding: 1em;\"> [HOME](../../../nways_MD_start_python.ipynb)</div></center>\n",
    "\n",
    "---\n",
    "\n",
    "# Links and Resources\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download Nsight System latest version from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##  References\n",
    "- https://docs.cupy.dev/en/stable/\n",
    "- https://cupy.dev/\n",
    "- CuPy Documentation Release 8.5.0, Preferred Networks, inc. and Preferred Infrastructure inc., Feb 26, 2021.\n",
    "- Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018.\n",
    "- Crissman Loomis and Emilio Castillo, CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features, GTC Digital March, March 2020.\n",
    "- https://www.gpuhackathons.org/technical-resources\n",
    "- https://rapids.ai/start.html\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
