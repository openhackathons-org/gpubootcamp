{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# \n",
    "\n",
    "#  CuPy Lab 1\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- **The goal of this lab is to:**\n",
    "     - enable you to quickly start using CuPy (beginner to intermediate level)\n",
    "     - teach you to apply the concepts of GPU programming to HPC field(s); and\n",
    "     - show you how to achieve a computational speedup on the GPU to maximize the throughput of your HPC implementation.\n",
    "\n",
    "Before we begin, let's execute the cell below to display information about the CUDA driver and GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by clicking on it with your mouse, and pressing Ctrl-Enter, or pressing the play button in the toolbar above. You should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  What is CuPy?\n",
    "- CuPy is an implementation of NumPy-compatible multi-dimensional array on CUDA. It is an open-source array library for GPU-accelerated computing with Python.\n",
    "- CuPy consists of :\n",
    "    - cupy.ndarray \n",
    "    - the core multi-dimensional array class \n",
    "    - many functions \n",
    "- It supports a subset of numpy.ndarray interface which include:\n",
    "    - Basic and advance indexing \n",
    "    - Data types (int32, float32, uint64, complex64,... )\n",
    "    - Array manipulation routine (reshape)\n",
    "    - Linear Algebra functions (dot, matmul, etc)\n",
    "    - Reduction along axis (max, sum, argmax, etc)\n",
    "   \n",
    "   \n",
    "You can run the examples given in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "#Basic indexing and slicing\n",
    "print(X[5:])\n",
    "\n",
    "#output: [5 6 7 8 9]\n",
    "\n",
    "print(X[1:7:2])\n",
    "\n",
    "#output: [1 3 5]\n",
    "\n",
    "#reduction and Linear Algebra function\n",
    "max(X)\n",
    "#output:  9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Advance indexing\n",
    "X = np.array([[1, 2],[3, 4],[5, 6]])\n",
    "print(X[[0, 1, 2], [0, 1, 0]])\n",
    "\n",
    "#output: [1 4 5]\n",
    "\n",
    "B = np.array([1,2,3,4], dtype=np.float32)\n",
    "C = np.array([5,6,7,8], dtype=np.float32)\n",
    "print(\"matmul:\",np.matmul(B, C))\n",
    "\n",
    "#output: matmul: 70.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#data type and array manipulation routine \n",
    "A =1j*np.arange(9, dtype=np.complex64).reshape(3,3)\n",
    "print(A)\n",
    "\n",
    "\n",
    "#output:\n",
    "#[[0.+0.j 0.+1.j 0.+2.j]\n",
    "# [0.+3.j 0.+4.j 0.+5.j]\n",
    "# [0.+6.j 0.+7.j 0.+8.j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Features of CuPy\n",
    "\n",
    "- **Features of CuPy includes:**\n",
    "    - User-define elementwise CUDA kernels\n",
    "    - User-define reduction CUDA kernels\n",
    "    - Fusing CUDA kernels to optimize user-define calculation\n",
    "    - Customizable memory allocator and memory pool\n",
    "    - cuDNN utilities\n",
    "- These features  are developed to support performance\n",
    "- CuPy uses on-the-fly kernel synthesis: when a kernel call is required, it compiles a kernel code optimized for the shapes and data type objects (dtypes) of given arguments, sends it to the GPU device, and execute the kernel.\n",
    "- CuPy also caches the kernel code sent to GPU device within the process, which reduces the kernel transfer time on further calls. \n",
    "\n",
    "\n",
    "##  CuPy Architecture\n",
    "\n",
    "The CuPy architecture exposes functionalities within the CuPy API that allows developers (or users) to create a user-defined CUDA kernel, make use of DNN utility through the cuDNN functionality, solve Linear algebra through cuTENSOR, cuBLAS, cuSOLVER, and cuSPARSE API functions. Also, random numbers can be generated using cuRAND. Sort, Scan and Reduction operations are conveniently executed using CUB and Thrust.  Furthermore, Multi-GPU data transfer tasks are initiated with NCCL functionality. It is important to know that all the API functionalities rely on CUDA, while the CUDA itself runs on the NVIDIA GPU as shown in the diagram below. \n",
    "\n",
    "<img src=\"../images/cupy_arch.png\">\n",
    "\n",
    "\n",
    "## CuPy Fundamentals\n",
    "- **Cupy.ndarray**: CuPy is a GPU array backend that implements a subset of NumPy interface.\n",
    "```python\n",
    "#CuPy version\n",
    "import cupy as cp\n",
    "X_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "#NumPy version\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "```\n",
    "\n",
    "- CuPy considers the current device as the default device with device ID 0. It also allows temporary switches between GPU devices.\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "####### Current device (GPU ID: 0)##############\n",
    "gpu_0 = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Switch device\n",
    "cp.cuda.Device(1).use()\n",
    "gpu_1 = cp.array([1, 2, 3, 4])\n",
    "\n",
    "###### Switch GPU temporarily################\n",
    "import numpy as np\n",
    "with cp.cuda.Device(1):\n",
    "      gpu_1 = cp.array([1, 2, 3, 4])\n",
    "# back to device id 0\n",
    "gpu0 = cp.array([1, 2, 3, 4, 5]) \n",
    "```\n",
    "\n",
    "## Data Transfer\n",
    "- Arrays can be moved from Host to Device (CPU -> GPU) using **cupy.asarray**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "x_gpu = cp.asarray(x)\n",
    "print(x_gpu)\n",
    "\n",
    "#output: [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Device array can be move to Host(GPU -> CPU) using: **cupy.asnumpy or cupy.ndarray.get()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "x_gpu = cp.array([1, 2, 3, 4, 5])\n",
    "#copy to Host\n",
    "x_cpu = cp.asnumpy(x_gpu)\n",
    "print(\"x_cpu: \",x_cpu)\n",
    "\n",
    "#alternative option\n",
    "x_cpu_alt = x_gpu.get()\n",
    "print(\"x_cpu_alt: \",x_cpu_alt) \n",
    "\n",
    "#output: \n",
    "#x_cpu:  [1 2 3 4 5]\n",
    "#x_cpu_alt:  [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to transfer an array between devices(GPU to GPU), use **cupy.ndarray**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "with cp.cuda.Device(0):\n",
    "    x_gpu_0 = cp.ndarray([ 2, 3, 3]) \n",
    "print(\"x_gpu_0:\\n\", x_gpu_0)\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "      x_gpu_1 = cp.asarray(x_gpu_0)\n",
    "print(\"x_gpu_1:\\n\", x_gpu_1)\n",
    "\n",
    "#output\n",
    "#x_gpu_0:\n",
    "# [[[0. 0. 0.]\n",
    "#  [0. 0. 0.]\n",
    "#  [0. 0. 0.]]\n",
    "\n",
    "# [[0. 0. 0.]\n",
    "#  [0. 0. 0.]\n",
    "#  [0. 0. 0.]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU  and CPU Agnostic Code\n",
    "- The compatibility of CuPy with NumPy enables the implementation of CPU/GPU generic code using **cupy.get_array_module()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "#example: log(1 + exp(x))\n",
    "x_cpu  = np.array([1, 2, 3, 4, 5])\n",
    "x_gpu  = cp.get_array_module(x_cpu)\n",
    "result = x_gpu.maximum(0, x_cpu) + x_gpu.log1p(x_gpu.exp(-abs(x_cpu)))\n",
    "print(result)\n",
    "\n",
    "#output: [1.31326169 2.12692801 3.04858735 4.01814993 5.00671535]\n",
    "\n",
    "#An explicit conversion to a host \n",
    "x_gpu  = cp.array([6, 7, 8, 9, 10])\n",
    "result = cp.asnumpy(x_gpu) + x_cpu\n",
    "print(result)\n",
    "\n",
    "#output: [ 7  9 11 13 15]\n",
    "\n",
    "#An explicit conversion to a device\n",
    "result = x_gpu + cp.asarray(x_cpu)\n",
    "print(result)\n",
    "\n",
    "#output: [ 7  9 11 13 15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CUDA Kernels\n",
    "\n",
    "- **CUDA kernels can be defined in CuPy as follows:**\n",
    "\n",
    "    - Elementwise Kernels\n",
    "    - Reduction Kernels\n",
    "    - Raw Kernels\n",
    "    - Kernel Fusion\n",
    "- These kernels are user-defined based.\n",
    "\n",
    "### Elementwise Kernels\n",
    "- The **ElementwiseKernel** class is used to define this type of kernel.\n",
    "- This kernel consists of four parts:\n",
    "    1. an input argument list \n",
    "    2. an output argument list\n",
    "    3. a loop body code\n",
    "    4. kernel name\n",
    "- Variable names starting with “n”, “i”, or with underscore “_” are regarded as reserved keywords.\n",
    "\n",
    "#### Example: z = x*w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'float32 x , float32 w, float32 b'\n",
    "output_list = 'float32 z'\n",
    "code_body  = 'z =  (x * w) + b'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "dnnLayerNode = cp.ElementwiseKernel(input_list, output_list, code_body,'dnnLayerNode')\n",
    "\n",
    "# data\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "b = cp.array([-0.5], dtype=cp.float32)\n",
    "z = cp.empty((3,3), dtype=cp.float32)\n",
    "\n",
    "# kernel call with argument passing\n",
    "dnnLayerNode(x,w,b,z)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "# [ 8.5 15.5 24.5]\n",
    "# [35.5 48.5 63.5]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Kernel: Generic-type kernels\n",
    "- The Generic-type kernel treats a type-specifier of one character as a type-placeholder. In the sample code given in the cell below, the character **T** represents a type-placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "input_list = 'T x , T w, T b'\n",
    "output_list = 'T z'\n",
    "code_body  = 'z =  (x * w) + b'\n",
    "\n",
    "# elementwisekernel class defined\n",
    "dnnLayerNode = cp.ElementwiseKernel(input_list, output_list, code_body,'dnnLayerNode')\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "b = cp.array([-0.5], dtype=cp.float32)\n",
    "z = cp.empty((3,3), dtype=cp.float32)\n",
    "\n",
    "# kernel call with argument passing\n",
    "dnnLayerNode(x,w,b,z)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "# [ 8.5 15.5 24.5]\n",
    "# [35.5 48.5 63.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Kernels\n",
    "\n",
    "- Reduction kernel is implemented through the ReductionKernel classand contains: \n",
    "    - **Identity value**: to initialize reduction value.\n",
    "    - **Mapping expression**: used for the pre-processing of each element to be reduced.\n",
    "    - **Reduction expression**: an operator to reduce the multiple mapped values. The special variables \"**a**\" and \"**b**\" are used for its operands.\n",
    "    - **Post mapping expression**: It is used to transform the resulting reduced values. The special variable \"**a**\" is used as its input. Output should be written to the output parameter.\n",
    "\n",
    "\n",
    "Run the following example in the cell below: <b>𝑧 = $∑_{𝑖=1}$ $𝑥_{𝑖}$ * $𝑤_{𝑖}$ + 𝑏 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "dnnLayer = cp.ReductionKernel(\n",
    "\t'T x, T w, T bias',              \n",
    "\t'T z',                         \n",
    "\t'x * w',                      \n",
    "\t'a + b', \n",
    "\t'z = a + bias',              \n",
    "\t'0',                            \n",
    "\t'dnnLayer'  )\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2,5)\n",
    "w = cp.arange(10, dtype=np.float32).reshape(2,5)\n",
    "bias = -0.1\n",
    "z = dnnLayer(x,w,bias)\n",
    "print(z)\n",
    "\n",
    "#output: 284.9 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Kernels\n",
    "\n",
    "- Raw kernels enable the direct use of kernels from CUDA source, and is defined through the RawKernel class.\n",
    "- The RawKernel object allows you to call the kernel with CUDA’s cuLaunchKernel interface giving you control over:\n",
    "    - grid size\n",
    "    - block size\n",
    "    - shared memory size \n",
    "    - stream. \n",
    "<img src=\"../images/cupy_kernel_memory.png\">\n",
    "\n",
    "\n",
    "The code shown below is an example of Raw Kernel. The Python code embeds a CUDA C kernel (add_func) that adds two arrays together. The statement **int tid = blockDim.x * blockIdx.x + threadIdx.x** estimates the unique ID for each threads, thus, enables the access to memory grid, blocks and threads.\n",
    "\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "add_kernel = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void add_func(const float* x1, const float* x2, float* y) {\n",
    "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "y[tid] = x1[tid] + x2[tid];\n",
    "}\n",
    "''', 'add_func')\n",
    "```\n",
    "\n",
    "Run the example in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "add_kernel = cp.RawKernel(r'''\n",
    "extern \"C\" __global__\n",
    "void add_func(const float* x1, const float* x2, float* y) {\n",
    "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "y[tid] = x1[tid] + x2[tid];\n",
    "}\n",
    "''', 'add_func')\n",
    "\n",
    "N = 100\n",
    "shape = (10, 10)\n",
    "\n",
    "x1 = cp.arange(N, dtype=cp.float32).reshape(shape)\n",
    "x2 = cp.arange(N, dtype=cp.float32).reshape(shape)\n",
    "y = cp.zeros((shape), dtype=cp.float32)\n",
    "\n",
    "add_kernel((10,), (10,), (x1, x2, y)) \n",
    "\n",
    "print(y)\n",
    "\n",
    "#output:\n",
    "#[[  0.   2.   4.   6.   8.  10.  12.  14.  16.  18.]\n",
    "#[ 20.  22.  24.  26.  28.  30.  32.  34.  36.  38.]\n",
    "#[ 40.  42.  44.  46.  48.  50.  52.  54.  56.  58.]\n",
    "#[ 60.  62.  64.  66.  68.  70.  72.  74.  76.  78.]\n",
    "#[ 80.  82.  84.  86.  88.  90.  92.  94.  96.  98.]\n",
    "#[100. 102. 104. 106. 108. 110. 112. 114. 116. 118.]\n",
    "#[120. 122. 124. 126. 128. 130. 132. 134. 136. 138.]\n",
    "#[140. 142. 144. 146. 148. 150. 152. 154. 156. 158.]\n",
    "#[160. 162. 164. 166. 168. 170. 172. 174. 176. 178.]\n",
    "#[180. 182. 184. 186. 188. 190. 192. 194. 196. 198.]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/raw_kernel.png\">\n",
    "\n",
    "**An example of complex-value array addition using Raw Kernels is presented in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "complex_kernel = cp.RawKernel(r'''\n",
    "#include <cupy/complex.cuh>\n",
    "extern \"C\" __global__\n",
    "void my_func(const complex<float>* x1, const complex<float>* x2, complex<float>* y, float a){\n",
    "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    y[tid] = x1[tid] + a * x2[tid];\n",
    "}\n",
    "''', 'my_func')\n",
    "\n",
    "# generating input x1 and x2, while y stores the output\n",
    "x1 = cp.arange(25, dtype=cp.complex64).reshape(5,5)\n",
    "x2 = 1j*cp.arange(25, dtype=cp.complex64).reshape(5,5)\n",
    "y = cp.zeros((5,5), dtype=cp.complex64)\n",
    "\n",
    "# calling the kernel\n",
    "complex_kernel((1,),(25,),(x1, x2,y,cp.float32(2.0)))\n",
    "\n",
    "print(y)\n",
    "\n",
    "#output:\n",
    "#[[ 0. +0.j  1. +2.j  2. +4.j  3. +6.j  4. +8.j]\n",
    "#[ 5.+10.j  6.+12.j  7.+14.j  8.+16.j  9.+18.j]\n",
    "#[10.+20.j 11.+22.j 12.+24.j 13.+26.j 14.+28.j]\n",
    "#[15.+30.j 16.+32.j 17.+34.j 18.+36.j 19.+38.j]\n",
    "#[20.+40.j 21.+42.j 22.+44.j 23.+46.j 24.+48.j]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This also produced the same output:\n",
    "complex_kernel((5,), (5,), (x1, x2, y, cp.float32(2.0)))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Kernel Attributes ##################\n",
    "print(\"max_dynamic_shared_size_bytes: \", complex_kernel.max_dynamic_shared_size_bytes )\n",
    "\n",
    "print(\"max_threads_per_block: \", complex_kernel.max_threads_per_block )\n",
    "\n",
    "print(\"attributes: \",complex_kernel.attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Modules \n",
    "\n",
    "- The **RawModule** class is used to define a large raw CUDA C source or to load an existing CUDA binary.\n",
    "- It is initialized by a CUDA C source code having several kernels (functions) such that needed kernels are retrieved by calling the **get_function()** method. A sample Raw Module construct written below consists of two kernels, **test_sum** (adds to arrays) and **test_multiply** (multiplication of two arrays). \n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "loaded_from_source = r'''\n",
    "extern \"C\" {\n",
    "__global__ void test_sum(const float* A, const float* B, float* C, int N)\n",
    " { \n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(tid < N)\n",
    "    {\n",
    "      C[tid] = A[tid] + B[tid]; \n",
    "    }\n",
    " }\n",
    " __global__ void test_multiply(const float* A, const float* B, float* C, int N )\n",
    " {\n",
    "    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(tid < N)\n",
    "    {\n",
    "        C[tid] = A[tid] * B[tid];\n",
    "    }\n",
    " }\n",
    "}'''\n",
    "```\n",
    "**The sample code in the cells below are implementations of Raw Module Kernel. Run the cells and see the outputs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "load_raw_module = r'''\n",
    "extern \"C\" {\n",
    "__global__ void sum_ker(const float* a, const float* b, float* c)\n",
    " { \n",
    "    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    c[tid] = a[tid] + b[tid]; \n",
    "    \n",
    " }\n",
    " __global__ void multiply_ker(const float* a, const float* b, float* c )\n",
    " {\n",
    "    unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    c[tid] = a[tid] * b[tid];\n",
    " }\n",
    "}'''\n",
    "\n",
    "# loading module\n",
    "module = cp.RawModule(code = load_raw_module)\n",
    "\n",
    "# accessing kernels within the Raw module\n",
    "ker_sum = module.get_function('sum_ker')\n",
    "ker_times = module.get_function('multiply_ker')\n",
    "\n",
    "# initializing array a, b, and c. c is the array that stores the output values\n",
    "a = cp.arange(25, dtype=cp.float32).reshape(5,5)\n",
    "b = cp.ones((5,5), dtype=cp.float32)\n",
    "c = cp.zeros((5,5), dtype=cp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the above cell before runing this cell\n",
    "\n",
    "#calling kernel ker_sum\n",
    "ker_sum((1,),(25,), (a,b,c))\n",
    "print(c)\n",
    "\n",
    "#output:\n",
    "#[[ 1.  2.  3.  4.  5.]\n",
    "#[ 6.  7.  8.  9. 10.]\n",
    "#[11. 12. 13. 14. 15.]\n",
    "#[16. 17. 18. 19. 20.]\n",
    "#[21. 22. 23. 24. 25.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling kernel ker_times\n",
    "ker_times((5,),(5,),(a,b,c))\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Fusion\n",
    "\n",
    "- Kernel fusion is a decorator that fuses functions. It can be used to define an elementwise or reduction kernels easily. This decorator is specified at the top of a function as: \n",
    "\n",
    "```python\n",
    "@cp.fuse(kernel_name='<target function name>') \n",
    "```\n",
    "or as\n",
    "```python\n",
    " @cp.fuse()\n",
    "```\n",
    "An example that computes **z = x * w + bias** is presented in the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "@cp.fuse(kernel_name='dnnlayerNode')\n",
    "def dnnlayerNode(x, w, bias):\n",
    "    return  (x * w) + bias\n",
    "\n",
    "# input parameters\n",
    "x = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "w = cp.arange(9, dtype=cp.float32).reshape(3,3)\n",
    "bias = cp.array([-0.5], dtype=cp.float32)\n",
    "         \n",
    "z = dnnlayerNode(x,w,bias)\n",
    "print(z)\n",
    "\n",
    "#output:\n",
    "#[[-0.5  0.5  3.5]\n",
    "#[ 8.5 15.5 24.5]\n",
    "#[35.5 48.5 63.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "<img src=\"../images/cupy_summary.png\" width=\"80%\" height=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab Task\n",
    "\n",
    "In this section, you are expected to click on the **Serial Code Lab Assignment** link and proceed to Lab 2. In this lab, you will find three python serial code functions. You are required to revise the **pair_gpu** function to run on the GPU, and likewise do a few modifications within the **main** function.\n",
    "\n",
    "## <div style=\"text-align:center; color:#FF0000; border:3px solid red;height:80px;\"> <b><br/> [Serial Code Lab Assignment](serial_RDF.ipynb) </b> </div>\n",
    "\n",
    " \n",
    "---\n",
    "\n",
    "\n",
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, we recommend you go to your browser's File menu (not the Jupyter notebook file menu) and save the complete web page. This will ensure the images are copied as well. You can also execute the following cell block to create a zip-file of the files you've been working on and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -f nways_files.zip\n",
    "zip -r nways_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**After** executing the above zip command, you should be able to download the zip file [here](../nways_files.zip).\n",
    "\n",
    "**IMPORTANT**: Please click on **HOME** to go back to the main notebook for *N ways of GPU programming for MD* code.\n",
    "\n",
    "---\n",
    "\n",
    "# <p style=\"text-align:center;border:3px; border-style:solid; border-color:#FF0000  ; padding: 1em\"> <a href=../../../nways_MD_start_python.ipynb>HOME</a></p>\n",
    "\n",
    "---\n",
    "\n",
    "# Links and Resources\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download Nsight System latest version from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "##  References\n",
    "- https://docs.cupy.dev/en/stable/\n",
    "- https://cupy.dev/\n",
    "- CuPy Documentation Release 8.5.0, Preferred Networks, inc. and Preferred Infrastructure inc., Feb 26, 2021.\n",
    "- Bhaumik Vaidya, Hands-On GPU-Accelerated Computer Vision with OpenCV and CUDA, Packt Publishing, 2018.\n",
    "- Crissman Loomis and Emilio Castillo, CuPy Overview: NumPy Syntax Computation with Advanced CUDA Features, GTC Digital March, March 2020.\n",
    "- https://www.gpuhackathons.org/technical-resources\n",
    "- https://rapids.ai/start.html\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
