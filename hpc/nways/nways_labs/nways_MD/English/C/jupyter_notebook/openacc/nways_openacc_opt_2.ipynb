{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's execute the cell below to display information about the CUDA driver and GPUs running on the server by running the `nvidia-smi` command. To do this, execute the cell block below by giving it focus (clicking on it with your mouse), and hitting Ctrl-Enter, or pressing the play button in the toolbar above. If all goes well, you should see some output returned below the grey cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "The **goal** of this lab is to:\n",
    "\n",
    "- Dig deeper into kernels by analyzing it with Nsight Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we learned to optimize the parallel [RDF](../serial/rdf_overview.ipynb) application using OpenACC. Moreover, we used NVIDIA Nsight Systems to get a system-wide performance analysis. Now, let's dig deeper and profile the kernel with the Nsight Compute profiler to get detailed performance metrics and find out how the OpenACC is mapped at the Compute Unified Device Architecture(CUDA) hardware level. Note: You will get a better understanding of the GPU architecture in the CUDA notebooks.\n",
    "\n",
    "\n",
    "To do this, let's use the [solution](../../source_code/openacc/SOLUTION/rdf_collapse.cpp) as a reference to get a similar report from Nsight Compute. Run the application, and profile it with the Nsight Systems first.\n",
    "\n",
    "Now, let's compile, and profile it with Nsight Systems first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the solution for Tesla GPU\n",
    "!cd ../../source_code/openacc && nvc++ -acc -ta=tesla:lineinfo  -Minfo=accel -o rdf SOLUTION/rdf_collapse.cpp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile the solution with Nsight Systems \n",
    "!cd ../../source_code/openacc && nsys profile -t nvtx,openacc --stats=true --force-overwrite true -o rdf_collapse_solution ./rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the profiler's report. [Download the profiler output](../../source_code/openacc/rdf_collapse_solution.qdrep) and open it via the GUI. Now, right click on the kernel `pair_gpu_182_gpu` and click on \"Analyze the Selected Kernel with NVIDIA Nsight Compute\" (see below screenshot). \n",
    "\n",
    "<img src=\"../images/compute_analyz.png\">\n",
    "\n",
    "Then, make sure to tick the radio button next to \"Display the command line to user NVIDIA Nsight Compute CLI\". \n",
    "\n",
    "<img src=\"../images/compute_command_line.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Then, you simply copy the command, run it and analyze the selected kernel. \n",
    "\n",
    "<img src=\"../images/compute_command.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "To profile the selected kernel, run the below cell (by adding `--set full` we make sure to capture all the sections in Nsight Compute profiler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile the selected kernel in the solution with Nsight compute\n",
    "!cd ../../source_code/openacc && ncu --set full -k pair_gpu --launch-skip 1 --launch-count 1 -o rdf_collapse_solution ./rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's checkout the Nsight Compute profiler's report together. [Download the profiler output](../../source_code/openacc/rdf_collapse_solution.ncu-rep) and open it via the GUI. Let's checkout the first section called \"GPU Speed Of Light\". This section gives an overview of the utilization for compute and memory resources on the GPU. As you can see from the below screenshot, the Speed of Light (SOL) reports the achieved percentage of utilization of 30.58% for SM and 19.47% for memory. \n",
    "\n",
    "\n",
    "<img src=\"../images/sol.png\">\n",
    "\n",
    "\n",
    "**Extra**: If you can use the baseline feature on the Nsight Compute and compare the analysis of the kernel from this version of the RDF (which uses data directives and collapse clause) with the very first parallel version where we only added parallel directives and used managed memory, you can see how much improvements we got (see the below screenshot for reference):\n",
    "\n",
    "<img src=\"../images/sol_baseline.png\">\n",
    "\n",
    "It is clear that we were able to reduce the execution time to half(red rectangle) and increase the SM and memory utilization (green rectangle). However, as you see the device is still underutilized. Let's look at the roofline analysis which indicates that the application is latency bound and the kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of the device this was profiled on.\n",
    "\n",
    "<img src=\"../images/roofline_collapse.png\">\n",
    "\n",
    "The Nsight Compute profiler suggests us to checkout the \"Scheduler Statistics\" and \"Warp State Statistics\" sections to see how we can improve the utilization. By checking the the \"Scheduler Statistics\" section, we can see that for this kernel the scheduler does not issue an instruction every cycle. We have many active warps (average of 11.29 out of 16) but only an average of 0.68 warps are eligible per cycle. This results in no instruction being issues and the issue slot remains unused. The \"Warp State Statistics\" section can indicate which stall reasons causes this.\n",
    "\n",
    "<img src=\"../images/scheduler_collapse.png\">\n",
    "\n",
    "From the  \"Warp State Statistics\" section, we can see that the most important stall reason is LG (local/global) Throttle due to extremely frequent memory instructions.\n",
    "\n",
    "<img src=\"../images/warp_collapse.png\">\n",
    "\n",
    "Now, let's have a look at the \"Source Counters\" section located at the end of \"Details\" page of the profiler report. \n",
    "\n",
    "<img src=\"../images/source_loc.png\">\n",
    "\n",
    "\n",
    "The section contains tables indicating the N highest or lowest values of one or more metrics in the selected kernel source code. Hotspot tables point out performance problems in the source. \n",
    "\n",
    "<img src=\"../images/source_collapse.png\">\n",
    "\n",
    "We can select the location links to navigate directly to this location in the \"Source\" page. Moreover, you can hover the mouse over a value to see which metrics contribute to it.\n",
    "\n",
    "<img src=\"../images/source_hover.png\">\n",
    "\n",
    "The \"Source\" page displays metrics that can be correlated with source code. It is filtered to only show (SASS) functions that were executed in the kernel launch.\n",
    "\n",
    "<!--<img src=\"../images/source_sass_collapse.png\">-->\n",
    "\n",
    "<img src=\"../images/source_sass.png\">\n",
    "\n",
    "The \"Source\" section in the \"Details\" page indicates  that the issue is *uncoalesced Global memory access*. \n",
    "\n",
    "<img src=\"../images/uncoalesced_hint.png\">\n",
    "\n",
    "**Memory Coalescing**\n",
    "\n",
    "On GPUs, threads are executed in warps. When we have a group of 32 contiguous threads called *warp* accessing adjacent locations in memory, we have *Coalesced memory* access and as a result we have few transactions and higher utilization. However, if a warp of 32 threads accessing scattered memory locations, then we have *Uncoalesced  memory* access and this results in high number of transactions and low utilization.\n",
    "\n",
    "\n",
    "<img src=\"../images/coalesced_mem.png\">\n",
    "\n",
    "Without changing the data structure and refactoring the code, we cannot fix this issue and improve the performance further using OpenACC in a straightforward easier way. The next step would be to look into how to optimize this application further with CUDA and perhaps take advantage of shared memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Lab Summary\n",
    "\n",
    "If you would like to download this lab for later viewing, it is recommend you go to your browsers File menu (not the Jupyter notebook file menu) and save the complete web page.  This will ensure the images are copied down as well. You can also execute the following cell block to create a zip-file of the files you've been working on, and download it with the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "rm -f nways_files.zip\n",
    "zip -r nways_files.zip *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After** executing the above zip command, you should be able to download the zip file [here](../nways_files.zip). Let us now go back to parallelizing our code using other approaches.\n",
    "\n",
    "**IMPORTANT**: Please click on **HOME** to go back to the main notebook for *N ways of GPU programming for MD* code.\n",
    "\n",
    "-----\n",
    "\n",
    "# <p style=\"text-align:center;border:3px; border-style:solid; border-color:#FF0000  ; padding: 1em\"> <a href=../../../nways_MD_start.ipynb>HOME</a></p>\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "# Links and Resources\n",
    "\n",
    "[NVIDIA Nsight System](https://docs.nvidia.com/nsight-systems/)\n",
    "\n",
    "[NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute)\n",
    "\n",
    "[CUDA Toolkit Download](https://developer.nvidia.com/cuda-downloads)\n",
    "\n",
    "**NOTE**: To be able to see the Nsight System profiler output, please download Nsight System latest version from [here](https://developer.nvidia.com/nsight-systems).\n",
    "\n",
    "Don't forget to check out additional [OpenACC Resources](https://www.openacc.org/resources) and join our [OpenACC Slack Channel](https://www.openacc.org/community#slack) to share your experience and get more help from the community.\n",
    "\n",
    "--- \n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
