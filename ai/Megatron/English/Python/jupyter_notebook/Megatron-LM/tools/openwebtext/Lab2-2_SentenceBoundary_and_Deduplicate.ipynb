{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beneficial-angle",
   "metadata": {},
   "source": [
    "## Custom Data Cleaning\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Big langauge model is sampling efficient, which implies that if the data fed to the model were full of misspelled words, vulgar by nature ( This is often the case with uncensored data extracted from web forums/chat rooms), contain large volume of other langauge than the target langauge, or were having undesired and mischievous characteristic. One should definately consider creating a pipeline to clean and filter the data.\n",
    "\n",
    "Since Megatron-LM will sample sentences from documents during training run, we also need to construct a mechanism to find sentence boundary per document.\n",
    "\n",
    "As is for data deduplication, we are offering one way, there are many other ways, to deduplicate the data on document level, base on a similarity threshold. Why is it important ? Let's take newspaper for example, imagine, a catastrophic event such as tsunami occured in Thailand which claimed  many lives, such event could be reported repeatedly in a great many news articles all over the world. Wouldn't we want to deduplicate the almost identical news articles, with a good similarity measuring mechanism ?\n",
    "\n",
    "Similarily, when we blend datasets from a variety of sources in order to obtain big data for training big langauge model, we would want to have a way to deduplicate the repeated documents which are present across the collected datasets.\n",
    "\n",
    "\n",
    "Therefore, the goal of this lab is to provide some basic tools for cleaning and filtering data which should be carefully applied to custom langauge datasets, and preserve the inherit charateristics in the datasets. \n",
    "\n",
    "In particular, this notebook covers the following steps :\n",
    "\n",
    "    1. Langauge Detection. \n",
    "    2. Find sentence boundaries.\n",
    "    3. Deduplicate documents based on similarity score.\n",
    "Note: The method recommanded in the [Megatron-LM repo, namely LSH](https://github.com/NVIDIA/Megatron-LM/tree/main/tools/openwebtext) will be used for deduplication.\n",
    "\n",
    "What this notebook will NOT cover :\n",
    "\n",
    "    - Constructing a black-list to block and filter out inappropriate words/sentences/document.\n",
    "    - Clean empty lines or empty sentences.\n",
    "    - Spell-check words and punctuations.\n",
    "    - Remove sentences/documents with too few tokens.\n",
    "     and many more customized data cleaning methods which one should consider adding to the data cleaning pipelins.\n",
    "\n",
    "At the end, there will be a **mini challenge** for hands-on practicing identifying the numbers of duplicated documents as close as you can to the groudtruth number!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-powder",
   "metadata": {},
   "source": [
    "---\n",
    "We will start by installing all the python libraries we need.\n",
    "\n",
    "In case you encounter problems of installing LSH, here is the fix :\n",
    "\n",
    "    Install LSH - \n",
    "\n",
    "    Follow instruction from [Megatron-LM/tools/openwebtext/README](https://github.com/NVIDIA/Megatron-LM/tree/main/tools/openwebtext) in openwebtext clearning folder \n",
    "\n",
    "    Note : In a restricted environment where sudo is not allowed, please follow the below instruction to modify installation.\n",
    "            \n",
    "            Call out a terminal as illustrated below.             \n",
    "   ![call out a terminal ](../../pics/Alt_callout2terminals.JPG)\n",
    "   \n",
    "            cd gpubootcamp/ai/Megatron/English/Python/jupyter_notebook/Megatron-LM/tools/openwebtext/\n",
    "        \n",
    "            git clone https://github.com/mattilyra/LSH.git\n",
    "            cd LSH\n",
    "            pip install -U --user cython>=0.24.1\n",
    "            open setup.py in an editor and modify as below\n",
    "   ![modify setup.py line 6](../../pics/modifyLSH_setuppy.JPG)\n",
    "\n",
    "            python setup.py install --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy langdetect numpy torch pandas nltk sentencepiece boto3 tqdm regex bs4 htmlmin tldextract sentence-splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-fifty",
   "metadata": {},
   "source": [
    "1. Langauge Detection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "swe_raw_text='Under fredagsförmiddagen höll polis och räddningstjänst presskonferens tillsammans med en representanter från flygplatsens egna räddningsenhet och Örebro kommun.'\n",
    "detect(swe_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "danish_text='1. januar 2021 var folketallet 5.840.045. Ved den første folketælling i 1735 var der 718.000 danskere.'\n",
    "detect(danish_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "finnish_text='Jokaisella on oikeus vapaasti osallistua yhteiskunnan sivistyselämään, nauttia taiteista sekä päästä osalliseksi tieteen edistyksen mukanaan tuomista eduista.'\n",
    "detect(finnish_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-lithuania",
   "metadata": {},
   "source": [
    "Once we have a way to identify which langauge does this document belong to, we can then filter or remove the documents at will and keep only the selected language(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-google",
   "metadata": {},
   "source": [
    "2. Find sentence boundaries - alternative 1 : NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text='Detta är ett stycke. Den innehåller flera meningar. “Men varför”, frågar du? Andersson pekas ut som nästa partiledare: “Medlemmarna ska säga sitt”'\n",
    "print(\"original doc is :\\n \", text)\n",
    "sents=sent_tokenize(text)\n",
    "i=0\n",
    "for sent in sents:\n",
    "    print(\"------- sentence {} -------\".format(str(i)))    \n",
    "    print(sent)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-singapore",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "\n",
    "Observe how NLTK tokenizer sentence per given document :\n",
    "\n",
    "    ------- sentence 0 -------\n",
    "    Detta är ett stycke.\n",
    "    ------- sentence 1 -------\n",
    "    Den innehåller flera meningar.\n",
    "    ------- sentence 2 -------\n",
    "    “Men varför”, frågar du?\n",
    "    ------- sentence 3 -------\n",
    "    Andersson pekas ut som nästa partiledare: “Medlemmarna ska säga sitt”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-gateway",
   "metadata": {},
   "source": [
    "2. Find sentence boundaries - alternative 2 : NLTK + custom function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def normal_cut_sentence(temp):\n",
    "    return sent_tokenize(temp)\n",
    "\n",
    "def cut_sentence_with_quotation_marks(text):\n",
    "    p = re.compile(\"“.*?”\")\n",
    "    ls = []\n",
    "    index = 0\n",
    "    length = len(text)\n",
    "    for i in p.finditer(text):\n",
    "        temp = ''\n",
    "        start = i.start()\n",
    "        end = i.end()\n",
    "        for j in range(index, start):\n",
    "            temp += text[j]\n",
    "        if temp != '':\n",
    "            temp_list = normal_cut_sentence(temp)\n",
    "            ls += temp_list\n",
    "        temp = ''\n",
    "        for k in range(start, end):\n",
    "            temp += text[k]\n",
    "        if temp != ' ':\n",
    "            ls.append(temp)\n",
    "        index = end\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=cut_sentence_with_quotation_marks(text)\n",
    "i=0\n",
    "for sent in sents:\n",
    "    print(\"------- sentence {} -------\".format(str(i)))  \n",
    "    print(sent)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-poison",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "\n",
    "Observe how the custom function `cut_sentence_with_quotation_marks` modifies NLTK and adding quotation marks as an additional sentence-splitter :\n",
    "\n",
    "        ------- sentence 0 -------\n",
    "        Detta är ett stycke.\n",
    "        ------- sentence 1 -------\n",
    "        Den innehåller flera meningar.\n",
    "        ------- sentence 2 -------\n",
    "        “Men varför”\n",
    "        ------- sentence 3 -------\n",
    "        , frågar du?\n",
    "        ------- sentence 4 -------\n",
    "        Andersson pekas ut som nästa partiledare:\n",
    "        ------- sentence 5 -------\n",
    "        “Medlemmarna ska säga sitt”\n",
    "        \n",
    "There are many ways to split the sentence within a document, the above is just one example.\n",
    "\n",
    "One can construct other custom function to do sentence splitting with or without NLTK. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-profession",
   "metadata": {},
   "source": [
    "3. Deduplicate documents based on similarity score.\n",
    "\n",
    "[Local Sensitive Hash](http://snap.stanford.edu/class/cs246-2012/slides/03-lsh.pdf)\n",
    "\n",
    "First, we create shingles from the document with ngram, then fingerprints were created and Jaccard Similarity measured is used in order to find the top K most similar items given pairs of documents based on an arbitrary threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lsh import cache, minhash # https://github.com/mattilyra/lsh\n",
    "from lsh import minhash\n",
    "\n",
    "# a pure python shingling function that will be used in comparing\n",
    "# LSH to true Jaccard similarities\n",
    "def shingles(text, char_ngram=5):\n",
    "    return set(text[head:head + char_ngram] for head in range(0, len(text) - char_ngram))\n",
    "\n",
    "\n",
    "def jaccard(set_a, set_b):\n",
    "    intersection = set_a & set_b\n",
    "    union = set_a | set_b\n",
    "    return len(intersection) / len(union)\n",
    "\n",
    "\n",
    "def candidate_duplicates(document_feed, char_ngram=5, seeds=100, bands=5, hashbytes=4):\n",
    "    char_ngram = char_ngram\n",
    "    sims = []\n",
    "    hasher = minhash.MinHasher(seeds=seeds, char_ngram=char_ngram, hashbytes=hashbytes)\n",
    "    if seeds % bands != 0:\n",
    "        raise ValueError('Seeds has to be a multiple of bands. {} % {} != 0'.format(seeds, bands))\n",
    "    \n",
    "    lshcache = cache.Cache(num_bands=bands, hasher=hasher)\n",
    "    for i_line, line in enumerate(document_feed):\n",
    "        line = line.decode('utf8')\n",
    "        docid, headline_text = line.split('\\t', 1)\n",
    "        fingerprint = hasher.fingerprint(headline_text.encode('utf8'))\n",
    "        \n",
    "        # in addition to storing the fingerpring store the line\n",
    "        # number and document ID to help analysis later on\n",
    "        lshcache.add_fingerprint(fingerprint, doc_id=(i_line, docid))\n",
    "\n",
    "    candidate_pairs = set()\n",
    "    for b in lshcache.bins:\n",
    "        for bucket_id in b:\n",
    "            if len(b[bucket_id]) > 1:\n",
    "                pairs_ = set(itertools.combinations(b[bucket_id], r=2))\n",
    "                candidate_pairs.update(pairs_)\n",
    "    \n",
    "    return candidate_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-consumer",
   "metadata": {},
   "source": [
    "We want to verify how good this algorithm is, hence, we will create deduplcates documents from our toy data `extractedNVblogs.txt` obtained in webscaping lab. We will flag the duplicated documents, in the column `duplicate=True`, and this column will serve as the groundtruth for us, we will use this hand-crafted data to verify whether this algorithm will work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols=['doc1']\n",
    "df=pd.read_csv('../../../../dataset/EN/extractedNVblogs.txt',sep='\\n', names=cols ,skiprows=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_duplicates(df):\n",
    "    doc2=[]\n",
    "    duplicate=[]\n",
    "    n=len(df)\n",
    "    for i in range(n):\n",
    "        other_population=[k for k in range(n) if k!=i]\n",
    "        \n",
    "        other_idx=np.random.choice(other_population)\n",
    "        current_idx=np.random.choice([i,other_idx], p=[0.3,0.7])\n",
    "        if current_idx==i:            \n",
    "            duplicate.append(True)\n",
    "        else:\n",
    "            duplicate.append(False)\n",
    "        doc2.append(df.iloc[current_idx,0])\n",
    "    df['index']=df.index\n",
    "    df['doc2']=doc2\n",
    "    df['duplicate']=duplicate\n",
    "    cols=['index','doc1','doc2','duplicate']\n",
    "    df=df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=create_duplicates(df)\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-insured",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "    \n",
    "            \n",
    "            index \tdoc1 \tdoc2 \tduplicate\n",
    "            65 \t65 \tThis post was updated July 20, 2021 to reflect... \tThis post was updated July 20, 2021 to reflect... \tTrue\n",
    "            66 \t66 \tResearchers, developers, and engineers worldwi... \tThis post was originally published in August 2... \tFalse\n",
    "            67 \t67 \tLooking to reveal secrets of days past, histor... \tThe NVIDIA Deep Learning Institute (DLI) exten... \tFalse\n",
    "            68 \t68 \tScientists searching the universe for gravitat... \tRobotics researchers from NVIDIA and Universit... \tFalse\n",
    "            69 \t69 \tAt GTC ’21, experts presented a variety of tec... \tThe NVIDIA Hardware Grant Program helps advanc... \tFalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is our groundtruth, count duplicate == True is 31 \n",
    "df2.duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-assurance",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "    \n",
    "    False    45\n",
    "    True     25\n",
    "    Name: duplicate, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "keep_cols_to_write=['index','doc1','doc2']\n",
    "df3=df2[keep_cols_to_write]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-communication",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "\n",
    "            index \tdoc1 \tdoc2\n",
    "        0 \t0 \tDeep learning models have been successfully us... \tDeep learning models have been successfully us...\n",
    "        1 \t1 \tBreast cancer is the most frequently diagnosed... \tIn NVIDIA Clara Train 4.0, we added homomorphi...\n",
    "        2 \t2 \tThe NVIDIA Deep Learning Institute (DLI) exten... \tThe NVIDIA Deep Learning Institute (DLI) exten...\n",
    "        3 \t3 \tEngineers, product developers and designers ar... \tDeep learning research requires working at sca...\n",
    "        4 \t4 \tDespite substantial progress in natural langua... \tNVIDIA announces our newest release of the CUD..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-outdoors",
   "metadata": {},
   "source": [
    "We could proceed to write the above dataframe to a csv file, however, for mini challenge, we would like to keep determinism. \n",
    "\n",
    "In order to preserve determinism, we will load the previously saved df2.csv file so that all attendees have the exact same file.\n",
    "\n",
    "The `df2.csv` file is provided in this repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do this deterministically , let's read in the saved df2.csv file\n",
    "df2=pd.read_csv('df2.csv', names=['index', 'doc1', 'doc2', 'duplicate'], skiprows=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-stream",
   "metadata": {},
   "source": [
    "Below is the expected outputs , outputs should look exactly the same as below:\n",
    "\n",
    "        index \tdoc1 \tdoc2 \tduplicate\n",
    "        0 \t0 \tToday, NVIDIA announced new pretrained models ... \tAstrophysics researchers have long faced a tra... \tFalse\n",
    "        1 \t1 \tThis post was updated July 20, 2021 to reflect... \tThis post was updated July 20, 2021 to reflect... \tTrue\n",
    "        2 \t2 \tIn part 1 of this series, we introduced new AP... \tEdge computing has been around for a long time... \tFalse\n",
    "        3 \t3 \tThe NVIDIA NGC team is hosting a webinar with ... \tThe NVIDIA NGC team is hosting a webinar with ... \tTrue\n",
    "        4 \t4 \tNVIDIA announces our newest release of the CUD... \tAs an undergraduate student excited about AI f... \tFalse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is our groundtruth, count duplicate == Truth is 31 \n",
    "df2.duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-sender",
   "metadata": {},
   "source": [
    "Below is the expected outputs , outputs should look exactly the same as below:\n",
    "\n",
    "            False    42\n",
    "            True     31\n",
    "            Name: duplicate, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l groundtruth.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-runner",
   "metadata": {},
   "source": [
    "Below is the expected outputs , outputs should look exactly the same as below:\n",
    "\n",
    "    73 groundtruth.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-mixer",
   "metadata": {},
   "source": [
    "## Mini-Challenge\n",
    "\n",
    "Task : \n",
    "    Overwrite the below parameters before calling `candidate_duplicates()` function, and rerun the cell block below.\n",
    "    \n",
    "    char_ngram= < input_value >\n",
    "    seeds=< input_value >\n",
    "    bands=< input_value >\n",
    "    hashbytes=< input_value >\n",
    "\n",
    "Pass : Consider yourself pass this mini challenge when you approach the number **31 +/- 3** ! \n",
    "\n",
    "\n",
    "Re-run the below cell for experiments in order to get as close as possible to the ground truth = 31 duplicates.\n",
    "<a id=\"Rerun_Cell\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is the Re Run Cell \n",
    "import itertools\n",
    "import random\n",
    "lines = []\n",
    "with open('groundtruth.txt', 'rb') as fh:\n",
    "    # read the first 1000 lines into memory so we can compare them\n",
    "    for line in itertools.islice(fh, 1000):\n",
    "        lines.append(line.decode('utf8'))\n",
    "    \n",
    "    # reset file pointer and do LSH\n",
    "    fh.seek(0)\n",
    "    feed = itertools.islice(fh, 1000)\n",
    "\n",
    "    ## modify the below numbers as input to function candidate_duplicates()\n",
    "    ## initial value given below, please modify them accordingly to obtain count of number of duplicates to be as close as 31 (= groundtruth)\n",
    "    char_ngram=13\n",
    "    seeds=100\n",
    "    bands=5\n",
    "    hashbytes=8\n",
    "    \n",
    "    candidates = candidate_duplicates(feed, char_ngram=char_ngram, seeds=seeds, bands=bands, hashbytes=hashbytes)\n",
    "\n",
    "# go over all the candidates comparing their similarities\n",
    "similarities = []\n",
    "for ((line_a, docid_a), (line_b, docid_b)) in candidates:\n",
    "    doc_a, doc_b = lines[line_a], lines[line_b]\n",
    "    shingles_a = shingles(lines[line_a])\n",
    "    shingles_b = shingles(lines[line_b])\n",
    "    \n",
    "    jaccard_sim = jaccard(shingles_a, shingles_b)\n",
    "    fingerprint_a = set(hasher.fingerprint(doc_a.encode('utf8')))\n",
    "    fingerprint_b = set(hasher.fingerprint(doc_b.encode('utf8')))\n",
    "    minhash_sim = len(fingerprint_a & fingerprint_b) / len(fingerprint_a | fingerprint_b)\n",
    "    similarities.append((docid_a, docid_b, jaccard_sim, minhash_sim))\n",
    "\n",
    "for a,b,jsim, msim in random.sample(similarities, k=2 ):\n",
    "    print(\"pair of similar sentences with jaccard_sim score:{} and minhash_sim score:{} --- \\n\".format(str(jsim),str(msim)))\n",
    "    a=int(a)\n",
    "    b=int(b)\n",
    "    text_a=df2.iloc[a,1]\n",
    "    text_b=df2.iloc[b,2]\n",
    "    if text_a==text_b:\n",
    "        print(\"100% duplicates \\n\")\n",
    "    print(\"text_a:\", text_a.split(' ')[:5])\n",
    "    print(\"text_b:\", text_b.split(' ')[:5])\n",
    "    print('-----'*10)\n",
    "    import random\n",
    "\n",
    "print('\\nThere are **{}** candidate duplicates in total\\n'.format(len(candidates)))\n",
    "random.sample(similarities, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-analyst",
   "metadata": {},
   "source": [
    "Below is the expected outputs :\n",
    "\n",
    "A naive run\n",
    "\n",
    "        pair of similar sentences with jaccard_sim score:0.8197797952482132 and minhash_sim score:0.639344262295082 --- \n",
    "\n",
    "        text_a: ['The', 'NVIDIA,', 'Facebook,', 'and', 'TensorFlow']\n",
    "        text_b: ['Deep', 'learning', '(DL)', 'is', 'the']\n",
    "        --------------------------------------------------\n",
    "        pair of similar sentences with jaccard_sim score:0.9133693568066934 and minhash_sim score:0.8867924528301887 --- \n",
    "\n",
    "        100% duplicates \n",
    "\n",
    "        text_a: ['The', 'first', 'post', 'in', 'this']\n",
    "        text_b: ['The', 'first', 'post', 'in', 'this']\n",
    "        --------------------------------------------------\n",
    "\n",
    "        There are **3** candidate duplicates in total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-cross",
   "metadata": {},
   "source": [
    "WOW that is way too LOW !!! We should have 31 duplicates as groundtruth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-slide",
   "metadata": {},
   "source": [
    "<a id=\"TheChallenge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-scheduling",
   "metadata": {},
   "source": [
    "Go back and rerun cell <a href=\"./Lab2-2_SentenceBoundary_and_Deduplicate.ipynb#Rerun_Cell\">Jump to ReRun Cell</a>\n",
    "\n",
    "Solution will be delivered to you at the end of the bootcamp !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-crisis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn the candidates into a dictionary so we have easy access to\n",
    "# candidates pairs that were found\n",
    "candidates_dict = {(line_a, line_b): (docid_a, docid_b) for ((line_a, docid_a), (line_b, docid_b)) in candidates}\n",
    "found = 0\n",
    "for i in range(len(lines)):\n",
    "    for j in range(i+1, len(lines)):\n",
    "        if sims_all[i, j] >= .9:\n",
    "            # documents i and j have an actual Jaccard similarity >= 90%\n",
    "            found += ((i, j) in candidates_dict or (j, i) in candidates_dict)\n",
    "\n",
    "print('Out of {} pairs with similarity >= 90% {} were found, that\\'s {:.1%}'.format((sims_all >= .9).sum(), found, found / (sims_all >= .9).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-minority",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_candidates = []\n",
    "bands = [2, 5, 10, 20]\n",
    "for num_bands in bands:\n",
    "    with open('groundtruth.txt', 'rb') as fh:\n",
    "        feed = itertools.islice(fh, 1000)\n",
    "        candidates = candidate_duplicates(feed, char_ngram=5, seeds=100, bands=num_bands, hashbytes=4)\n",
    "        num_candidates.append(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-mouth",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.bar(bands, num_candidates, align='center');\n",
    "plt.title('Number of candidate duplicate pairs found by LSH using 100 minhash fingerprint.');\n",
    "plt.xlabel('Number of bands');\n",
    "plt.ylabel('Number of candidate duplicates');\n",
    "plt.xticks(bands, bands);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-bankruptcy",
   "metadata": {},
   "source": [
    "--- \n",
    "## Links and Resources\n",
    "Don't forget to check out additional resources such as [Language Detect](https://github.com/Mimino666/langdetect), [NLTK Sentence Tokenizer](https://www.nltk.org/api/nltk.tokenize.html) and [Local Sensitive Hashing](http://snap.stanford.edu/class/cs246-2012/slides/03-lsh.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-producer",
   "metadata": {},
   "source": [
    "-----\n",
    "## <p style=\"text-align:center;border:3px; padding: 1em\"> <a href=../../../../Start_Here.ipynb>HOME</a>&nbsp; &nbsp; &nbsp; <a href=../../../Lab2-3_train_own_GPT2BPETokenizer.ipynb>NEXT</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-scoop",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "\n",
    "## Licensing \n",
    "\n",
    "This material is released by OpenACC-Standard.org, in collaboration with NVIDIA Corporation, under the Creative Commons Attribution 4.0 International (CC BY 4.0). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
