{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_BOOTCAMP.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](speech-to-text-deployment.ipynb)\n",
    "[3](speechtotext-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](speech-to-text-deployment.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Speech Recognition(ASR) is often the first step in a building a Conversational AI model. An ASR model converts audible speech into text. The main metric for these models is to reduce Word Error Rate (WER) while transcribing the text. Simply put, the goal is to take an audio file and trancribe it.\n",
    "\n",
    "In this work, we are going to dicuss two models, [QuartzNet](https://arxiv.org/pdf/1910.10261.pdf) and [Jasper (Just Another SPeech Recognizer) model](https://arxiv.org/abs/1904.03288), both of which are end to end ASR models which take in audio and produce text.\n",
    "\n",
    "Jasper architectures consist of a repeated block structure that utilizes 1D convolutions. In a Jasper_KxR model, R sub-blocks (consisting of a 1D convolution, batch norm, ReLU, and dropout) are grouped into a single block, which is then repeated K times. We also have a one extra block at the beginning and a few more at the end that are invariant of K and R, and we use CTC loss.\n",
    "\n",
    "The QuartzNet is better variant of Jasper with a key difference that it uses time-channel separable 1D convolutions. This allows it to dramatically reduce number of weights while keeping similar accuracy.\n",
    "\n",
    "![QuartzNet with CTC](https://developer.nvidia.com/blog/wp-content/uploads/2020/05/quartznet-model-architecture-1-625x742.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Toolkit (TLT) is a python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. \n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible. \n",
    "\n",
    "Developers, researchers and software partners building intelligent vision AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transfer Learning Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Automatic Speech Recognition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Dig in: ASR using TLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure you have tlt installed and running (see setup notebook)\n",
    "The next step is to setup the mounts for TLT. The TLT launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tlt_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TLT launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tlt_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir <add path to DATA_DIR>\n",
    "! mkdir <add path to SPECS_DIR>\n",
    "! mkdir <add path to RESULTS_DIR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tlt_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"/home/<USERNAME>/.cache\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the docker image versions and the tasks that tlt can perform. You can also check this out with a `tlt --help` or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tlt info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TLT Docker.\n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is setup, we would like to take a bit of time to explain the tlt interface for ease of use. The command structure can be broken down as follows: `tlt <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading Specs\n",
    "TLT's Conversational AI Toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tlt speech_to_text download_specs \\\n",
    "    -r $RESULTS_DIR/speech_to_text \\\n",
    "    -o $SPECS_DIR/speech_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of demonstration we will use the popular AN4 dataset. Let's download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step converts the mp3 files into wav files and splits the data into training and testing sets. It also generates a \"meta-data\" file to be consumed by the dataloader for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tlt speech_to_text dataset_convert \\\n",
    "    -e $SPECS_DIR/speech_to_text/dataset_convert_an4.yaml \\\n",
    "    source_data_dir=$DATA_DIR/an4 \\\n",
    "    target_data_dir=$DATA_DIR/an4_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a listen to a sample audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path of the file here\n",
    "import IPython.display as ipd\n",
    "path = DATA_DIR + '/an4_converted/wavs/an268-mbmg-b.wav'\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously discussed, there are two models we would like to discuss, the QuartzNet model and the Jasper Model. Training commands for both of them are simillar. Let's have a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very neat interface which allows the end user to configure training parameters from the command line interface. <br>\n",
    "\n",
    "The process of opening the training script; finding the parameters of interest(which might be spread across multiple files), making the changes needed, and double checking everything is being replaced by a much more easy to use and visible command line interface.\n",
    "\n",
    "For instance if the number of epochs are needed to be modified along with a change in learning rate, the user can add `trainer.max_epochs=10` and `optim.lr=0.02` and train the model. Sample commands are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>A list of some of the customizable parameters along with their default values is as follows:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer:<br>\n",
    "<ul>  \n",
    "  <li>gpus: 1 </li>\n",
    "  <li>num_nodes: 1 </li>\n",
    "  <li>max_epochs: 5 </li>\n",
    "  <li>max_steps: null </li>\n",
    "  <li>checkpoint_callback: false </li>\n",
    "</ul>\n",
    "\n",
    "training_ds:\n",
    "<ul>  \n",
    "  <li>sample_rate: 16000 </li>\n",
    "  <li>labels: [\" \", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\",\n",
    "           \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"'\"] </li>\n",
    "  <li>batch_size: 32 </li>\n",
    "  <li>trim_silence: true </li>\n",
    "  <li>max_duration: 16.7 </li>\n",
    "  <li>shuffle: true </li>\n",
    "  <li>is_tarred: false </li>\n",
    "  <li>tarred_audio_filepaths: null </li>\n",
    "</ul>  \n",
    "\n",
    "validation_ds:\n",
    "<ul>  \n",
    "  <li>sample_rate: 16000 </li>\n",
    "  <li>labels: [\" \", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\",\n",
    "           \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"'\"] </li>\n",
    "  <li>batch_size: 32 </li>\n",
    "  <li>shuffle: false </li>\n",
    "</ul>  \n",
    "optim:\n",
    "<ul>\n",
    "  <li>name: novograd </li>\n",
    "  <li>lr: 0.01 </li>\n",
    "  <li>betas: [0.8, 0.5] </li>\n",
    "  <li>weight_decay: 0.001 </li>\n",
    "</ul>\n",
    "\n",
    "The steps below might take considerable time depending on the GPU being used. For best experience, we recommend using an A100 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training aN ASR model in TLT, we use the `tlt speech_to_text train` command with the following args:\n",
    "<ul>\n",
    "    <li> <b>-e</b> : Path to the spec file </li>\n",
    "    <li> <b>-g</b> : Number of GPUs to use </li>\n",
    "    <li> <b>-r</b> : Path to the results folder </li>\n",
    "    <li> <b>-m</b> : Path to the model </li>\n",
    "    <li> <b>-k</b> : User specified encryption key to use while saving/loading the model </li>\n",
    "    <li> Any overrides to the spec file eg. trainer.max_epochs </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training QuartzNet 15x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text train \\\n",
    "     -e $SPECS_DIR/speech_to_text/train_quartznet.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/quartznet/train \\\n",
    "     training_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n",
    "     trainer.max_epochs=1 \\\n",
    "     training_ds.num_workers=4 \\\n",
    "     validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Jasper 10x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text train \\\n",
    "     -e $SPECS_DIR/speech_to_text/train_jasper.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -r $RESULTS_DIR/jasper/train \\\n",
    "     training_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n",
    "     trainer.max_epochs=1 \\\n",
    "     training_ds.num_workers=4 \\\n",
    "     validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model trained, we need to check how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text evaluate \\\n",
    "     -e $SPECS_DIR/speech_to_text/evaluate.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/quartznet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/quartznet/evaluate \\\n",
    "     test_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained and evaluated and there is a need for fine tuning, the following command can be used to fine tune the ASR model. This step can also be used for transfer learning by making changes in the `train.json` and `dev.json` files to add new data.\n",
    "\n",
    "The list for customizations is same as the training parameters with the exception for parameters which affect the model architecture. Also, instead of `training_ds` we have `finetuning_ds`\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline.\n",
    "\n",
    "**Note: The finetune spec files contain specifics to finetune the English model we just trained to Russian. If you wish to proceed with English, please make the changes in the spec file *finetune.yaml* which you can find in the SPEC_DIR folder you mapped. Be sure to delete older finetuning checkpoints if you choose to change the language after finetuning it as is.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text finetune \\\n",
    "     -e $SPECS_DIR/speech_to_text/finetune.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/quartznet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/quartznet/finetune \\\n",
    "     finetuning_ds.manifest_filepath=$DATA_DIR/an4_converted/train_manifest.json \\\n",
    "     validation_ds.manifest_filepath=$DATA_DIR/an4_converted/test_manifest.json \\\n",
    "     trainer.max_epochs=1 \\\n",
    "     finetuning_ds.num_workers=20 \\\n",
    "     validation_ds.num_workers=20 \\\n",
    "     trainer.gpus=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR model export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TLT, you can also export your model in a format that can deployed using Nvidia RIVA, a highly performant application framework for multi-modal conversational AI services using GPUs! The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text export \\\n",
    "     -e $SPECS_DIR/speech_to_text/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/quartznet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/quartznet/export \\\n",
    "     export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to Riva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text export \\\n",
    "     -e $SPECS_DIR/speech_to_text/export.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/quartznet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/quartznet/riva \\\n",
    "     export_format=RIVA \\\n",
    "     export_to=asr-model.ejrvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have to work with the infer.yaml file to select the files you want for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text infer \\\n",
    "     -e $SPECS_DIR/speech_to_text/infer.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/quartznet/train/checkpoints/trained-model.tlt \\\n",
    "     -r $RESULTS_DIR/quartznet/infer \\\n",
    "     file_paths=[$path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Inference using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLT provides the capability to use the exported .eonnx model for inference. The command `tlt speech_to_text infer_onnx` is very similar to the inference command for .tlt models. Again, the inputs in the spec file used is just for demo purposes, you may choose to try out your custom input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt speech_to_text infer_onnx \\\n",
    "     -e $SPECS_DIR/speech_to_text/infer_onnx.yaml \\\n",
    "     -g 1 \\\n",
    "     -k $KEY \\\n",
    "     -m $RESULTS_DIR/quartznet/export/exported-model.eonnx \\\n",
    "     -r $RESULTS_DIR/infer_onnx \\\n",
    "     file_paths=[$path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You could use TLT to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_BOOTCAMP.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](speech-to-text-deployment.ipynb)\n",
    "[3](speechtotext-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](speech-to-text-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
