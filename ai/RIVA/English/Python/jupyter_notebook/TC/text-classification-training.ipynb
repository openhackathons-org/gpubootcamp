{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_HACKATHON.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](text-classification-deployment.ipynb)\n",
    "[3](text-classification-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](text-classification-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Transfer Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TLT to:\n",
    "- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) Text Classification model, [**Train**](#training) and [**Finetune**](#ft) it on the [SST-2 dataset](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)\n",
    "- Run [**Evaluation**](#evaluation) and [**Inference**](#inference)\n",
    "- [**Export**](#export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#export-riva) to deployment on [Riva](https://developer.nvidia.com/riva).\n",
    "\n",
    "The earlier sections in the notebook give a brief introduction to the Text Classification task and the SST-2 dataset. If you are already familiar with these, and want to jump right into the meat of the matter, you can start at section on [Download and preprocess the dataset](#prepare-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Please make sure that the steps in the setup notebook to install and deploy TLT are performed correctly\n",
    "\n",
    "For ease of use, please install TLT inside a python virtual environment. We recommend performing this step first and then launching the notebook from the virtual environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "### Task Description\n",
    "Text Classification is one of the most common tasks in NLP, which is the process of categorizing the text into a group of words. By using NLP, text classification can automatically analyze text and then assign a set of predefined tags or categories based on its context. It is applied in a wide variety of applications, including sentiment analysis, spam filtering, news categorization, domain/intent detection for dialogue systems, etc. The dataset we use in this notebook, [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip), pose this as a sentiment analysis task, i.e. given a piece of text, the goal is to estimate its sentiment polarity based solely on its content. \n",
    "\n",
    "For every entry in the training dataset, we predict the sentiment label of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning is a technique of extracting learned features from an existing neural network to a new one. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task, when creating a large training dataset is not feasible. \n",
    "\n",
    "**Transfer Learning Toolkit (TLT)** is a simple and easy-to-use python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with users' own data.\n",
    "\n",
    "\n",
    "![Transfer Learning Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)\n",
    "\n",
    "\n",
    "Let's see this in action with a use case for text classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "This model supports text classification problems such as sentiment analysis or domain/intent detection for dialogue systems, as long as the data follows the format specified below.\n",
    "\n",
    "The Text Classification Model requires the data to be stored in TAB separated files (.tsv) with two columns of sentence and label. Each line of the data file contains text sequences, where words are separated with spaces and label separated with [TAB], i.e.:\n",
    "\n",
    "    [WORD] [SPACE] [WORD] [SPACE] [WORD] [TAB] [LABEL]\n",
    "\n",
    "For example:\n",
    "\n",
    "    hide new secretions from the parental units [TAB] 0\n",
    "\n",
    "    that loves its characters and communicates something rather beautiful about human nature [TAB] 1\n",
    "\n",
    "    ...\n",
    "If your dataset is stored in another format, you need to convert it to this format to use the Text Classification Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The SST-2 Dataset\n",
    "\n",
    "The Stanford Sentiment Treebank dataset contains 215,154 phrases with fine-grained sentiment labels in the parse trees of 11,855 sentences from movie reviews. Model performances are evaluated either based on a fine-grained (5-way) or binary classification model based on accuracy. The [SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) Binary classification version is identical to SST-1, but with neutral reviews deleted.\n",
    "\n",
    "The SST-2 format consists of a .tsv file for each dataset split, i.e. train, dev, and test data. Each entry has a space-separated sentence, followed by a tab and a label.\n",
    "\n",
    "For example:\n",
    "\n",
    "    sentence\tlabel\n",
    "    \n",
    "    hexcruciatingly unfunny and pitifully unromantic \t0\n",
    "    \n",
    "    enriched by an imaginatively mixed cast of antic spirits \t1\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prepare-data'></a>\n",
    "### Download and preprocess the dataset \n",
    "\n",
    "We provide a function, download_sst2, for downloading the data. Please refer to the TLT workflow section for the [data preprocessing and conversion part](#dataset-convert)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset\n",
    "\n",
    "For convenience, you may use the code below to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# simple utility function to download the SST-2 dataset\n",
    "def download_sst2(save_path):\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    # url and filename of the dataset\n",
    "    download_urls = {\n",
    "            'https://dl.fbaipublicfiles.com/glue/data/SST-2.zip': 'SST-2.zip',\n",
    "    }\n",
    "         \n",
    "    for item in download_urls:\n",
    "        url = item\n",
    "        file = download_urls[item]\n",
    "        print('Downloading:', url)\n",
    "        if os.path.isfile(save_path + '/' + file):\n",
    "            print('** Download file already exists, skipping download **')\n",
    "        else:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            with open(save_path + '/' + file, \"wb\") as handle:\n",
    "                handle.write(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT Set the following path\n",
    "DATA_DOWNLOAD_DIR = \"<YOUR_PATH_TO_DATA_DIR>\"\n",
    "\n",
    "# This will download the SST-2 dataset\n",
    "download_sst2(DATA_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip -o $DATA_DOWNLOAD_DIR/SST-2.zip -d {DATA_DOWNLOAD_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell below, $DATA_DIR/SST-2 will contain the following 3 files and 1 folder:\n",
    "- dev.tsv\n",
    "- test.tsv\n",
    "- train.tsv\n",
    "- original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data is present\n",
    "!ls $DATA_DOWNLOAD_DIR/SST-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLT workflow\n",
    "The rest of the notebook shows what a sample TLT workflow looks like.\n",
    "\n",
    "### Setting TLT Mounts\n",
    "\n",
    "Now that our dataset has been downloaded, an important step in using TLT is to set up the directory mounts. The TLT launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. \n",
    "\n",
    "The launcher can be configured using the config file `~/.tlt_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TLT launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `.tlt_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container. **Please also ensure that the source directories exist on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tlt_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_CACHE_DIR eg. /home/user/.cache>\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir <YOUR_PATH_TO_SPECS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_RESULTS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_CACHE_DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TLT workflow. \n",
    "\n",
    "Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration/Specification Files\n",
    "\n",
    "The essence of all commands in TLT lies in the **YAML specification files**. There are sample specification files already available for you to use directly or as reference to create your own.\n",
    "\n",
    "Through these specification files, you can tune many knobs like the model, dataset, hyperparameters, optimizers, etc.\n",
    "\n",
    "Each command (like download_and_convert, train, finetune, evaluate, infer, etc.) should have a dedicated specification file with configurations pertinent to it. \n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "\n",
    "```\n",
    "trainer:\n",
    "  max_epochs: 100\n",
    "\n",
    "# Name of the .tlt file where trained model will be saved.\n",
    "save_to: trained-model.tlt\n",
    "\n",
    "model:\n",
    "  # Labels that will be used to \"decode\" predictions.\n",
    "  labels:\n",
    "    \"0\": \"negative\"\n",
    "    \"1\": \"positive\"\n",
    "\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    " language_model:\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null \n",
    "\n",
    "  classifier_head:\n",
    "    # This comes directly from number of labels/target classes.\n",
    "    num_output_layers: 2\n",
    "    fc_dropout: 0.1\n",
    "\n",
    "\n",
    "training_ds:\n",
    "  file_path: ???\n",
    "  batch_size: 64\n",
    "  shuffle: true\n",
    "  num_samples: -1 # number of samples to be considered, -1 means all the dataset\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TLT Docker. \n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR = '/data'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR = '/specs/text_classification'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR = '/results/text_classification'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is setup, we would like to take a bit of time to explain the tlt interface for ease of use. The command structure can be broken down as follows: `tlt <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt text_classification download_specs \\\n",
    "    -r $RESULTS_DIR \\\n",
    "    -o $SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset-convert'></a>\n",
    "### Dataset Convert\n",
    "\n",
    "The dataset conversion feature currently supports SST-2 and IMDB dataset. You need to specify the following parameters in the command:\n",
    "\n",
    "- dataset_name: \"sst2\" or \"imdb\"\n",
    "- source_data_dir: directory path for the downloaded dataset\n",
    "- target_data_dir: directory path for the processed dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. For BERT dataset conversion:\n",
    "!tlt text_classification dataset_convert \\\n",
    "    -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "    -r $SPECS_DIR/dataset_convert \\\n",
    "    dataset_name=sst2 source_data_dir=$DATA_DIR/SST-2 target_data_dir=$DATA_DIR/sst2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "### Training\n",
    "\n",
    "Training a model using TLT is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the `bert-base-uncased` pretrained model. Additionally, these configurations could easily be overridden using the tlt-launcher CLI as shown below. For instance, below we override the `training_ds.file`, `validation_ds.file`, `trainer.max_epochs`, `training_ds.num_workers` and `validation_ds.num_workers` configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide! <br>\n",
    "\n",
    "In order to get good results, you need to train for 20-50 epochs (depends on the size of the data). Training with 1 epoch in the tutorial is just for demonstration purposes.\n",
    "\n",
    "\n",
    "For training a Text Classification model in TLT, we use the `tlt text_classification train` command with the following args:\n",
    "- `-e`: Path to the spec file\n",
    "- `-g`: Number of GPUs to use\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n",
    "- Any overrides to the spec file eg. trainer.max_epochs\n",
    "\n",
    "More details about these arguments are present in the [TLT Getting Started Guide](https://docs.nvidia.com/tlt/tlt-user-guide/index.html) <br>\n",
    "`NOTE:` All file paths corresponds to the destination mounted directory that is visible in the TLT docker container used in backend.<br>\n",
    "\n",
    "Also worth noting is that the first time you run training on the dataset, it will run pre-processing and save that processed data in the same directory as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. For BERT training on SST-2:\n",
    "!tlt text_classification train \\\n",
    "    -e $SPECS_DIR/train.yaml \\\n",
    "    -g 1  \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/train \\\n",
    "    training_ds.file_path=$DATA_DIR/sst2/train.tsv \\\n",
    "    validation_ds.file_path=$DATA_DIR/sst2/dev.tsv \\\n",
    "    model.class_labels.class_labels_file=$DATA_DIR/sst2/label_ids.csv \\\n",
    "    trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`. This file can be fed directly into the fine-tuning stage as we see in the next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tips and tricks:\n",
    "- To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n",
    "- The batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ft'></a>\n",
    "### Fine-Tuning\n",
    "\n",
    "The command for fine-tuning is very similar to that of training. Instead of `tlt text_classification train`, we use `tlt text_classification finetune` instead. We also specify the spec file corresponding to fine-tuning. All commands in TLT follow a similar pattern, streamlining the workflow even further!\n",
    "\n",
    "`Note:` we use SST-2 dataset here to showcase how to use the fine-tuning command, however we recommend bringing your own data for fine-tuning on the pre-trained models. You would need to make sure your dataset is downloaded and pre-processed so that it's ready for use.\n",
    "\n",
    "`Note:` If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. For BERT finetuning on SST-2:\n",
    "!tlt text_classification finetune \\\n",
    "    -e $SPECS_DIR/finetune.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/finetune \\\n",
    "    finetuning_ds.file_path=$DATA_DIR/sst2/train.tsv \\\n",
    "    validation_ds.file_path=$DATA_DIR/sst2/dev.tsv \\\n",
    "    trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/finetune/checkpoints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec .yaml is as simple as:\n",
    "\n",
    "```\n",
    "test_ds:\n",
    "  file: ??? # e.g. $DATA_DIR/test.tsv\n",
    "  batch_size: 32\n",
    "  shuffle: false\n",
    "  num_samples: 500\n",
    "```\n",
    "\n",
    "Below, we use `tlt text_classification evaluate` and override the test data configuration by specifying `test_ds.file_path`. Other arguments follow the same pattern as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. For BERT evaluation on SST-2:\n",
    "!tlt text_classification evaluate \\\n",
    "    -e $SPECS_DIR/evaluate.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/evaluate \\\n",
    "    test_ds.file_path=$DATA_DIR/SST-2/dev.tsv \\\n",
    "    test_ds.batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On evaluating the model you will get some results, and based on that you can either retrain the model for more epochs, or continue with the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='inference'></a>\n",
    "### Inference\n",
    "Inference using a .tlt trained or fine-tuned model uses the `tlt text_classification infer` command.  <br>\n",
    "The infer.yaml is also straightforward, which includes some \"simulated\" user input, here we start with a batch of four samples. \n",
    "\n",
    "- \"by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\"\n",
    "- \"director rob marshall went out gunning to make a great one .\"\n",
    "- \"uneasy mishmash of styles and genres .\"\n",
    "- \"I love exotic science fiction / fantasy movies but this one was very unpleasant to watch . Suggestions and images of child abuse , mutilated bodies (live or dead) , other gruesome scenes , plot holes , boring acting made this a regretable experience , The basic idea of entering another person's mind is not even new to the movies or TV (An Outer Limits episode was better at exploring this idea) . i gave it 4 / 10 since some special effects were nice .\"\n",
    "\n",
    "We encourage you to try out custom inputs as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. For BERT inference on user data:\n",
    "!tlt text_classification infer \\\n",
    "    -e $SPECS_DIR/infer.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='export-onnx'></a>\n",
    "### Export to ONNX\n",
    "\n",
    "[ONNX](https://onnx.ai/) is a popular open format for machine learning models. It enables interoperability between different frameworks, making the path to production much easier. \n",
    "\n",
    "TLT provides commands to export the .tlt model to the ONNX format in an .eonnx archive. The `export_format` configuration can be set to `ONNX` to achieve this.\n",
    "\n",
    "Sample usage of the `tlt text_classification export` command is shown in the following code cell. The export.yaml file we use looks like:\n",
    "```\n",
    "# Name of the .tlt EFF archive to be loaded/model to be exported.\n",
    "restore_from: finetuned-model.tlt\n",
    "\n",
    "# Set export format: ONNX | JARVIS\n",
    "export_format: ONNX\n",
    "\n",
    "# Output EFF archive containing ONNX.\n",
    "export_to: exported-model.eonnx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. For export to ONNX:\n",
    "!tlt text_classification export \\\n",
    "    -e $SPECS_DIR/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/export \\\n",
    "    export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as `exported-model.eonnx` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using ONNX\n",
    "\n",
    "TLT provides the capability to use the exported .eonnx model for inference. \n",
    "\n",
    "The command `tlt text_classification infer_onnx` is very similar to the inference command for .tlt models. Again, the input file includes some \"simulated\" user input, we start with a batch of four samples for demo purposes, and encourage you to try out custom inputs as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. For inference using ONNX:\n",
    "!tlt text_classification infer_onnx \\\n",
    "    -e $SPECS_DIR/infer_onnx.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/export/exported-model.eonnx \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/infer_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='export-riva'></a>\n",
    "### Export to Riva\n",
    "\n",
    "With TLT, you can also export your model in a format that can deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs! The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. For export to Riva:\n",
    "!tlt text_classification export \\\n",
    "    -e $SPECS_DIR/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -k $KEY \\\n",
    "    -r $RESULTS_DIR/export_riva \\\n",
    "    export_format=JARVIS \\\n",
    "    export_to=tc-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `tc-model.riva` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "You can use TLT to build custom models for your own applications, or deploy the custom models to NVIDIA Riva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
