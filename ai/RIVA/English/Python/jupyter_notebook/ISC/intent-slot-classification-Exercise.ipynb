{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Learning Objectives\n",
    "In this notebook, you will learn how to:  \n",
    "- Use Riva ServiceMaker to take a TLT exported .riva and generate a model repository\n",
    "- Deploy the model(s) locally  on the Riva Server\n",
    "- Exercise: Send inference requests from a demo client using Riva API bindings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To follow along, please make sure:\n",
    "- You have access to NVIDIA NGC, and are able to download the Riva Quickstart [resources](https://ngc.nvidia.com/resources/ea-riva-stage:riva_quickstart/)\n",
    "- Have an .riva model file that you wish to deploy. You can obtain this from `tlt <task> export` (with `export_format=RIVA`). Please refer the tutorial on *Joint Intent Detection and Slot Filling using Transfer Learning Toolkit* for more details on training and exporting an .riva model.\n",
    "- You have followed the steps in the setup notebook to startup, initialize and deploy the Riva Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "!docker run --rm --gpus 1 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f /data/intent-slot.rmir:$KEY /data/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Start Riva Server\n",
    "Once the model repository is generated, we are ready to start the Riva server. From this step onwards you should follow the Riva setup notebook and modify the config.sh file as required, and proceed to the Inference step after deploying the Riva service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Inference\n",
    "Once the Riva server is up and running with your models, you can send inference requests querying the server. \n",
    "\n",
    "To send GRPC requests, the Riva Python API bindings for client can be used. This is available as a pip .whl which is installed in the setup notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sample shows how you can perform inference using Riva Python API gRPC bindings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n",
    "def run_intent_slot(grpc_server, query):\n",
    "    channel = grpc.insecure_channel(grpc_server)\n",
    "    riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "\n",
    "    for q in query:\n",
    "        req = rnlp.AnalyzeIntentRequest()\n",
    "        req.query = q\n",
    "        req.options.domain = \"default\" # The <domain_name> is appended to \"riva_intent_\" to look for a\n",
    "                                        # model \"riva_intent_<domain_name>\". So the model \"riva_intent_default\"\n",
    "                                        # needs to be preloaded in riva server. If you would like to deploy your\n",
    "                                        # custom Joint Intent and Slot model use the `--domain_name` parameter in\n",
    "                                        # ServiceMaker's `riva-build intent_slot` command.\n",
    "        resp = riva_nlp.AnalyzeIntent(req)\n",
    "        print(\"Query:\", q)\n",
    "        print(\"Intent:\", resp.intent)\n",
    "        print(\"Slots:\", resp.slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_intent_slot(grpc_server=\"localhost:50051\",\n",
    "                query=[\"Please set an alarm for 6 am\", \"Play some pop music\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE`: You could also run the above inference code from inside the Riva Client container. The QuickStart provides a script `riva_start_client.sh` to run the container. It has more examples for different services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can stop all docker container before shutting down the jupyter kernel. **Caution: The following command will stop all running containers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop $(docker ps -a -q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The AnalyzeIntent API can be used to query a Intent Slot classifier. The API can leverage a\n",
    "# text classification model to classify the domain of the input query and then route to the \n",
    "# appropriate intent slot model.\n",
    "\n",
    "# Lets first see an example where the domain is known. This skips execution of the domain classifier\n",
    "# and proceeds directly to the intent/slot model for the requested domain.\n",
    "\n",
    "req = rnlp.AnalyzeIntentRequest()\n",
    "req.query = \"How is the humidity in San Francisco?\"\n",
    "req.options.domain = \"weather\"  # The <domain_name> is appended to \"riva_intent_\" to look for a \n",
    "                                # model \"riva_intent_<domain_name>\". So in this e.g., the model \"riva_intent_weather\"\n",
    "                                # needs to be preloaded in riva server. If you would like to deploy your \n",
    "                                # custom Joint Intent and Slot model use the `--domain_name` parameter in \n",
    "                                # ServiceMaker's `riva-build intent_slot` command.\n",
    "\n",
    "resp = riva_nlp.AnalyzeIntent(req)\n",
    "print(resp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is an example where the input domain is not provided.\n",
    "\n",
    "req = rnlp.AnalyzeIntentRequest()\n",
    "req.query = \"Is it going to rain tomorrow?\"\n",
    "\n",
    "        # The input query is first routed to the a text classification model called \"riva_text_classification_domain\"\n",
    "        # The output class label of \"riva_text_classification_domain\" is appended to \"riva_intent_\"\n",
    "        # to get the appropriate Intent Slot model to execute for the input query.\n",
    "        # Note: The model \"riva_text_classification_domain\" needs to be loaded into Riva server and have the appropriate\n",
    "        # class labels that would invoke the corresponding intent slot model.\n",
    "\n",
    "resp = riva_nlp.AnalyzeIntent(req)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weather Intent queries grouped together\n",
    "queries = [\n",
    "    \"Is it currently cloudy in Tokyo?\",\n",
    "    \"What is the annual rainfall in Pune?\",\n",
    "    \"What is the humidity going to be tomorrow?\"\n",
    "]\n",
    "for q in queries:\n",
    "    req = rnlp.AnalyzeIntentRequest()\n",
    "    req.query = q\n",
    "    start = time()\n",
    "    resp = riva_nlp.AnalyzeIntent(req)\n",
    "\n",
    "    print(f\"[{resp.intent.class_name}]\\t{req.query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use the Intent Slot classification API to classify individual Intents and Slots for the following utterances:\n",
    "\n",
    "\n",
    "- I need to catch the early train tomorrow evening.\n",
    "- I am very tired after all the work done yesterday.\n",
    "- Will it rain tomorrow in New York?\n",
    "- Is it currently sunny in Chicago?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##first, import required libraries\n",
    "\n",
    "\n",
    "\n",
    "## next, setup service configuration\n",
    "\n",
    "\n",
    "## next, setup request variables\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
