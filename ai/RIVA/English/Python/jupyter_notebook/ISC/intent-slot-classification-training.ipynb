{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_HACKATHON.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](intent-slot-classification-deployment.ipynb)\n",
    "[3](intent-slot-classification-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](intent-slot-classification-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-task-description'></a>\n",
    "## Joint Intent Detection and Slot Filling\n",
    "\n",
    "### Task Description\n",
    "\n",
    "Understanding the intent in natural language (Intent Classification) and extracting values of pertinent attributes or specific pieces of information from a sentence (Slot Filling) are two essential tasks in Natural Language Understanding (NLU). For example: <br>\n",
    "\n",
    "> In the query:  *What is the weather in Santa Clara tomorrow morning?*\n",
    "> we would like to classify the query as a `weather` Intent,\n",
    "> and detect `Santa Clara` as a location slot and tomorrow morning as a `date_time` slot. Intents and Slots names are usually task specific and defined as labels in the training data. This is a fundamental step that is executed in any task-driven Conversational Assistant. <br>\n",
    "\n",
    "Recent research has shown the proficiency of BERT models in this task. TLT provides the capability to train a BERT model and perform inference for both intent detection and slot filling together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model\n",
    "In this notebook, we will show how to use a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) (Bidirectional Encoder Representations from Transformers) model for Joint Intent and Slot Classification leveraging TLT. The BERT model has made major breakthroughs in Natural Language Understanding in recent years. For most applications, the model is typically trained in two phases, pre-training and fine-tuning. \n",
    "- The BERT core model can be pre-trained on large, generic datasets to generate dense vector representations of input sentence(s). \n",
    "- It can be quickly fine-tuned to perform a wide variety of tasks such as question/answering, sentiment analysis, or named entity recognition.\n",
    "\n",
    "The figure below shows a high-level block diagram of pre-training and fine-tuning BERT.\n",
    "<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/05/bert-model-625x268.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alignment with the above, for pre-training we can take one of two approaches. We can either pre-train the BERT model with our own data, or use a model pre-trained by Nvidia. After we obtain a pre-trained model, the next step would be to fine-tune it for the Intent and Slot Classification task and run inference on the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/Fig4revised-625x340.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TLT to:\n",
    "- Pre-process/convert a dataset for the [**Joint Intent and Slot Classification**](#isc-task-description).\n",
    "- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) model and [**Train/Finetune**](#isc-training) it on the [NLU Evaluation](https://github.com/xliuhw/NLU-Evaluation-Data) dataset\n",
    "- Run [**Inference**](#isc-inference)\n",
    "- [**Export**](#isc-export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#isc-export-riva) in a format suitable for deployment in [Riva](https://developer.nvidia.com/riva).\n",
    "\n",
    "The earlier sections in the notebook give a brief introduction to the Intent and Slot Classification task, the NLU Evaluation dataset and BERT. If you are already familiar with these, and want to jump right in, you can start at section on [Data Preparation](#isc-prepare-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-requisites\n",
    "For ease of use, please install TLT inside a python virtual environment. We recommend performing this step first and then launching the notebook from the virtual environment. This can be done using the Setup notebook (../riva-setup.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-prepare-data'></a>\n",
    "### Preparing the dataset\n",
    "#### The NLU Evaluation Dataset\n",
    "For this tutorial, we use a virtual assistant interaction in home domain - `NLU Evaluation` dataset. The NLU dataset is available [here](https://github.com/xliuhw/NLU-Evaluation-Data). It was collected and annotated for various NLU tasks by Liu et. al in their IWSDS 2019 [paper](https://arxiv.org/abs/1903.05566), and more information about this dataset is present in the Github README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available in the github [repo](https://github.com/xliuhw/NLU-Evaluation-Data) and can be downloaded directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Set path to a folder where you want you data and results to be saved\n",
    "DATA_DOWNLOAD_DIR = \"<YOUR_PATH_TO_DATA_DIR>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Ensure that wget and unzip utilities are available. If not, please install them\n",
    "!wget 'https://github.com/xliuhw/NLU-Evaluation-Data/archive/master.zip' -P $DATA_DOWNLOAD_DIR\n",
    "\n",
    "# Extract the data\n",
    "!unzip $DATA_DOWNLOAD_DIR/master.zip -d $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Intent Detection and Slot Filling using Transfer Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transfer learning* is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task.\n",
    "\n",
    "**Transfer Learning Toolkit (TLT)** is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data. Developers, researchers and software partners building Conversational AI and Vision AI can leverage TLT to avoid the hassle of training from scratch, and significantly accelerate their workflow. \n",
    "\n",
    "<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\"><\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TLT workflow\n",
    "The rest of the notebook shows what a sample TLT workflow looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting TLT Mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been downloaded, an important step in using TLT is to set up the directory mounts. The TLT launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tlt_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TLT launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tlt_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container. **Please also ensure that the source directories exist on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tlt_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_CACHE_DIR eg. /home/user/.cache>\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir <YOUR_PATH_TO_SPECS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_RESULTS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_CACHE_DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TLT workflow. Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of all commands in TLT lies in the YAML spec files. There are sample spec files already available for you to use directly or as reference to create your own.  Through these spec files, you can tune many knobs like the model, dataset, hyperparameters, optimizer etc. Each command (like train, finetune, evaluate etc.) should have a dedicated spec file with configurations pertinent to it. <br>\n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "---\n",
    "```\n",
    "# Name of the file where trained model will be saved.\n",
    "save_to: trained-model.tlt\n",
    "\n",
    "optim:\n",
    "  name: adam\n",
    "  lr: 2e-5\n",
    "  # optimizer arguments\n",
    "  betas: [0.9, 0.999]\n",
    "  weight_decay: 0.01\n",
    "\n",
    "  # scheduler setup\n",
    "  sched:\n",
    "    name: WarmupAnnealing\n",
    "    # Scheduler params\n",
    "    warmup_steps: null\n",
    "    warmup_ratio: 0.1\n",
    "    last_epoch: -1\n",
    "    # pytorch lightning args\n",
    "    monitor: val_loss\n",
    "    reduce_on_plateau: false\n",
    "\n",
    "model:\n",
    "  class_balancing: null # choose from [null, weighted_loss]. weighted_loss enables the weighted class balancing of the loss, may be used for handling unbalanced classes\n",
    "  intent_loss_weight: 0.6 # relation of intent to slot loss in total loss (between 0 to 1)\n",
    "  pad_label: -1 # if -1 not slot token will be used\n",
    "  ignore_extra_tokens: false\n",
    "  ignore_start_end: true # do not use first and last token for slot training\n",
    "\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    "  language_model:\n",
    "    max_seq_length: 50\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null\n",
    "\n",
    "  head:\n",
    "    num_output_layers: 2\n",
    "    fc_dropout: 0.1\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths\n",
    "Please set these paths according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TLT Docker. \n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR='/data'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR='/specs/intent_slot_classification'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR='/results/intent_slot_classification'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY='tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification download_specs \\\n",
    "    -r $RESULTS_DIR \\\n",
    "    -o $SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Convert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for training/fine-tuning, we need to preprocess the dataset. `tlt intent_slot_classification dataset_convert` command can be used in conjunction with appropriate configuration in the spec file. Here is the sample `dataset_convert.yaml` spec file we use:\n",
    "```\n",
    "# Dataset. Available options: [assistant]\n",
    "dataset_name: assistant\n",
    "\n",
    "# Path to the folder containing the dataset source files.\n",
    "source_data_dir: ???\n",
    "\n",
    "# Path to the output folder.\n",
    "target_data_dir: ???\n",
    "\n",
    "```\n",
    " We encourage you to take a look at the .yaml spec files we provide!\n",
    "As we show below, you can override the `source_data_dir` and `target_data_dir` options with appropriate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification dataset_convert \\\n",
    "                                -e $SPECS_DIR/dataset_convert.yaml \\\n",
    "                                -r $RESULTS_DIR/dataset_convert \\\n",
    "                                source_data_dir=$DATA_DIR/NLU-Evaluation-Data-master \\\n",
    "                                target_data_dir=$DATA_DIR/NLU-Evaluation-Data-processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command writes the processed assistant commands along with train/val splits dataset at the specific `target_data_dir`. With this dataset, it found 64 intents and 55 slot types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-training'></a>\n",
    "### Training / Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model using TLT is as simple as configuring your spec file and running the train command. The code cell below uses the train.yaml spec file available for users as reference. It is configured by default to use the pretrained `bert-base-uncased` model. For this task, you will almost always use the pretrained BERT language models, and train for this task with your custom data. In this sense, this step could also be thought of as fine-tuning. Typically, to get good results you may need to train the model for 20-50 epochs depending on the size of the data.  Training with your own data will take about 15-30 mins on a single GPU. Since this is a demonstration, we train for just 1 epoch below.\n",
    "\n",
    "The spec file configurations can easily be overridden using the tlt-launcher CLI as shown below. For instance, below we override the `data_dir`, `trainer.max_epochs`, `training_ds.num_workers` and `validation_ds.num_workers` configurations to suit our needs. <br>\n",
    "\n",
    "For training a Joint Intent Detection and Slot Classification model in TLT, we use the `tlt intent_slot_classification train` command with the following args:\n",
    "- `-e`: Path to the spec file\n",
    "- `-g`: Number of GPUs to use\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n",
    "- Any overrides to the spec file eg. `trainer.max_epochs`\n",
    "<br>\n",
    "\n",
    "\n",
    "More details about these arguments are present in the [TLT Getting Started Guide](https://docs.nvidia.com/tlt/tlt-user-guide/index.html) <br>\n",
    "`Note:` All file paths correspond to the destination mounted directory that is visible in the TLT docker container used in backend.<br>\n",
    "\n",
    "`Note:` If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification train \\\n",
    "                                -e $SPECS_DIR/train.yaml \\\n",
    "                                -g 1 \\\n",
    "                                -k $KEY \\\n",
    "                                -r $RESULTS_DIR/train \\\n",
    "                                data_dir=$DATA_DIR/NLU-Evaluation-Data-processed \\\n",
    "                                trainer.max_epochs=1 \\\n",
    "                                training_ds.num_workers=4 \\\n",
    "                                validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec .yaml is as simple as:\n",
    "\n",
    "```\n",
    "# Name of the .tlt file where trained model will be restored from.\n",
    "restore_from: trained-model.tlt\n",
    "\n",
    "data_dir: ???\n",
    "\n",
    "test_ds:\n",
    "  prefix: test\n",
    "  batch_size: 32\n",
    "  shuffle: false\n",
    "  num_samples: -1\n",
    "  num_workers: 2\n",
    "  drop_last: false\n",
    "  pin_memory: false\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification evaluate \\\n",
    "                                -e $SPECS_DIR/evaluate.yaml \\\n",
    "                                -g 1 \\\n",
    "                                -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                                -k $KEY \\\n",
    "                                -r $RESULTS_DIR/evaluate \\\n",
    "                                data_dir=$DATA_DIR/NLU-Evaluation-Data-processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Evaluation should give the precision, recall, and f1 report for intents and slots. Remember that we had trained for just 1 epoch since this is a demonstration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-inference'></a>\n",
    "### Inference\n",
    "Inference using a .tlt trained or fine-tuned model uses the `tlt intent_slot_classification infer` command.  <br>\n",
    "The infer.yaml is also very simple, and we can directly give inputs for the model to run inference.\n",
    "```\n",
    "# \"Simulate\" user input:\n",
    "input_batch:\n",
    "  - 'set alarm for seven thirty am'\n",
    "  - 'lower volume by fifty percent'\n",
    "  - 'what is my schedule for tomorrow'\n",
    "\n",
    "```\n",
    "\n",
    "We encourage you to try out your own inputs as an exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification infer \\\n",
    "                                -e $SPECS_DIR/infer.yaml \\\n",
    "                                -g 1 \\\n",
    "                                -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                                -r $RESULTS_DIR/infer \\\n",
    "                                -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command returns the predicted intents and slots for each of the input sequences. Of course, these intents and slots are what it was trained on. You may see a full list of intents and slots in the processed data directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-export-onnx'></a>\n",
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ONNX](https://onnx.ai/) is a popular open format for machine learning models. It enables interoperability between different frameworks, easing the path to production. TLT provides commands to export the .tlt model to the ONNX format in an .eonnx archive.  \n",
    "\n",
    "Sample usage of the `tlt intent_slot_classification export` command is shown in the following code cell. The `export_format` configuration can be set to `ONNX` to achieve this.\n",
    "\n",
    "The export.yaml file we use looks like:\n",
    "```\n",
    "# Name of the .tlt EFF archive to be loaded/model to be exported.\n",
    "restore_from: trained-model.tlt\n",
    "\n",
    "# Set export format: ONNX | JARVIS\n",
    "export_format: ONNX\n",
    "\n",
    "# Output EFF archive containing ONNX.\n",
    "export_to: exported-model.eonnx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification export \\\n",
    "                        -e $SPECS_DIR/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -r $RESULTS_DIR/export \\\n",
    "                        -k $KEY \\\n",
    "                        export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as `exported-model.eonnx` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLT provides the capability to use the exported .eonnx model for inference. The command `tlt intent_slot_classification infer_onnx` is very similar to the inference command for .tlt models. Again, the inputs in the spec file used are just for demo purposes, you may choose to try out your custom input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification infer_onnx \\\n",
    "                        -e $SPECS_DIR/infer_onnx.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/export/exported-model.eonnx \\\n",
    "                        -r $RESULTS_DIR/infer_onnx \\\n",
    "                        -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='isc-export-riva'></a>\n",
    "### Export to RIVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TLT, you can also export your model in a format that can deployed using [Nvidia Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs! The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt intent_slot_classification export \\\n",
    "                        -e $SPECS_DIR/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -r $RESULTS_DIR/export_riva \\\n",
    "                        -k $KEY \\\n",
    "                        export_format=JARVIS \\\n",
    "                        export_to=intent-slot-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `slot-model.riva` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use TLT to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
