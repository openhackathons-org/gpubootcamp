{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_HACKATHON.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](punctuation-and-capitalization-deployment.ipynb)\n",
    "[3](punctuation-and-capitalization-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](punctuation-and-capitalization-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation And Capitalization using Transfer Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TLT to:\n",
    "- Take a BERT model and __Train/Finetune__ it on a dataset for Punctuation and Capitalization task\n",
    "- Run __Inference__\n",
    "- __Export__ the model to the ONNX format, or export in the format that is suitable for deployment in Riva\n",
    "\n",
    "The earlier section in this notebook gives a brief introduction to the Punctuation and Capitalization task and the dataset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Please follow the steps shown in the setup notebook to install the TLT package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check for tlt install and availabiility\n",
    "!tlt info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the GPU device(s) are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation And Capitalization using TLT \n",
    "### Task Description\n",
    "\n",
    "Automatic Speech Recognition (ASR) systems typically generate text with no punctuation and capitalization of the words. This tutorial explains how to implement a model that will predict punctuation and capitalization for each word in a sentence to make ASR output more readable and to boost performance of the named entity recognition, machine translation or text-to-speech models. We'll show how to train a model for this task using a pre-trained BERT model. For every word in our training dataset weâ€™re going to predict:\n",
    "\n",
    "- punctuation mark that should follow the word \n",
    "- whether the word should be capitalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### TLT Workflow\n",
    "### Setting TLT Mounts\n",
    "\n",
    "Once the TLT gets installed, the next step is to set up the directory mounts that is the ``source`` and ``destination``. The ``source_mount`` will store all the data, pre-trained models and scripts pertaining to Punctuation and Capitalization task. Please select an empty directory for ``source_mount``. \n",
    "The ``destination_mount`` is the directory to which ``source_mount`` will be mapped to, inside the container.\n",
    "\n",
    "The TLT launcher uses docker container under the hood, and for our data and results directory to be visible to the docker, they must be mapped.\n",
    "\n",
    "The launcher can be configured using the config file ``.tlt_mounts.json``. Apart from the mounts you can also configure additional options like the Environmental Variables and amount of Shared Memory available to the TLT launcher.\n",
    "\n",
    "``Important Note:`` The code below creates a sample ``.tlt_mounts.json`` file. Here, we can map directories in which we save the data, specs, results and cache.  You must configure it for your specific case to make sure both your data and results are correctly mapped to the docker. **Please also ensure that the source directories exist on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tlt_mounts.json <<'EOF' \n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_CACHE_DIR>\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir <YOUR_PATH_TO_SPECS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_RESULTS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_CACHE_DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TLT workflow. \n",
    "Users with any level of Deep Learning knowledge can get started building their own custom models using a simple specification file. It's essentially just one command each to run data download and preprocessing, training, fine-tuning, evaluation, inference, and export. \n",
    "All configurations happen through YAML specification files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files\n",
    "\n",
    "All commands in TLT lies in the YAML specification files. There are sample specification files already available for you to use directly or as reference to create your own YAML specification files.  \n",
    "\n",
    "Through these specification files, you can tune many a lot of things like the model, dataset, hyperparameters, optimizer etc.\n",
    "\n",
    "Each command (like download_and_convert, train, finetune, evaluate etc.) should have a dedicated specification file with configurations pertinent to it.\n",
    "\n",
    "Here is an example of the training spec file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "save_to: trained-model.tlt\n",
    "\n",
    "trainer:\n",
    "  max_epochs: 5\n",
    "  \n",
    "model:\n",
    "  punct_label_ids:\n",
    "    O: 0\n",
    "    ',': 1\n",
    "    '.': 2\n",
    "    '?': 3\n",
    "\n",
    "  capit_label_ids:\n",
    "    O: 0\n",
    "    U: 1 \n",
    "\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece =\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    "  language_model:\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null \n",
    "\n",
    "  punct_head:\n",
    "    punct_num_fc_layers: 1\n",
    "    fc_dropout: 0.1\n",
    "    activation: 'relu'\n",
    "    use_transformer_init: true\n",
    "\n",
    "  capit_head:\n",
    "    capit_num_fc_layers: 1\n",
    "    fc_dropout: 0.1\n",
    "    activation: 'relu'\n",
    "    use_transformer_init: true\n",
    "\n",
    "# Data dir containing dataset.\n",
    "data_dir: ???\n",
    "\n",
    "training_ds:\n",
    "  text_file: text_train.txt\n",
    "  labels_file: labels_train.txt\n",
    "  shuffle: true\n",
    "  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset\n",
    "  batch_size: 64\n",
    "\n",
    "validation_ds:\n",
    "  text_file: text_dev.txt\n",
    "  labels_file: labels_dev.txt\n",
    "  shuffle: false\n",
    "  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset\n",
    "  batch_size: 64\n",
    "\n",
    "optim:\n",
    "  name: adam\n",
    "  lr: 1e-5\n",
    "  weight_decay: 0.00\n",
    "\n",
    "  sched:\n",
    "    name: WarmupAnnealing\n",
    "    # Scheduler params\n",
    "    warmup_steps: null\n",
    "    warmup_ratio: 0.1\n",
    "    last_epoch: -1\n",
    "\n",
    "    # pytorch lightning args\n",
    "    monitor: val_loss\n",
    "    reduce_on_plateau: false \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths\n",
    "Please set these paths according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TLT Docker. \n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR='/data'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR='/specs/punctuation_and_capitalization'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR='/results/punctuation_and_capitalization'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY='tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt punctuation_and_capitalization download_specs \\\n",
    "    -r $RESULTS_DIR/ \\\n",
    "    -o $SPECS_DIR/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dataset \n",
    "\n",
    "This model can work with any dataset as long as it follows the format specified below. The training and evaluation data is divided into _2 files: text.txt and labels.txt_. Each line of the __text.txt__ file contains text sequences, where words are separated with spaces: [WORD] [SPACE] [WORD] [SPACE] [WORD], for example:\n",
    "\n",
    "when is the next flight to new york<br>\n",
    "the next flight is ...<br>\n",
    "...<br>\n",
    "\n",
    "The __labels.txt__ file contains corresponding labels for each word in text.txt, the labels are separated with spaces. Each label in labels.txt file consists of 2 symbols:\n",
    "\n",
    "- the first symbol of the label indicates what punctuation mark should follow the word (where O means no punctuation needed);\n",
    "- the second symbol determines if a word needs to be capitalized or not (where U indicates that the word should be upper cased, and O - no capitalization needed.)\n",
    "In this tutorial, we are considering only commas, periods, and question marks the rest punctuation marks were removed. To use more punctuation marks, update the dataset to include desired labels, no changes to the model needed.\n",
    "\n",
    "Each line of the __labels.txt__ should follow the format: [LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for labels.txt). For example, labels for the above text.txt file should be:\n",
    "\n",
    "OU OO OO OO OO OO OU ?U<br>\n",
    "OU OO OO OO ...<br>\n",
    "...\n",
    "\n",
    "The complete list of all possible labels for this task used in this tutorial is: OO, ,O, .O, ?O, OU, ,U, .U, ?U."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess the data \n",
    "\n",
    "In this notebook we are going to use a subset of English examples from the [Tatoeba collection of sentences](https://tatoeba.org/eng). \n",
    "\n",
    "Downloading and preprocessing the data using TLT is as simple as configuring YAML specification file and running the ``download_and_convert_tatoeba`` command. The code cell below uses the default `download_and_convert_tatoeba.yaml` available for the users as a reference. \n",
    "\n",
    "The configurations in the specification file can be easily overridden using the tlt-launcher CLI as shown below. For instance, we override the ``source_data_dir`` and ``target_data_dir`` configurations.\n",
    "\n",
    "We encourage you to take a look at the YAML files we have provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell below, your data folder will contain the following 4 files:\n",
    "- labels_dev.txt\n",
    "- labels_train.txt\n",
    "- text_dev.txt\n",
    "- text_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To download and convert the dataset\n",
    "!tlt punctuation_and_capitalization download_and_convert_tatoeba \\\n",
    "    -e $SPECS_DIR/download_and_convert_tatoeba.yaml \\\n",
    "    -r $RESULTS_DIR/download_and_convert_tatoeba \\\n",
    "    source_data_dir=$DATA_DIR \\\n",
    "    target_data_dir=$DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Training \n",
    "\n",
    "In the Punctuation and Capitalization Model, we are jointly training two token-level classifiers on top of the pretrained [BERT](https://arxiv.org/pdf/1810.04805.pdf) model:\n",
    "\n",
    "- one classifier to predict punctuation and\n",
    "- the other one - capitalization.\n",
    "\n",
    "Training a model using TLT is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the ``bert-base-uncased`` pretrained model. Additionally, these configurations could easily be overridden using the tlt-launcher CLI as shown below. For instance, below we override the trainer.max_epochs, training_ds.num_samples and validation_ds.num_samples configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide!\n",
    "\n",
    "The command for training is very similar to the of ``download_and_convert_tatoeba``. Instead of ``tlt punctuation_and_capitalization download_and_convert_tatoeba``, we use ``tlt punctuation_and_capitalization train`` instead. The  ``tlt punctuation_and_capitalization train`` command has the following arguments:\n",
    "\n",
    "- ``-e`` : Path to the spec file\n",
    "- ``-g`` : Number of GPUs to use\n",
    "- ``-k`` : User specified encryption key to use while saving/loading the model\n",
    "- ``-r`` : Path to the folder where the outputs should be written. Make sure this is mapped in the tlt_mounts.json\n",
    "- Any overrides to the spec file eg. trainer.max_epochs\n",
    "\n",
    "More details about these arguments are present in the  [TLT Getting Started Guide](https://docs.nvidia.com/tlt/tlt-user-guide/index.html).\n",
    "\n",
    "``NOTE:`` All file paths corresponds to the destination mounted directory that is visible in the TLT docker container used in backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To train the dataset with BERT-base-uncased model\n",
    "!tlt punctuation_and_capitalization train \\\n",
    "    -e $SPECS_DIR/train.yaml \\\n",
    "    -g 4 \\\n",
    "    -r $RESULTS_DIR/train \\\n",
    "    data_dir=$DATA_DIR \\\n",
    "    trainer.max_epochs=2 \\\n",
    "    training_ds.num_samples=-1  \\\n",
    "    validation_ds.num_samples=-1 \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called ``trained-model.tlt`` saved at ``$RESULTS_DIR/checkpoints/trained-model.tlt``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tips and tricks:\n",
    "\n",
    "- To accelerate the training without loss of quality, it is possible to train with these parameters: ``trainer.amp_level=\"O1\"`` and ``trainer.precision=16`` for reduced precision.\n",
    "- The batch size ``training_ds.batch_size`` may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches.\n",
    "- You can also change the optimizer parameter ``optim.name`` and can see its effect on the punctuation and capitalization task by the change in accuracy.\n",
    "- You can specify the number of layers in the head of the model ``model.punct_head.punct_num_fc_layers`` and ``model.capit_head.capit_num_fc_layers``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finetuning\n",
    "\n",
    "As stated above the command for all the tasks are very similar but have different YAML specification files that can be tweaked.\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To finetune on the dataset\n",
    "!tlt punctuation_and_capitalization finetune \\\n",
    "    -e $SPECS_DIR/finetune.yaml \\\n",
    "    -g 4 \\\n",
    "    -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "    -r $RESULTS_DIR/finetune \\\n",
    "    data_dir=$DATA_DIR \\\n",
    "    trainer.max_epochs=3 \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called ``finetuned-model.tlt`` saved in the results directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation\n",
    "\n",
    "To evaluate our TLT model we will run the command below. It is always advisable to look at the YAML file for evaluate to understand the command in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For evaluation\n",
    "!tlt punctuation_and_capitalization evaluate \\\n",
    "    -e $SPECS_DIR/evaluate.yaml \\\n",
    "    -g 4 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    data_dir=$DATA_DIR \\\n",
    "    -r $RESULTS_DIR/evaluate \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On evaluating the model you will get some results and based on that we can either retrain the model for more epochs or continue with the inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference\n",
    "\n",
    "Inference using a TLT trained and fine-tuned model can be done by ``tlt punctuation_and_capitalization infer`` command. It is again advisable to look at the infer.yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For inference\n",
    "!tlt punctuation_and_capitalization infer \\\n",
    "    -e $SPECS_DIR/infer.yaml \\\n",
    "    -g 4 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -r $RESULTS_DIR/infer \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export to ONNX\n",
    "[ONNX](https://onnx.ai/) is a popular open format for machine learning models. It enables interoperability between different frameworks, making the path to production much easier. \n",
    "\n",
    "TLT provides commands to export the .tlt model to the ONNX format in an .eonnx archive. The `export_format` configuration can be set to `ONNX` to achieve this.\n",
    "\n",
    "The tlt export command for ``punctuation_and_capitalization`` is shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For export to ONNX\n",
    "!tlt punctuation_and_capitalization export \\\n",
    "    -e $SPECS_DIR/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -r $RESULTS_DIR/export \\\n",
    "    -k $KEY \\\n",
    "    export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as ``exported-model.eonnx`` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference using ONNX\n",
    "\n",
    "TLT provides the capability to use the exported .eonnx model for inference. The command ``tlt punctuation_and_capitalization infer_onnx`` is very similar to the inference command for .tlt models. Again, the input file used is just for demo purposes, you may choose to try out your own custom input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For inference using ONNX\n",
    "!tlt punctuation_and_capitalization infer_onnx \\\n",
    "    -e $SPECS_DIR/infer_onnx.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/export/exported-model.eonnx \\\n",
    "    -r $RESULTS_DIR/infer_onnx \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export to Riva\n",
    "\n",
    "With TLT, you can also export your model in a format that can deployed using [NVIDIA Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs. The same command for exporting to ONNX can be used here. The only small variation is the configuration for ``export_format`` in the spec file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For export to RIVA\n",
    "!tlt punctuation_and_capitalization export \\\n",
    "    -e $SPECS_DIR/export.yaml \\\n",
    "    -g 1 \\\n",
    "    -m $RESULTS_DIR/finetune/checkpoints/finetuned-model.tlt \\\n",
    "    -r $RESULTS_DIR/export_riva \\\n",
    "    export_format=JARVIS \\\n",
    "    export_to=punct-capit-model.riva \\\n",
    "    -k $KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as ``punct-capit-model.riva`` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What next?\n",
    "\n",
    "You can use TLT to build custom models for your own NLP applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
