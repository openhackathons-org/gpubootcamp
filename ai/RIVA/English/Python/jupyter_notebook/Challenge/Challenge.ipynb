{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe17e068",
   "metadata": {},
   "source": [
    "# Challenge - Building a Semantic Index and Knowledge Base from Video Data\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This notebook walks through an end-to-end GPU enabled workflow where a semantic index and knowledge base is built from data available in video format; using tools found in the Riva Framework.\n",
    "\n",
    "After completing this excercise, you will be able to use Riva to transcribe video data, build an index for keywords and entities found in the videos, and construct a basic knowledge base from the provided data. \n",
    "\n",
    "\n",
    "It is not required that the user is familiar with Riva beforehand. Since our aim is to go from raw videos to a knowledge base, a detailed introduction is out of scope for this notebook. We recommend [Introduction to Riva](../Introduction_to_Riva.ipynb) for additional information.\n",
    "\n",
    "### 1.2. Problem statement\n",
    "\n",
    "We are trying to answer the following questions, using videos from the GTC 2021 special address on healthcare as a source of data.\n",
    "\n",
    "0: how is drug discovery traditionally done?\n",
    "\n",
    "1: how many compounds are inferred in the molecular latent space learned?\n",
    "\n",
    "2: what type of model was used to generate chemistry?\n",
    "\n",
    "3: How many parameters does the world's largest clinical model have?\n",
    "\n",
    "4: how many drug candidates can be simulated per year using the a100 MIG architecture?\n",
    "\n",
    "5: how many new models are part of Clara discovery?\n",
    "\n",
    "6: where does medical data flow in from?\n",
    "\n",
    "7: how much data do hospitals generate each year?\n",
    "\n",
    "8: what is Nvidia Clara?\n",
    "\n",
    "9: how many models does Clara have?\n",
    "\n",
    "10: how much of the population can AI driven diagnostic devices reach?\n",
    "\n",
    "We will first use a small example video to answer a simple sample question, and use this as a template to answer the questions posed in the problem statement.\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Why RIVA?\n",
    "\n",
    "Riva allows us to transcribe audio data into text using ASR, and we can gain insights from these transcripts using NLP methods provided by Jarvis such as Named Entity Recognition and Question Answering\n",
    "\n",
    "\n",
    "### 1.2.1 References\n",
    "\n",
    "\n",
    "Dataset sources:\n",
    "- NVIDIA Youtube Channel\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e2f497",
   "metadata": {},
   "source": [
    "## 1. Sample Exercise\n",
    "Let us use the following file as a sample input:\n",
    "Sample Video (The Amazon Rainforest)\n",
    "\n",
    "For convenience, we have converted the youtube video into a WAV file which can be used in our pipeline\n",
    "The code for this process is found in section 2 \n",
    "\n",
    "Sample Audio(../data/amazon-rainforest.wav)\n",
    "\n",
    "For this sample, we will build a list of the Entities found in the transcript, and answer the following 2 questions:\n",
    "\n",
    "1: What is the Amazon Rainforest?\n",
    "\n",
    "2: How many reptile species are found in the Amazon Rainforest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf720971",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first let us preview the video \n",
    "## use builtin IPython to playback youtube\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# Sample Video\n",
    "IFrame(\"https://www.youtube.com/embed/M_9xIVfXA1w?rel=0&amp;controls=0&amp;showinfo=0\", width=\"560\", height=\"315\", frameborder=\"0\", allowfullscreen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to convert from Youtube URL to audio file\n",
    "## Warning! do not use this for copyright violations\n",
    "## this code for reference, the audio file is already provided in the next cell\n",
    "\n",
    "##first, we will need to have two required libraries\n",
    "## ffmpeg and youtube-dl\n",
    "\n",
    "\n",
    "# !pip install youtube_dl\n",
    "# !pip install ffmpeg\n",
    "\n",
    "##lets make this available as a function, takes in a URL and generates a WAV file\n",
    "\n",
    "# from youtube_dl import YoutubeDL\n",
    "\n",
    "# def getAudio(url):\n",
    "    \n",
    "#     audio_downloader = YoutubeDL({'format':'bestaudio'})\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         print('Dowloading Audio')\n",
    "\n",
    "#         URL = url\n",
    "\n",
    "#         audio_downloader.extract_info(URL)\n",
    "\n",
    "#     except Exception:\n",
    "\n",
    "#         print(\"Couldn\\'t download the audio\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's import required basic libraries\n",
    "import io\n",
    "import librosa\n",
    "from time import time\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import grpc\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's import required RIVA libraries\n",
    "# ASR \n",
    "import riva_api.riva_asr_pb2 as rasr\n",
    "import riva_api.riva_asr_pb2_grpc as rasr_srv\n",
    "import riva_api.riva_audio_pb2 as ra\n",
    "\n",
    "# NLP \n",
    "\n",
    "import riva_api.riva_nlp_pb2 as rnlp\n",
    "import riva_api.riva_nlp_pb2_grpc as rnlp_srv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abc7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's configure our services\n",
    "## note: change the host and portnumber to your provider Riva SERVER PORT and DGX that you are connected to\n",
    "\n",
    "\n",
    "channel = grpc.insecure_channel('YOUR DGX HOST HERE:RIVA PORT NUM')\n",
    "\n",
    "##setup service layer objects\n",
    "\n",
    "riva_asr = rasr_srv.RivaSpeechRecognitionStub(channel)\n",
    "riva_nlp = rnlp_srv.RivaLanguageUnderstandingStub(channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cded8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets preview the audio \n",
    "## note, the audio is provided in the data folder\n",
    "\n",
    "# read in an audio file from local disk\n",
    "path = \"../datasets/amazon-rainforest.wav\"\n",
    "audio, sr = librosa.core.load(path, sr=None)\n",
    "with io.open(path, 'rb') as fh:\n",
    "    content = fh.read()\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now lets use Riva ASR to transcribe the audio\n",
    "# Set up an offline/batch recognition request\n",
    "\n",
    "req = rasr.RecognizeRequest()\n",
    "req.audio = content                                   # raw bytes from previous cell\n",
    "req.config.encoding = ra.AudioEncoding.LINEAR_PCM     # Supports LINEAR_PCM, FLAC, MULAW and ALAW audio encodings\n",
    "req.config.sample_rate_hertz = sr                     # Audio will be resampled if necessary\n",
    "req.config.language_code = \"en-US\"                    # Language model ode\n",
    "req.config.max_alternatives = 1                       # How many top-N hypotheses to return, 1 means return the best one\n",
    "req.config.enable_automatic_punctuation = True        # Add punctuation when end of VAD detected\n",
    "req.config.audio_channel_count = 1                    # Mono channel, change to 2 for stereo\n",
    "\n",
    "response = riva_asr.Recognize(req)\n",
    "asr_best_transcript = response.results[0].alternatives[0].transcript\n",
    "\n",
    "print(\"ASR Transcript:\", asr_best_transcript) ## we will use this in the next step\n",
    "\n",
    "## lets inspect the response to ensure no anomalies\n",
    "\n",
    "print(\"\\n\\nFull Response Message:\")\n",
    "print(response) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f22c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## now, we have the raw transcript. Let us identify the named entities in this text\n",
    "## first setup the nlp request object\n",
    "req = rnlp.TokenClassRequest()\n",
    "\n",
    "## now define the model\n",
    "req.model.model_name = \"riva_ner\"     # If you have deployed a custom model with the domain_name \n",
    "                                        # parameter in ServiceMaker's `jarvis-build` command then you should use \n",
    "                                        # \"jarvis_ner_<your_input_domain_name>\" where <your_input_domain_name>\n",
    "                                        # is the name you provided to the domain_name parameter.\n",
    "\n",
    "## use the text from the previous part here            \n",
    "req.text.append(asr_best_transcript)\n",
    "\n",
    "resp = riva_nlp.ClassifyTokens(req)\n",
    "\n",
    "##show us what entities are present\n",
    "print(\"Named Entities:\")\n",
    "for result in resp.results[0].results:\n",
    "    print(f\"  {result.token} ({result.label[0].class_name})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189caad",
   "metadata": {},
   "source": [
    "Now that we have seen which entities are available, we can use the text to answer our questions. \n",
    "for our sample we have a small file which contains all entities needed to answer our query, so we can go ahead and jump to the question answering phase straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##now define the request object\n",
    "req = rnlp.NaturalQueryRequest()\n",
    "\n",
    "## now lets make a list of our queries\n",
    "\n",
    "queries = ['What is the Amazon Rainforest?', 'How many reptile species are found in the Amazon Rainforest?']\n",
    "\n",
    "## setup answers/knowledge base object\n",
    "answers = []\n",
    "\n",
    "##now lets iterate through the loop and provide answers\n",
    "\n",
    "for input_query in queries:\n",
    "    \n",
    "    req.query = input_query\n",
    "    ## we have a small transcript so we will only use this one context.\n",
    "    ## for larger amounts of data we need to split the text to narrow down the context\n",
    "    req.context = asr_best_transcript\n",
    "    resp = riva_nlp.NaturalQuery(req)\n",
    "\n",
    "    print(f\"Query: {input_query}\")\n",
    "    print(f\"Answer: {resp.results[0].answer}\")\n",
    "    \n",
    "    ##add the answers to the knowledge base\n",
    "    answers.append(resp.results[0].answer)\n",
    "\n",
    "## show us the combined answer list\n",
    "print(answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fff36b",
   "metadata": {},
   "source": [
    "## 2. Challenge Exercise\n",
    "Let us use the following files as a sample input:\n",
    "Challenge Video (GTC-2021-healthcare)\n",
    "Video URL: https://www.youtube.com/watch?v=AfC7-Iksl_M\n",
    "\n",
    "For convenience, we have converted the youtube video into WAV files which can be used in our pipeline\n",
    "The code for this process is found above \n",
    "\n",
    "Sample Audio(../data/amazon-rainforest.wav)\n",
    "\n",
    "For the exercise, find a list of named entities in each video (or the entire talk) and  answer the following questions:\n",
    "\n",
    "0: how is drug discovery traditionally done?\n",
    "\n",
    "1: how many compounds are inferred in the molecular latent space learned?\n",
    "\n",
    "2: what type of model was used to generate chemistry?\n",
    "\n",
    "3: How many parameters does the world's largest clinical model have?\n",
    "\n",
    "4: how many drug candidates can be simulated per year using the a100 MIG architecture?\n",
    "\n",
    "5: how many new models are part of Clara discovery?\n",
    "\n",
    "6: where does medical data flow in from?\n",
    "\n",
    "7: how much data do hospitals generate each year?\n",
    "\n",
    "8: what is Nvidia Clara?\n",
    "\n",
    "9: how many models does Clara have?\n",
    "\n",
    "10: how much of the population can AI driven diagnostic devices reach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0676049",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first let us preview the video \n",
    "## use builtin IPython to playback youtube\n",
    "from IPython.display import IFrame\n",
    "\n",
    "# GTC Healthcare Special Talk\n",
    "IFrame(\"https://www.youtube.com/embed/AfC7-Iksl_M?rel=0&amp;controls=0&amp;showinfo=0\", width=\"560\", height=\"315\", frameborder=\"0\", allowfullscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21ae11",
   "metadata": {},
   "source": [
    "The talk has been split into sections and the following WAV files are available in the data folder  \n",
    "gtc1.wav  \n",
    "gtc2.wav   \n",
    "gtc3.wav    \n",
    "gtc4.wav  \n",
    "gtc5.wav  \n",
    "gtc6.wav  \n",
    "gtc7.wav  \n",
    "gtc8.wav  \n",
    "gtc9.wav  \n",
    "gtc10.wav  \n",
    "gtc11.wav  \n",
    "gtc12.wav  \n",
    "gtc13.wav  \n",
    "gtc14.wav  \n",
    "gtc15.wav  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2336872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##first, transcribe the files \n",
    "## using a function and a loop will be useful here\n",
    "## TODO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4174f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next, identify entities\n",
    "## again, defining a function and a loop is useful\n",
    "##TODO here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97017d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "##next, match entities with questions\n",
    "## some entities may not appear, may be misspelled and mispunctuated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8881ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## next, run the QA pipeline for matches found for each question\n",
    "## a loop or a nested loop is useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "519cc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##finally, compile answers into knowledge base\n",
    "## a dictionary structure, combined with lists is ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1025f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
