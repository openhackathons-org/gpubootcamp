{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_BOOTCAMP.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](token-classification-deployment.ipynb)\n",
    "[3](token-classification-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](token-classification-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLT - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning Toolkit (TLT) is a python based AI toolkit for taking purpose-built pre-trained AI models and customizing them with your own data. \n",
    "\n",
    "Transfer learning extracts learned features from an existing neural network to a new one. Transfer learning is often used when creating a large training dataset is not feasible. \n",
    "\n",
    "Developers, researchers and software partners building intelligent vision AI apps and services, can bring their own data to fine-tune pre-trained models instead of going through the hassle of training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transfer Learning Toolkit](https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this toolkit is to reduce that 80 hour workload to an 8 hour workload, which can enable data scientist to have considerably more train-test iterations in the same time frame.\n",
    "\n",
    "Let's see this in action with a use case for Named Entity Recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity recognition (NER), also referred to as entity chunking, identification or extraction, is the task of detecting and classifying key information (entities) in text. \n",
    "\n",
    "For example, in a sentence: Mary lives in Santa Clara and works at NVIDIA, we should detect that **Mary** is a person, **Santa Clara** is a location and **NVIDIA** is a company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition using TLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dataset Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we going to use [GMB(Groningen Meaning Bank)](http://www.let.rug.nl/bjerva/gmb/about.php) corpus for entity recognition.\n",
    "\n",
    "GMB is a fairly large corpus with a lot of annotations. Note, that GMB is not completely human annotated and itâ€™s not considered 100% correct.\n",
    "\n",
    "The data is labeled using the [IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging) (short for inside, outside, beginning). \n",
    "\n",
    "The following classes appear in the dataset:\n",
    "\n",
    "* LOC = Geographical Entity\n",
    "* ORG = Organization\n",
    "* PER = Person\n",
    "* GPE = Geopolitical Entity\n",
    "* TIME = Time indicator\n",
    "* ART = Artifact\n",
    "* EVE = Event\n",
    "* NAT = Natural Phenomenon\n",
    "\n",
    "For this tutorial, classes ART, EVE, and NAT were combined into a MISC class due to small number of examples for these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Set path to a folder where you want you data to be saved.\n",
    "DATA_DOWNLOAD_DIR = \"<YOUR_PATH_TO_DATA_DIR>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://dldata-public.s3.us-east-2.amazonaws.com/gmb_v_2.2.0_clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! unzip gmb_v_2.2.0_clean.zip -d $DATA_DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data folder should contain 5 files:\n",
    "\n",
    "- labels_dev.txt\n",
    "- labels_train.txt\n",
    "- text_dev.txt\n",
    "- text_train.txt\n",
    "- label_ids.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l $DATA_DOWNLOAD_DIR/gmb_v_2.2.0_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the data \n",
    "print('Train text:')\n",
    "! head -n 5 {DATA_DOWNLOAD_DIR}/gmb_v_2.2.0_clean/text_train.txt\n",
    "\n",
    "print('\\nTrain label:')\n",
    "! head -n 5 {DATA_DOWNLOAD_DIR}/gmb_v_2.2.0_clean/labels_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once tlt is installed, the next step is to setup the mounts for TLT. <br>\n",
    "\n",
    "The file `~/.tlt_mounts.json` takes care of the mounts inside docker container and also for additional arguments to be passed to docker run command. This file is stored in the users home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ~/.tlt_mounts.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overwrite `~/.tlt_mounts.json` with the mounts needed. Please change the paths to \"source\" key below.\n",
    "After installing tlt, the next step is to setup the mounts for TLT. The TLT launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tlt_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TLT launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tlt_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container. **Please also ensure that the source directories exist on your machine!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/.tlt_mounts.json\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<add path to DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<add path to SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<add path to RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<add path to CACHE_DIR eg. /home/user/.cache>\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ],\n",
    "   \"DockerOptions\": {\n",
    "        \"shm_size\": \"16G\",\n",
    "        \"ulimits\": {\n",
    "            \"memlock\": -1,\n",
    "            \"stack\": 67108864\n",
    "         }\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"source\" and \"destination\" are mounts for the source and destination folders to access the pre-processed and processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir <add path to SPECS_DIR>\n",
    "! mkdir <add path to RESULTS_DIR>\n",
    "! mkdir <add path to CACHE_DIR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TLT Docker.\n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the docker image versions and the tasks that tlt can perform. You can also check this out with `tlt info --verbose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tlt info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is setup, we would like to take a bit of time to explain the tlt interface for ease of use. The command structure can be broken down as follows: `tlt <task name> <subcommand>` <br> \n",
    "\n",
    "Let's see this in further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "TLT's Conversational AI Toolkit works off of spec files which make it easy to edit hyperparameters on the fly. We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tlt token_classification download_specs \\\n",
    "    -r $RESULTS_DIR/token_classification \\\n",
    "    -o $SPECS_DIR/token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TokenClassification Model in NeMo](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/token_classification/token_classification_model.py) supports NER and other token level classification tasks, as long as the data follows the format specified below. \n",
    "\n",
    "Token Classification Model requires the data to be split into 2 files: \n",
    "* text.txt and \n",
    "* labels.txt. \n",
    "\n",
    "Each line of the **text.txt** file contains text sequences, where words are separated with spaces, i.e.: \n",
    "[WORD] [SPACE] [WORD] [SPACE] [WORD].\n",
    "\n",
    "The **labels.txt** file contains corresponding labels for each word in text.txt, the labels are separated with spaces, i.e.:\n",
    "[LABEL] [SPACE] [LABEL] [SPACE] [LABEL].\n",
    "\n",
    "Example of a text.txt file:\n",
    "```\n",
    "Jennifer is from New York City .\n",
    "She likes ...\n",
    "...\n",
    "```\n",
    "Corresponding labels.txt file:\n",
    "```\n",
    "B-PER O O B-LOC I-LOC I-LOC O\n",
    "O O ...\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert an IOB format data to the format required for training, we can use the `dataset_convert` command in TLT on your train and dev files\n",
    "\n",
    "For this tutorial, we are using the preprocessed GMB dataset, thus we won't be required to convert the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Train the NER model from scratch using pre-trained BERT base uncased model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Named Entity Recognition model is comprised of the pretrained BERT model followed by a Token Classification layer.\n",
    "\n",
    "The model is defined in a config file which are already available for you to use directly or as reference to create your own. \n",
    "\n",
    "Through these spec files, the user can tune many knobs like the model, dataset, hyperparameters, optimizer etc. Each command (like train, finetune, evaluate etc.) should have a dedicated spec file. These sample spec files are available at `$SPECS_DIR/token_classification`\n",
    "\n",
    "The important sections to look out for are:\n",
    "\n",
    "- model: All arguments that are related to the model - language model, token classifier, optimizer and schedulers, datasets and any other related information\n",
    "- trainer: Any argument to be passed to PyTorch Lightning\n",
    "\n",
    "The model config file for token classification is available at [Github](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/token_classification/conf/token_classification_config.yaml). A similar file (at the above location inside TLT docker image) is used for TLT.\n",
    "\n",
    "Among other things, the config file contains dictionaries called **dataset**, **train_ds** and **validation_ds**. These are configurations used to setup the Dataset and DataLoaders of the corresponding config.\n",
    "\n",
    "We assume that both training and evaluation files are located in the same directory, and use the default names mentioned during the data download step. So, to start model training, we simply need to specify `model.dataset.data_dir`, like we are going to do below.\n",
    "\n",
    "Also notice that some config lines, including `model.dataset.data_dir` have ??? in place of paths, which means that the values for these fields are required to be specified by the user.\n",
    "\n",
    "Thus, we must provide the following parameters to TLT:\n",
    "1. **data_dir** - path to the processed data\n",
    "2. **model.label_ids** - mapping of labels with their numerical ids\n",
    "\n",
    "We can pass these parameters while using the TLT train command. In addition to these, we can also change other parameters like max_epochs.\n",
    "\n",
    "A similar version of model config file we saw above is also passed to the TLT train command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt token_classification train \\\n",
    "                        -e $SPECS_DIR/token_classification/train.yaml \\\n",
    "                        -g 1  \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/train \\\n",
    "                        data_dir=$DATA_DIR/gmb_v_2.2.0_clean \\\n",
    "                        training_ds.num_samples=-1 \\\n",
    "                        validation_ds.num_samples=-1 \\\n",
    "                        model.label_ids=$DATA_DIR/gmb_v_2.2.0_clean/label_ids.csv \\\n",
    "                        trainer.max_epochs=1 \\\n",
    "                        model.language_model.pretrained_model_name=bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Finetune NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we were training from scratch, the datasets were prepared for training during the model initialization. When we are using a pretrained NER model, before training, we need to setup training and evaluation data, and also provide path to the pre-trained model.tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity of this tutorial, we will be using the `trained-model.tlt` that was saved during the last section (Train the NER model from scratch using pre-trained BERT Base uncased model) and finetune it again on the GMB dataset.\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt token_classification finetune \\\n",
    "                        -e $SPECS_DIR/token_classification/finetune.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/finetune-bert-base \\\n",
    "                        data_dir=$DATA_DIR/gmb_v_2.2.0_clean \\\n",
    "                        trainer.max_epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generate a fine-tuned model `finetuned-model.tlt` at $RESULTS_DIR/finetune-bert-base/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate NER on the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model performs, we can generate prediction the same way we did it earlier or we can use our model to generate predictions for a dataset from a file, for example, to perform final evaluation or to do error analysis. Below, we are using a subset of dev set, but it could be any text file as long as it follows the data format described above. \n",
    "\n",
    "`labels_file` is optional here, and if provided will be used to get metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the below command, TLT will evaluate on `text_dev.txt` present in the data_dir, using `trained-model.tlt` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt token_classification evaluate \\\n",
    "                        -e $SPECS_DIR/token_classification/evaluate.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/finetune-bert-base/checkpoints/finetuned-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/evaluate \\\n",
    "                        data_dir=$DATA_DIR/gmb_v_2.2.0_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export model to ONNX format for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained up-to satisfactory metrics, we can export it in ONNX format to be deployed with any inferencing solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt token_classification export \\\n",
    "                        -e $SPECS_DIR/token_classification/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/finetune-bert-base/checkpoints/finetuned-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/export \\\n",
    "                        export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as `exported-model.eonnx` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're specifically interested to deploy this model using NVIDIA Riva, please set `export_format=JARVIS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt token_classification export \\\n",
    "                        -e $SPECS_DIR/token_classification/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/finetune-bert-base/checkpoints/finetuned-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/riva \\\n",
    "                        export_format=JARVIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `exported-model.riva` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Infer on a custom input sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the model's predictions on a given sentence of your choice, you can save those sentences in infer.yaml and use TLT infer command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infer.yaml contains:\n",
    "\n",
    "```\n",
    "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
    "# TLT Spec file for inference using the previously pretrained Token Classification model.\n",
    "\n",
    "# \"Simulate\" user input: batch with two samples.\n",
    "input_batch:\n",
    "  - 'We bought four shirts from the Nvidia gear store in Santa Clara.'\n",
    "  - 'Nvidia is a company.'\n",
    "\n",
    "```\n",
    "Please edit `input_batch` with your own sentences to measure the output of this NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt token_classification infer \\\n",
    "                        -e $SPECS_DIR/token_classification/infer.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/token_classification/finetune-bert-base/checkpoints/finetuned-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/token_classification/infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use TLT to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
