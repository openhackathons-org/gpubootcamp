{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c176255",
   "metadata": {},
   "source": [
    "### Installing and setting up TLT\n",
    "\n",
    "For ease of use, please install TLT inside a python virtual environment. We recommend performing this step first and then launching the notebook from the virtual environment.\n",
    "\n",
    "In addition to installing TLT python package, please make sure of the following software requirements:\n",
    "\n",
    "1. python 3.6.9\n",
    "2. docker-ce > 19.03.5\n",
    "3. docker-API 1.40\n",
    "4. nvidia-container-toolkit > 1.3.0-1\n",
    "5. nvidia-container-runtime > 3.4.0-1\n",
    "6. nvidia-docker2 > 2.5.0-1\n",
    "7. nvidia-driver >= 455.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##installing tlt\n",
    "! pip install nvidia-pyindex\n",
    "! pip install nvidia-tlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e043b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "##setting relevant paths\n",
    "# NOTE: The following paths are set from the perspective of the TLT Docker.\n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR = \"/data\"\n",
    "SPECS_DIR = \"/specs\"\n",
    "RESULTS_DIR = \"/results\"\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY = 'tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f956768",
   "metadata": {},
   "source": [
    "### Download AN4 Speech Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda58d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://www.speech.cs.cmu.edu/databases/an4/an4_sphere.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvf an4_sphere.tar.gz \n",
    "! mv an4 $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96fd9c9",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ecfa5",
   "metadata": {},
   "source": [
    "This step converts the mp3 files into wav files and splits the data into training and testing sets. It also generates a \"meta-data\" file to be consumed by the dataloader for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3604982",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tlt speech_to_text dataset_convert \\\n",
    "    -e $SPECS_DIR/speech_to_text/dataset_convert_an4.yaml \\\n",
    "    source_data_dir=$DATA_DIR/an4 \\\n",
    "    target_data_dir=$DATA_DIR/an4_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc93df70",
   "metadata": {},
   "source": [
    "---\n",
    "## Riva ServiceMaker\n",
    "Servicemaker is the set of tools that aggregates all the necessary artifacts (models, files, configurations, and user settings) for Riva deployment to a target environment. It has two main components as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a6ad3",
   "metadata": {},
   "source": [
    "### 1. Riva-build\n",
    "\n",
    "This step helps build a Riva-ready version of the model. Itâ€™s only output is an intermediate format (called a RMIR) of an end to end pipeline for the supported services within Riva. We are taking a ASR QuartzNet Model in consideration<br>\n",
    "\n",
    "`riva-build` is responsible for the combination of one or more exported models (.riva files) into a single file containing an intermediate format called Riva Model Intermediate Representation (.rmir). This file contains a deployment-agnostic specification of the whole end-to-end pipeline along with all the assets required for the final deployment and inference. Please checkout the [documentation](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/service-asr.html#pipeline-configuration) to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9990631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: UPDATE THESE PATHS \n",
    "\n",
    "# ServiceMaker Docker\n",
    "RIVA_SM_CONTAINER = \"<add container name>\"\n",
    "\n",
    "# Directory where the .riva model is stored $MODEL_LOC/*.riva\n",
    "MODEL_LOC = \"<add path to model location>\"\n",
    "\n",
    "# Name of the .riva file\n",
    "MODEL_NAME = \"<add model name>\"\n",
    "\n",
    "# Key that model is encrypted with, while exporting with TLT\n",
    "KEY = \"<add encryption key used for trained model>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4ede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ServiceMaker docker\n",
    "! docker pull $RIVA_SM_CONTAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key --acoustic_model_name=<quartznet/jasper>\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build speech_recognition /data/asr.rmir:$KEY /data/$MODEL_NAME:$KEY --offline \\\n",
    "            --decoder_type=greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605b8b8f",
   "metadata": {},
   "source": [
    "###  2. Riva-deploy\n",
    "\n",
    "The deployment tool takes as input one or more Riva Model Intermediate Representation (RMIR) files and a target model repository directory. It creates an ensemble configuration specifying the pipeline for the execution and finally writes all those assets to the output model repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616af5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR ASR build\n",
    "# Syntax: riva-deploy -f dir-for-rmir/model.rmir:key output-dir-for-repository\n",
    "! docker run --rm --gpus 0 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-deploy -f  /data/asr.rmir:$KEY /data/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR QA build\n",
    "# Syntax: riva-build <task-name> output-dir-for-rmir/model.rmir:key dir-for-riva/model.riva:key\n",
    "!docker run --rm --gpus 1 -v $MODEL_LOC:/data $RIVA_SM_CONTAINER -- \\\n",
    "            riva-build qa /data/question-answering.rmir:$KEY /data/$MODEL_NAME:$KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4078ce",
   "metadata": {},
   "source": [
    "`NOTE:` Above, qa-model.riva is the qa model obtained from `tlt question_answering export`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf51ec",
   "metadata": {},
   "source": [
    "---\n",
    "## Start Riva Server\n",
    "Once the model repository is generated, we are ready to start the Riva server. From this step onwards you need to download the Riva QuickStart Resource from NGC. \n",
    "Set the path to the directory here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e79251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Riva QuickStart directory\n",
    "RIVA_DIR = \"<Path to the uncompressed folder downloaded from quickstart(include the folder name)>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85208b4a",
   "metadata": {},
   "source": [
    "Next, we modify config.sh to enable relevant Riva services (asr for QuartzNet Model), provide the encryption key, and path to the model repository (`riva_model_loc`) generated in the previous step among other configurations. \n",
    "\n",
    "For instance, if above the model repository is generated at `$MODEL_LOC/models`, then you can specify `riva_model_loc` as the same directory as `MODEL_LOC` <br>\n",
    "\n",
    "Pretrained versions of models specified in models_asr/nlp/tts are fetched from NGC. Since we are using our custom model, we can comment it in models_asr (and any others that are not relevant to your use case). <br>\n",
    "\n",
    "\n",
    "`NOTE:` You can perform this step of editing config.sh from outside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e394d8",
   "metadata": {},
   "source": [
    "#### config.sh snippet\n",
    "```\n",
    "# Enable or Disable Riva Services \n",
    "service_enabled_asr=true                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_nlp=false                                                      ## MAKE CHANGES HERE\n",
    "service_enabled_tts=false                                                     ## MAKE CHANGES HERE\n",
    "\n",
    "# Specify one or more GPUs to use\n",
    "# specifying more than one GPU is currently an experimental feature, and may result in undefined behaviours.\n",
    "gpus_to_use=\"device=0\"\n",
    "\n",
    "# Specify the encryption key to use to deploy models\n",
    "MODEL_DEPLOY_KEY=\"tlt_encode\"                                                  ## MAKE CHANGES HERE\n",
    "\n",
    "# Locations to use for storing models artifacts\n",
    "#\n",
    "# If an absolute path is specified, the data will be written to that location\n",
    "# Otherwise, a docker volume will be used (default).\n",
    "#\n",
    "# riva_init.sh will create a `rmir` and `models` directory in the volume or\n",
    "# path specified. \n",
    "#\n",
    "# RMIR ($riva_model_loc/rmir)\n",
    "# Riva uses an intermediate representation (RMIR) for models\n",
    "# that are ready to deploy but not yet fully optimized for deployment. Pretrained\n",
    "# versions can be obtained from NGC (by specifying NGC models below) and will be\n",
    "# downloaded to $riva_model_loc/rmir by `riva_init.sh`\n",
    "# \n",
    "# Custom models produced by NeMo or TLT and prepared using riva-build\n",
    "# may also be copied manually to this location $(riva_model_loc/rmir).\n",
    "#\n",
    "# Models ($riva_model_loc/models)\n",
    "# During the riva_init process, the RMIR files in $riva_model_loc/rmir\n",
    "# are inspected and optimized for deployment. The optimized versions are\n",
    "# stored in $riva_model_loc/models. The riva server exclusively uses these\n",
    "# optimized versions.\n",
    "riva_model_loc=\"<add path>\"                              ## MAKE CHANGES HERE (Replace with MODEL_LOC)                      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893749fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have permission to execute these scripts\n",
    "! cd $RIVA_DIR && chmod +x ./riva_init.sh && chmod +x ./riva_start.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Init. This will fetch the containers/models\n",
    "# YOU CAN SKIP THIS STEP IF YOU DID RIVA DEPLOY\n",
    "! cd $RIVA_DIR && ./riva_init.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4328de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Riva Start. This will deploy your model(s).\n",
    "! cd $RIVA_DIR && ./riva_start.sh config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe3413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
