{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&ensp;\n",
    "[Home Page](../START_HERE_RIVA_BOOTCAMP.ipynb)\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;\n",
    "[1]\n",
    "[2](question-answering-deployment.ipynb)\n",
    "[3](question-answering-Exercise.ipynb)\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "[Next Notebook](question-answering-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering using Transfer Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transfer learning* is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task.\n",
    "\n",
    "**Transfer Learning Toolkit (TLT)** is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "<center><img src=\"https://developer.nvidia.com/sites/default/files/akamai/embedded-transfer-learning-toolkit-software-stack-1200x670px.png\"><\\center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TLT to:\n",
    "- Take a [BERT](https://arxiv.org/pdf/1810.04805.pdf) QA model and [**Train/Finetune**](#training) it on the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset\n",
    "- Run [**Inference**](#inference)\n",
    "- [**Export**](#export-onnx) the model for the [ONNX](https://onnx.ai/) format, or [export](#export-riva) in a format suitable for deployment in [Riva](https://developer.nvidia.com/riva).\n",
    "\n",
    "The earlier sections in the notebook give a brief introduction to the QA task, the SQuAD dataset and BERT. If you are already familiar with these, and want to jump right into the meat of the matter, you can start at section on [Data Preparation](#prepare-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-requisites\n",
    "For ease of use, please install TLT inside a python virtual environment. We recommend performing this step first and then launching the notebook from the virtual environment.\n",
    "\n",
    "Let's install TLT. It is a simple pip install!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nvidia-pyindex\n",
    "! pip install nvidia-tlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the docker image versions and the tasks that tlt can perform, use the `tlt info` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt info --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to installing TLT package, please make sure of the following software requirements:\n",
    "\n",
    "1. python 3.6.9\n",
    "2. docker-ce > 19.03.5\n",
    "3. docker-API 1.40\n",
    "4. nvidia-container-toolkit > 1.3.0-1\n",
    "5. nvidia-container-runtime > 3.4.0-1\n",
    "6. nvidia-docker2 > 2.5.0-1\n",
    "7. nvidia-driver >= 455.23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the GPU device(s) is visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question Answering (QA)\n",
    "\n",
    "### Task Description\n",
    "The Question Answering task in NLP pertains to building a model which can answer questions posed in natural language. Many datasets (including [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/), the dataset we use in this notebook) pose this as a reading comprehension task i.e. given a question and a context, the goal is to predict the span within the context with a start and end position which indicates the answer to the question. For every word in the training dataset we predict:\n",
    "- likelihood this word is the start of the span\n",
    "- likelihood this word is the end of the span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The SQuAD Dataset\n",
    "\n",
    "[SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) is a large dataset for QA consisting of reading passages obtained from high-quality Wikipedia articles. With each passage, the dataset contains accompanying reading comprehension questions based on the content of the passage. For each question, there are one or more answers. These questions and corresponding answers were obtained through crowdsourcing.\n",
    "\n",
    "\n",
    "The SQuAD format consists of a JSON file for each dataset split. Each title has one or multiple paragraph entries, each consisting of the text - \"context\", and question-answer entries. Each question-answer entry has:\n",
    "\n",
    "- a question\n",
    "- a boolean flag \"is_impossible\" which shows if the question is answerable or not\n",
    "- a globally unique id\n",
    "- in case the question is answerable one answer entry, which contains the text span and its starting character index in the context. If not answerable, the \"answers\" list is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"Super_Bowl_50\", \n",
    "            \"paragraphs\": [\n",
    "                {\n",
    "                    \"context\": \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\", \n",
    "                    \"qas\": [\n",
    "                        {\n",
    "                            \"question\": \"Where did Super Bowl 50 take place?\", \n",
    "                            \"is_impossible\": \"false\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ee\", \n",
    "                            \"answers\": [\n",
    "                                {\n",
    "                                    \"answer_start\": \"403\", \n",
    "                                    \"text\": \"Santa Clara, California\"\n",
    "                                }\n",
    "                            ]\n",
    "                        },\n",
    "                        {\n",
    "                            \"question\": \"What was the winning score of the Super Bowl 50?\", \n",
    "                            \"is_impossible\": \"true\", \n",
    "                            \"id\": \"56be4db0acb8001400a502ez\", \n",
    "                            \"answers\": [\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation files (for validation and testing) follow the above format except for it can provide more than one answer to the same question. The inference file follows the above format except for it does not require the \"answers\" and \"is_impossible\" keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model for QA\n",
    "In this notebook, we will show how to use a pre-trained [BERT](https://arxiv.org/pdf/1810.04805.pdf) (Bidirectional Encoder Representations from Transformers) model for QA leveraging TLT. The BERT model has made major breakthroughs in Natural Language Understanding in recent years. For most applications, the model is typically trained in two phases, pre-training and fine-tuning. \n",
    "- The BERT core model can be pre-trained on large, generic datasets to generate dense vector representations of input sentence(s). \n",
    "- It can be quickly fine-tuned to perform a wide variety of tasks such as question/answering, sentiment analysis, or named entity recognition.\n",
    "\n",
    "The figure below shows a high-level block diagram of pre-training and fine-tuning BERT for QA.\n",
    "<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/05/bert-model-625x268.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alignment with the above, for pre-training we can take one of two approaches. We can either pre-train the BERT model with our own data, or use a model pre-trained by Nvidia. After we obtain a pre-trained model, the next step would be to fine-tune it for the QA task and run inference on the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2020/06/Fig4revised-625x340.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='prepare-data'></a>\n",
    "### Preparing the dataset\n",
    "The SQuAD dataset is available [here](https://rajpurkar.github.io/SQuAD-explorer/). You will find that there are 2 versions of the Squad datasets: v1.1 and v2.0. \n",
    "\n",
    "- SQuAD 1.1, the older version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles.\n",
    "- SQuAD 2.0 dataset combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you may use the code below to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: Set path to a folder where you want you data to be saved\n",
    "DATA_DOWNLOAD_DIR = \"<YOUR_PATH_TO_DATA_DIR>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the data and results directories if they don't exist\n",
    "if not os.path.exists(DATA_DOWNLOAD_DIR):\n",
    "        os.makedirs(DATA_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# simple utility function to download the squad dataset\n",
    "def download_squad(save_path):\n",
    "    save_path = save_path + '/squad'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if not os.path.exists(save_path + '/v1.1'):\n",
    "        os.makedirs(save_path + '/v1.1')\n",
    "\n",
    "    if not os.path.exists(save_path + '/v2.0'):\n",
    "        os.makedirs(save_path + '/v2.0')\n",
    "        \n",
    "    # urls for both SQuAD v1.1 and v2.0. You may modify the dict below if you choose to download only one of the two.\n",
    "    download_urls = {\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/train-v1.1.json': 'v1.1/train-v1.1.json',\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/dev-v1.1.json': 'v1.1/dev-v1.1.json',\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/train-v2.0.json': 'v2.0/train-v2.0.json',\n",
    "            'https://rajpurkar.github.io/SQuAD-explorer' '/dataset/dev-v2.0.json': 'v2.0/dev-v2.0.json',\n",
    "    }\n",
    "         \n",
    "    for item in download_urls:\n",
    "        url = item\n",
    "        file = download_urls[item]\n",
    "        print('Downloading:', url)\n",
    "        if os.path.isfile(save_path + '/' + file):\n",
    "            print('** Download file already exists, skipping download **')\n",
    "        else:\n",
    "            response = urllib.request.urlopen(url)\n",
    "            with open(save_path + '/' + file, \"wb\") as handle:\n",
    "                handle.write(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will download both v1.1 and v2.0 versions of SQuAD dataset\n",
    "download_squad(DATA_DOWNLOAD_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the data is present\n",
    "!ls $DATA_DOWNLOAD_DIR/squad/v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TLT workflow\n",
    "The rest of the notebook shows what a sample TLT workflow looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting TLT Mounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset has been downloaded, an important step in using TLT is to set up the directory mounts. The TLT launcher uses docker containers under the hood, and **for our data and results directory to be visible to the docker, they need to be mapped**. The launcher can be configured using the config file `~/.tlt_mounts.json`. Apart from the mounts, you can also configure additional options like the Environment Variables and amount of Shared Memory available to the TLT launcher. <br>\n",
    "\n",
    "`IMPORTANT NOTE:` The code below creates a sample `~/.tlt_mounts.json`  file. Here, we can map directories in which we save the data, specs, results and cache. You should configure it for your specific case such your these directories are correctly visible to the docker container. **Please also ensure that the source directories exist on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tee ~/.tlt_mounts.json <<'EOF'\n",
    "{\n",
    "   \"Mounts\":[\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_DATA_DIR>\",\n",
    "           \"destination\": \"/data\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_SPECS_DIR>\",\n",
    "           \"destination\": \"/specs\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_RESULTS_DIR>\",\n",
    "           \"destination\": \"/results\"\n",
    "       },\n",
    "       {\n",
    "           \"source\": \"<YOUR_PATH_TO_CACHE_DIR eg. /home/user/.cache>\",\n",
    "           \"destination\": \"/root/.cache\"\n",
    "       }\n",
    "   ]\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the source directories exist, if not, create them\n",
    "! mkdir <YOUR_PATH_TO_SPECS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_RESULTS_DIR>\n",
    "! mkdir <YOUR_PATH_TO_CACHE_DIR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the notebook exemplifies the simplicity of the TLT workflow. Users with basic knowledge of Deep Learning can get started building their own custom models using a simple specification file. It's essentially just one command each to run data preprocessing, training, fine-tuning, evaluation, inference, and export! All configurations happen through YAML spec files <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration/Specification Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of all commands in TLT lies in the YAML spec files. There are sample spec files already available for you to use directly or as reference to create your own.  Through these spec files, you can tune many knobs like the model, dataset, hyperparameters, optimizer etc. Each command (like train, finetune, evaluate etc.) should have a dedicated spec file with configurations pertinent to it. <br>\n",
    "\n",
    "Here is an example of the training spec file:\n",
    "\n",
    "---\n",
    "```\n",
    "trainer:\n",
    "  max_epochs: 100\n",
    "\n",
    "model:\n",
    "  tokenizer:\n",
    "      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece\n",
    "      vocab_file: null # path to vocab file \n",
    "      tokenizer_model: null # only used if tokenizer is sentencepiece\n",
    "      special_tokens: null\n",
    "\n",
    "  language_model:\n",
    "    pretrained_model_name: bert-base-uncased\n",
    "    lm_checkpoint: null\n",
    "    config_file: null # json file, precedence over config\n",
    "    config: null\n",
    "\n",
    "  token_classifier:\n",
    "    num_layers: 1\n",
    "    dropout: 0.0\n",
    "    num_classes: 2\n",
    "    activation: relu\n",
    "    log_softmax: false\n",
    "    use_transformer_init: true\n",
    "\n",
    "\n",
    "training_ds:\n",
    "  file: ??? # e.g. squad/v1.1/train-v2.0.json\n",
    "  batch_size: 12 # per GPU\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Convert\n",
    "For the QA task, the commands in TLT accepts data in the SQuAD JSON format (refer `Preparing the dataset` section above). If you have your data in any other format, be sure to convert it in the SQuAD format. Since we are using the SQuAD dataset for this notebook, we don't need to convert the data into any other format. We can proceed with training/fine-tuning directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Set Relevant Paths\n",
    "Set these paths according to your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The following paths are set from the perspective of the TLT Docker. \n",
    "\n",
    "# The data is saved here\n",
    "DATA_DIR='/data/squad'\n",
    "\n",
    "# The configuration files are stored here\n",
    "SPECS_DIR='/specs/question_answering'\n",
    "\n",
    "# The results are saved at this path\n",
    "RESULTS_DIR='/results/question_answering'\n",
    "\n",
    "# Set your encryption key, and use the same key for all commands\n",
    "KEY='tlt_encode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Specs\n",
    "We can proceed to downloading the spec files. The user may choose to modify/rewrite these specs, or even individually override them through the launcher. You can download the default spec files by using the `download_specs` command. <br>\n",
    "\n",
    "The -o argument indicating the folder where the default specification files will be downloaded, and -r that instructs the script where to save the logs. **Make sure the -o points to an empty folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt question_answering download_specs \\\n",
    "    -r $RESULTS_DIR \\\n",
    "    -o $SPECS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='training'></a>\n",
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model using TLT is as simple as configuring your spec file and running the train command. The code cell below uses the default train.yaml available for users as reference. It is configured by default to use the `bert-base-uncased` pretrained model. Additionally, these configurations could easily be overridden using the tlt-launcher CLI as shown below. For instance, below we override the `training_ds.file`, `validation_ds.file`, `trainer.max_epochs`, `training_ds.num_workers` and `validation_ds.num_workers` configurations to suit our needs. We encourage you to take a look at the .yaml spec files we provide! <br>\n",
    "\n",
    "For training a QA model in TLT, we use the `tlt question_answering train` command with the following args:\n",
    "- `-e`: Path to the spec file\n",
    "- `-g`: Number of GPUs to use\n",
    "- `-k`: User specified encryption key to use while saving/loading the model\n",
    "- `-r`: Path to a folder where the outputs should be written. Make sure this is mapped in tlt_mounts.json\n",
    "- Any overrides to the spec file eg. trainer.max_epochs <br>\n",
    "\n",
    "More details about these arguments are present in the [TLT Getting Started Guide](https://docs.nvidia.com/tlt/tlt-user-guide/index.html) <br>\n",
    "`NOTE:` All file paths corresponds to the destination mounted directory that is visible in the TLT docker container used in backend.<br>\n",
    "\n",
    "Also worth noting is that the first time you run training on the dataset, it will run pre-processing and save that processed data in the same directory as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a demonstration, we just train for 1 epoch below. You may need to train for more depending on your dataset.\n",
    "!tlt question_answering train \\\n",
    "                        -e $SPECS_DIR/train_bert.yaml \\\n",
    "                        -g 1  \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/train \\\n",
    "                        training_ds.file=$DATA_DIR/v1.1/train-v1.1.json \\\n",
    "                        validation_ds.file=$DATA_DIR/v1.1/dev-v1.1.json \\\n",
    "                        trainer.max_epochs=1 \\\n",
    "                        training_ds.num_workers=4 \\\n",
    "                        validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train command produces a .tlt file called `trained-model.tlt` saved at `$RESULTS_DIR/train/checkpoints/trained-model.tlt`. This file can be fed directly into the fine-tuning stage as we see in the next block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tips and tricks:\n",
    "- To accelerate the training without loss of quality, it is possible to train with these parameters:  `trainer.amp_level=\"O1\"` and `trainer.precision=16` for reduced precision.\n",
    "- The batch size (`training_ds.batch_size`) may influence the validation accuracy. Larger batch sizes are faster to train with, however, you may get slightly better results with smaller batches.\n",
    "- You can use the parameter: `trainer.val_check_interval` to define how many times per epoch to see validation accuracy metric calculated and printed. For instance, using `trainer.val_check_interval=0.25` will show the metric 4 times per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fine-Tuning\n",
    "Like many other NLP tasks, since we begin with a pretrained BERT model the step shown above for (re)training with your custom data should do the trick. However, TLT does provide a command for fine-tuning if your use-case demands that. Instead of `tlt question_answering train`, we use `tlt question_answering finetune` instead. We also specify the spec file corresponding to fine-tuning. All commands in TLT follow a similar pattern, streamlining the workflow even further!\n",
    "\n",
    "Note: If you wish to proceed with a trained dataset for better inference results, you can find a .nemo model [here](\n",
    "https://ngc.nvidia.com/catalog/collections/nvidia:nemotrainingframework).\n",
    "\n",
    "Simply re-name the .nemo file to .tlt and pass it through the finetune pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt  question_answering finetune \\\n",
    "                        -e $SPECS_DIR/finetune.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/finetune \\\n",
    "                        finetuning_ds.file=$DATA_DIR/v2.0/train-v2.0.json \\\n",
    "                        validation_ds.file=$DATA_DIR/v2.0/dev-v2.0.json \\\n",
    "                        trainer.max_epochs=1 \\\n",
    "                        finetuning_ds.num_workers=4 \\\n",
    "                        validation_ds.num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command will generate a fine-tuned model `finetuned-model.tlt` at `$RESULTS_DIR/finetune/checkpoints`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='evaluation'></a>\n",
    "### Evaluation\n",
    "The evaluation spec .yaml is as simple as:\n",
    "\n",
    "```\n",
    "test_ds:\n",
    "  file: ??? # e.g. squad/v1.1/dev-v1.1.json \n",
    "  batch_size: 32\n",
    "  shuffle: false\n",
    "  num_samples: 500\n",
    "```\n",
    "\n",
    "Below, we use `tlt question_answering evaluate` and override the test data configuration by specifying `test_ds.file`. Other arguments follow the same pattern as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt question_answering evaluate \\\n",
    "                        -e $SPECS_DIR/evaluate.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/evaluate \\\n",
    "                        test_ds.file=$DATA_DIR/v2.0/dev-v2.0.json \\\n",
    "                        test_ds.batch_size=32 \\\n",
    "                        test_ds.num_samples=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Evaluation should give the exact match/f1 scores for each data point. Remember that we had trained for just 1 epoch since this is a demonstration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='inference'></a>\n",
    "### Inference\n",
    "Inference using a .tlt trained or fine-tuned model uses the `tlt question_answering infer` command.  <br>\n",
    "The infer.yaml is also very uncomplicated:\n",
    "```\n",
    "# Name of  file containing data used as inputs during the inference.\n",
    "infer_ds:\n",
    "    file: ???  # e.g. squad/v1.1/dev-v1.1.json\n",
    "\n",
    "```\n",
    "\n",
    "We use the SQuAD 2.0 evaluation file for the sake of demonstration, you can also try out your own custom inputs as an exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt question_answering infer \\\n",
    "                        -e $SPECS_DIR/infer.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/infer \\\n",
    "                        infer_ds.file=$DATA_DIR/v2.0/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of inference are saved at `$RESULTS_DIR/infer/prediction.txt` in the format:\n",
    "```\n",
    "{\n",
    "    <question_id>: <answer>,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "A file with n-best results for each question is also saved at `$RESULTS_DIR/infer/nbest.txt` in the format:\n",
    "```\n",
    "{\n",
    "    <question_id>: [\n",
    "        {\n",
    "            \"text\": <answer-1>,\n",
    "            \"probability\": 0.9576789114427999,\n",
    "            \"start_logit\": 7.168248653411865,\n",
    "            \"end_logit\": 6.93817138671875\n",
    "        },\n",
    "        {\n",
    "            \"text\": <answer-2>,\n",
    "            \"probability\": 0.02714239211951417,\n",
    "            \"start_logit\": 7.168248653411865,\n",
    "            \"end_logit\": 3.374755620956421\n",
    "        },\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='export-onnx'></a>\n",
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ONNX](https://onnx.ai/) is a popular open format for machine learning models. It enables interoperability between different frameworks, easing the path to production. \n",
    "\n",
    "TLT provides commands to export the .tlt model to the ONNX format in an .eonnx archive.  The `export_format` configuration can be set to `ONNX` to achieve this.\n",
    "\n",
    "Sample usage of the `tlt question_answering export` command is shown in the following code cell. The export.yaml file we use looks like:\n",
    "```\n",
    "# Name of the .tlt EFF archive to be loaded/model to be exported.\n",
    "restore_from: trained-model.tlt\n",
    "\n",
    "# Set export format: ONNX | JARVIS\n",
    "export_format: JARVIS\n",
    "\n",
    "# Output EFF archive containing ONNX.\n",
    "export_to: exported-model.eonnx\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt question_answering export \\\n",
    "                        -e $SPECS_DIR/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/export \\\n",
    "                        export_format=ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command exports the model as `exported-model.eonnx` which is essentially an archive containing the .onnx model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLT provides the capability to use the exported .eonnx model for inference. The command `tlt question_answering infer_onnx` is very similar to the inference command for .tlt models. Again, the input file used are just for demo purposes, you may choose to try out their custom input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt question_answering infer_onnx \\\n",
    "                        -e $SPECS_DIR/infer_onnx.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/export/exported-model.eonnx \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/infer_onnx \\\n",
    "                        input_file=$DATA_DIR/v2.0/dev-v2.0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this `infer_onnx` is similar to that of the `infer` command before. A `prediction.txt` and a `nbest.txt` file is generated. You can configure the exact name/location of these results files in the spec yaml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='export-riva'></a>\n",
    "### Export to Riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TLT, you can also export your model in a format that can deployed using [Nvidia Riva](https://developer.nvidia.com/riva), a highly performant application framework for multi-modal conversational AI services using GPUs! The same command for exporting to ONNX can be used here. The only small variation is the configuration for `export_format` in the spec file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tlt question_answering export \\\n",
    "                        -e $SPECS_DIR/export.yaml \\\n",
    "                        -g 1 \\\n",
    "                        -m $RESULTS_DIR/train/checkpoints/trained-model.tlt \\\n",
    "                        -k $KEY \\\n",
    "                        -r $RESULTS_DIR/export_riva \\\n",
    "                        export_format=JARVIS \\\n",
    "                        export_to=qa-model.riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is exported as `qa-model.riva` which is in a format suited for deployment in Riva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use TLT to build custom models for your own applications, or you could deploy the custom model to Nvidia Riva!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
